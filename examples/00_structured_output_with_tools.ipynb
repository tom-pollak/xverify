{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Output with Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from xverify import ToolUse, run_tools, Env\n",
    "from xverify.tools import calculator, search\n",
    "from pydantic import ValidationError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Often when running multi-step reasoning, we want to use tools to help us.\n",
    "\n",
    "However, not many libraries natively support this. Pydantic for instance is optimized for a static declarative schema, which isn't well suited to ad-hoc tool use.\n",
    "\n",
    "Here we can see two examples of tools:\n",
    "- `calculator`: essentially a wrapper around the `eval` function\n",
    "- `search`: uses duckduckgo to search the web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculator(expression='3 + 4 * (6 ** 7)')='1119747'\n",
      "\n",
      "---\n",
      "\n",
      "search(query='What is the capital of France?', num_results=1)='• Paris - Wikipedia\\n  Paris is a global city of culture, finance, diplomacy, and tourism, with an estimated population of 2 million residents in 2025.'\n"
     ]
    }
   ],
   "source": [
    "print(f\"{calculator(expression='3 + 4 * (6 ** 7)')=}\")\n",
    "print(\"\\n---\\n\")\n",
    "print(f\"{search(query='What is the capital of France?', num_results=1)=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is we can't (natively) include a tool call in a Pydantic model (due to the static declarative schema).\n",
    "\n",
    "However, we can use the new `ToolUse` class to handle tool calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reasoning=\"Let's add two numbers\" tool_use=calculator(tool_name='calculator', expression='2 + 2')\n",
      "calc_2_2.tool_use.run_tool()='4'\n"
     ]
    }
   ],
   "source": [
    "class ReasoiningTool(BaseModel):\n",
    "    \"\"\"The result of a reasoning tool\"\"\"\n",
    "\n",
    "    reasoning: str\n",
    "    tool_use: ToolUse[calculator, search]\n",
    "\n",
    "\n",
    "calc_2_2 = ReasoiningTool.model_validate(\n",
    "    {\n",
    "        \"reasoning\": \"Let's add two numbers\",\n",
    "        \"tool_use\": {\"tool_name\": \"calculator\", \"expression\": \"2 + 2\"},\n",
    "    }\n",
    ")\n",
    "print(calc_2_2)\n",
    "print(f\"{calc_2_2.tool_use.run_tool()=}\") # on a ToolUse object, we can call run_tool() to run the tool and get the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is nice because if we can easily validate any arbitary schema and tool use is correct without any ad-hoc parsing (and we'll be able to enforce the LLM output is correct with guided decoding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tool not found!\n",
      "wrong argument!\n",
      "wrong argument type!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    ReasoiningTool.model_validate(\n",
    "        {\n",
    "            \"reasoning\": \"\",\n",
    "            \"tool_use\": {\"tool_name\": \"none_existing_tool\", \"expression\": \"2 + 2\"},\n",
    "        }\n",
    "    )\n",
    "except ValidationError:\n",
    "    print(\"tool not found!\")\n",
    "try:\n",
    "    ReasoiningTool.model_validate(\n",
    "        {\n",
    "            \"reasoning\": \"\",\n",
    "            \"tool_use\": {\"tool_name\": \"calculator\", \"wrong_arg\": \"2 + 2\"},\n",
    "        },\n",
    "    )\n",
    "except ValidationError:\n",
    "    print(\"wrong argument!\")\n",
    "\n",
    "try:\n",
    "    ReasoiningTool.model_validate(\n",
    "        {\n",
    "            \"reasoning\": \"\",\n",
    "            \"tool_use\": {\"tool_name\": \"calculator\", \"expression\": 2 + 2},\n",
    "        },\n",
    "    )\n",
    "except ValidationError:\n",
    "    print(\"wrong argument type!\")\n",
    "\n",
    "try:\n",
    "    # TODO: this should be a validation error\n",
    "    ReasoiningTool.model_validate(\n",
    "        {\n",
    "            \"reasoning\": \"\",\n",
    "            \"tool_use\": {\"tool_name\": \"calculator\", \"expression\": \"2 + 2\", \"extra_arg\": \"extra_arg\"},\n",
    "        },\n",
    "    )\n",
    "except ValidationError:\n",
    "    print(\"extra argument!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can implement a ReACT loop with tools really easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reason_and_Act(scratchpad='the question is 2 + 2', reasoning='we should use the calculator tool!', response=Tools(action='tool_use', tool_use=calculator(tool_name='calculator', expression='2 + 2')))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from typing import Literal, Union\n",
    "\n",
    "class Tools(BaseModel):\n",
    "    \"\"\"\n",
    "    Run a tool.\n",
    "    \"\"\"\n",
    "    action: Literal[\"tool_use\"] = Field(..., description=\"Action discriminator\")\n",
    "    tool_use: ToolUse[calculator, search] = Field(..., description=\"The tool call to use\")\n",
    "\n",
    "class FinalAnswer(BaseModel):\n",
    "    \"\"\"\n",
    "    Return a final answer.\n",
    "    \"\"\"\n",
    "    action: Literal[\"final_answer\"] = Field(..., description=\"Action discriminator\")\n",
    "    answer: str = Field(..., description=\"The final answer to the question\")\n",
    "\n",
    "class Reason_and_Act(BaseModel):\n",
    "    scratchpad: str = Field(..., description=\"Information from the Observation useful to answer the question\")\n",
    "    reasoning: str = Field(..., description=\"It describes your thoughts about the question you have been asked\")\n",
    "    response: Union[Tools, FinalAnswer] = Field(..., description=\"Final output: choose between the tool call or the final answer\", discriminator=\"action\")\n",
    "\n",
    "\n",
    "res = Reason_and_Act.model_validate(\n",
    "    {\n",
    "        \"scratchpad\": \"the question is 2 + 2\",\n",
    "        \"reasoning\": \"we should use the calculator tool!\",\n",
    "        \"response\": {\n",
    "            \"action\": \"tool_use\",\n",
    "            \"tool_use\": {\n",
    "                \"tool_name\": \"calculator\",\n",
    "                \"expression\": \"2 + 2\",\n",
    "            }\n",
    "        }\n",
    "    },\n",
    ")\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And in case we just want to run all the tools in a response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response': {'tool_use': {'calculator': '4'}}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_tools(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will return `None` where no tools were called, which is useful for checking for the end of the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(run_tools(Reason_and_Act(scratchpad=\"\", reasoning=\"\", response=FinalAnswer(action=\"final_answer\", answer=\"42\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you just want the output, run `run_tools` on the instantiated `ToolUse` object itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res.response.tool_use.run_tool()='4'\n"
     ]
    }
   ],
   "source": [
    "if isinstance(res.response, Tools):\n",
    "    print(f\"{res.response.tool_use.run_tool()=}\")\n",
    "else:\n",
    "    print(f\"{res.response.answer=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And of course we can do multiple tool calls in a single response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tool_use': [{'calculator': '4'},\n",
      "              {'search': '• Moon Fact Sheet - NSSDCA\\n'\n",
      "                         '  Equatorial radius (km) 1738.1: 6378.1: 0.2725: '\n",
      "                         'Polar radius (km) 1736.0: 6356.8: 0.2731: Volumetric '\n",
      "                         'mean radius (km) 1737.4: 6371.0: 0.2727: Ellipticity '\n",
      "                         '(Flattening) ...\\n'\n",
      "                         '\\n'\n",
      "                         '• Moon - Wikipedia\\n'\n",
      "                         '  The Moon has a solid iron-rich inner core with a '\n",
      "                         'radius possibly as small as 240 kilometres (150 mi) '\n",
      "                         'and a fluid outer core primarily made of liquid iron '\n",
      "                         'with a radius of roughly 300 kilometres (190 m.'},\n",
      "              {'calculator': '1480647168'}]}\n"
     ]
    }
   ],
   "source": [
    "class MultiToolUse(BaseModel):\n",
    "    tool_use: list[ToolUse[calculator, search]]\n",
    "\n",
    "\n",
    "res = MultiToolUse.model_validate(\n",
    "    {\n",
    "        \"tool_use\": [\n",
    "            {\"tool_name\": \"calculator\", \"expression\": \"2 + 2\"},\n",
    "            {\n",
    "                \"tool_name\": \"search\",\n",
    "                \"query\": \"What is the radius of the moon?\",\n",
    "                \"num_results\": 2,\n",
    "            },\n",
    "            {\"tool_name\": \"calculator\", \"expression\": \"3424 * 432432\"},\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "pprint(run_tools(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is cool, kinda, but now we have structed schema, we can **enforce** the LLM output is correct with guided decoding.\n",
    "\n",
    "Let's go back to our ReACT loop, and use `Env` to enforce the tool calls are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-12 09:12:52 __init__.py:207] Automatically detected platform cuda.\n",
      "INFO 03-12 09:12:58 config.py:549] This model supports multiple tasks: {'score', 'embed', 'classify', 'reward', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 03-12 09:12:58 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='Qwen/Qwen2.5-1.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-1.5B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 03-12 09:12:59 cuda.py:229] Using Flash Attention backend.\n",
      "INFO 03-12 09:13:00 model_runner.py:1110] Starting to load model Qwen/Qwen2.5-1.5B-Instruct...\n",
      "INFO 03-12 09:13:00 weight_utils.py:254] Using model weights format ['*.safetensors']\n",
      "INFO 03-12 09:13:00 weight_utils.py:304] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "129f3e2d660644568e2105e7554635bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-12 09:13:01 model_runner.py:1115] Loading model weights took 2.8875 GB\n",
      "INFO 03-12 09:13:02 worker.py:267] Memory profiling takes 0.51 seconds\n",
      "INFO 03-12 09:13:02 worker.py:267] the current vLLM instance can use total_gpu_memory (23.58GiB) x gpu_memory_utilization (0.90) = 21.22GiB\n",
      "INFO 03-12 09:13:02 worker.py:267] model weights take 2.89GiB; non_torch_memory takes 0.06GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 16.89GiB.\n",
      "INFO 03-12 09:13:02 executor_base.py:111] # cuda blocks: 39529, # CPU blocks: 9362\n",
      "INFO 03-12 09:13:02 executor_base.py:116] Maximum concurrency for 2000 tokens per request: 316.23x\n",
      "INFO 03-12 09:13:06 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:14<00:00,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-12 09:13:20 model_runner.py:1562] Graph capturing finished in 15 secs, took 0.20 GiB\n",
      "INFO 03-12 09:13:20 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 19.46 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-12 09:13:21 chat_utils.py:332] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n",
      "================================================================================\n",
      "Assistant: \n",
      "<Reason_and_Act>\n",
      "<scratchpad>\n",
      "To find the distance from the moon to the sun, I would first need to find both distances separately.\n",
      "</scratchpad>\n",
      "<reasoning>\n",
      "I need to find the distance from Earth to the Sun, and then add the distance from Earth to the Moon to that total.\n",
      "The standard unit of distance is the Astronomical Unit (AU), which is approximately the average distance from Earth to the Sun. An AU is about 93 million miles.\n",
      "The distance from Earth to the Moon is approximately 238,000 miles.\n",
      "To do the calculation, I need to use a calculator tool. I will search for \"distance from Earth to the Sun\" to get the AU value, and then find \"distance from Earth to the Moon\" to add that information.\n",
      "</reasoning>\n",
      "<response>\n",
      "<Tools>\n",
      "<action>\n",
      "tool_use\n",
      "</action>\n",
      "<tool_use>\n",
      "<calculator>\n",
      "<tool_name>\n",
      "calculator\n",
      "</tool_name>\n",
      "<expression>\n",
      "93000000 + 238000\n",
      "</expression>\n",
      "</calculator>\n",
      "</tool_use>\n",
      "</Tools>\n",
      "</response>\n",
      "</Reason_and_Act>\n",
      "================================================================================\n",
      "Tool output: {'response': {'tool_use': {'calculator': '93238000'}}}\n",
      "================================================================================\n",
      "Assistant: \n",
      "<Reason_and_Act>\n",
      "<scratchpad>\n",
      "To find the distance between the Moon and the Sun, we need to perform a mathematical calculation. The distance from the Moon to the Earth varies slightly due to gravitational interactions between the two bodies. However, if we consider the average distance between the Earth and the Moon, we can use that to estimate the distance between the Moon and the Sun.\n",
      "\n",
      "The average radius of the Earth's orbit around the Sun is approximately 93,000,000 miles, and the average radius of the Moon's orbit around the Earth is about 238,855 miles. When the Moon is positioned directly between the Earth and the Sun, the distance between the Earth and the Sun is the sum of these two distances. \n",
      "\n",
      "(93,000,000 + 238,855) miles = 93,238,855 miles\n",
      "</scratchpad>\n",
      "<reasoning>\n",
      "To find the actual average distance between the Moon and the Sun, we would need to use more precise data, but for our purposes here, the average distance varies slightly due to the Moon's position in its orbit. For an approximation, we can use the sum of Earth's orbit around the Sun (93,000,000 miles) and the average radius of the Moon's orbit around the Earth (238,855 miles). This gives us an approximate distance of 93,238,855 miles.\n",
      "</reasoning>\n",
      "<response>\n",
      "<FinalAnswer>\n",
      "<action>\n",
      "final_answer\n",
      "</action>\n",
      "<answer>\n",
      "238,855 miles\n",
      "</answer>\n",
      "</FinalAnswer>\n",
      "</response>\n",
      "</Reason_and_Act>\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM\n",
    "from vllm.sampling_params import RequestOutputKind\n",
    "\n",
    "if \"llm\" not in globals():  # interactive use\n",
    "    llm = LLM(model=\"Qwen/Qwen2.5-1.5B-Instruct\", max_model_len=2000)\n",
    "\n",
    "env = Env(Reason_and_Act)\n",
    "\n",
    "env.sampling_params()\n",
    "\n",
    "sampling_params = env.sampling_params(\n",
    "    max_tokens=500,\n",
    "    guided_decoding=dict(whitespace_pattern=r\"\"),\n",
    "    n=1,\n",
    "    temperature=1.,\n",
    "    output_kind=RequestOutputKind.FINAL_ONLY,\n",
    ")\n",
    "\n",
    "max_steps = 5\n",
    "messages: list[dict] = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": f\"\"\"\\\n",
    "You are a helpful assistant, responding in XML structured output.\n",
    "\n",
    "- Think step by step using the scratchpad and reasoning outputs. You have {max_steps - 1} steps to think before responding.\n",
    "- Use the tools provided. DO NOT rely on your own knowledge when a tool is available to help you.\n",
    "- Respond with a final answer only once your are absolutely sure you have the answer.\n",
    "\n",
    "Respond with a XML object, following the schema below:\n",
    "\n",
    "{env.doc}\n",
    "\n",
    "Use the tools!\n",
    "\"\"\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"What is the distance from the moon to the sun?\"},\n",
    "]\n",
    "\n",
    "for _ in range(max_steps):\n",
    "    outp = llm.chat(  # type: ignore\n",
    "        messages=messages,  # type: ignore\n",
    "        sampling_params=sampling_params,\n",
    "        use_tqdm=False,\n",
    "    )\n",
    "    text = outp[0].outputs[0].text\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Assistant:\", text)\n",
    "\n",
    "    struct_res = env.parse(text)\n",
    "    if not struct_res:\n",
    "        print(\"*** Invalid response, skipping ***\")\n",
    "        continue\n",
    "\n",
    "    messages.append({\"role\": \"assistant\", \"content\": struct_res.model_dump()})\n",
    "    tool_outp = run_tools(struct_res)\n",
    "    if tool_outp:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Tool output:\", tool_outp)\n",
    "        messages.append({\"role\": \"user\", \"content\": tool_outp})\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "issubclass() arg 1 must be a class",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mMultiToolUse\u001b[39;00m(BaseModel):\n\u001b[32m      7\u001b[39m     tool_use: \u001b[38;5;28mlist\u001b[39m[ToolUse[calculator, search]]\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43mgenerate_gbnf_grammar_and_documentation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mMultiToolUse\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/src/xverify/xml/grammar.py:1278\u001b[39m, in \u001b[36mgenerate_gbnf_grammar_and_documentation\u001b[39m\u001b[34m(pydantic_model_list, outer_object_name, outer_object_content, model_prefix, fields_prefix, list_of_outputs, documentation_with_field_description)\u001b[39m\n\u001b[32m   1257\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1258\u001b[39m \u001b[33;03mGenerate GBNF grammar and documentation for a list of Pydantic models.\u001b[39;00m\n\u001b[32m   1259\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1270\u001b[39m \u001b[33;03m    tuple: GBNF grammar string, documentation string.\u001b[39;00m\n\u001b[32m   1271\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1272\u001b[39m documentation = generate_markdown_documentation(\n\u001b[32m   1273\u001b[39m     copy(pydantic_model_list),\n\u001b[32m   1274\u001b[39m     model_prefix,\n\u001b[32m   1275\u001b[39m     fields_prefix,\n\u001b[32m   1276\u001b[39m     documentation_with_field_description=documentation_with_field_description,\n\u001b[32m   1277\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1278\u001b[39m grammar = \u001b[43mgenerate_gbnf_grammar_from_pydantic_models\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1279\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpydantic_model_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mouter_object_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mouter_object_content\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlist_of_outputs\u001b[49m\n\u001b[32m   1280\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1281\u001b[39m grammar = remove_empty_lines(grammar + get_primitive_grammar(grammar))\n\u001b[32m   1282\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m grammar, documentation\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/src/xverify/xml/grammar.py:693\u001b[39m, in \u001b[36mgenerate_gbnf_grammar_from_pydantic_models\u001b[39m\u001b[34m(models, outer_object_name, outer_object_content, list_of_outputs)\u001b[39m\n\u001b[32m    691\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m outer_object_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    692\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m models:\n\u001b[32m--> \u001b[39m\u001b[32m693\u001b[39m         model_rules, _ = \u001b[43mgenerate_gbnf_grammar\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    694\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessed_models\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreated_rules\u001b[49m\n\u001b[32m    695\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    696\u001b[39m         all_rules.extend(model_rules)\n\u001b[32m    698\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m list_of_outputs:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/src/xverify/xml/grammar.py:616\u001b[39m, in \u001b[36mgenerate_gbnf_grammar\u001b[39m\u001b[34m(model, processed_models, created_rules)\u001b[39m\n\u001b[32m    612\u001b[39m     field_info = model.model_fields[field_name]\n\u001b[32m    613\u001b[39m     is_optional = (\n\u001b[32m    614\u001b[39m         field_info.is_required \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_origin(field_type) \u001b[38;5;129;01mis\u001b[39;00m Optional\n\u001b[32m    615\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m616\u001b[39m rule_name, additional_rules = \u001b[43mgenerate_gbnf_rule_for_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    617\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    618\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfield_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    619\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfield_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    620\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_optional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    621\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprocessed_models\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreated_rules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    623\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfield_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    624\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    625\u001b[39m look_for_markdown_code_block = (\n\u001b[32m    626\u001b[39m     \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m rule_name == \u001b[33m\"\u001b[39m\u001b[33mmarkdown_code_block\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    627\u001b[39m )\n\u001b[32m    628\u001b[39m look_for_triple_quoted_string = (\n\u001b[32m    629\u001b[39m     \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m rule_name == \u001b[33m\"\u001b[39m\u001b[33mtriple_quoted_string\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    630\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/src/xverify/xml/grammar.py:431\u001b[39m, in \u001b[36mgenerate_gbnf_rule_for_type\u001b[39m\u001b[34m(model_name, field_name, field_type, is_optional, processed_models, created_rules, field_info)\u001b[39m\n\u001b[32m    428\u001b[39m     union_rules.append(union_gbnf_type)\n\u001b[32m    429\u001b[39m     rules.extend(union_rules_list)\n\u001b[32m--> \u001b[39m\u001b[32m431\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43missubclass\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43munion_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    432\u001b[39m     union_gbnf_type, union_rules_list = generate_gbnf_rule_for_type(\n\u001b[32m    433\u001b[39m         model_name,\n\u001b[32m    434\u001b[39m         field_name,\n\u001b[32m   (...)\u001b[39m\u001b[32m    438\u001b[39m         created_rules,\n\u001b[32m    439\u001b[39m     )\n\u001b[32m    440\u001b[39m     union_rules.append(union_gbnf_type)\n",
      "\u001b[31mTypeError\u001b[39m: issubclass() arg 1 must be a class"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from xverify import ToolUse\n",
    "from xverify.tools import calculator, search\n",
    "from xverify.xml import generate_gbnf_grammar_and_documentation\n",
    "\n",
    "class MultiToolUse(BaseModel):\n",
    "    tool_use: list[ToolUse[calculator, search]]\n",
    "\n",
    "\n",
    "generate_gbnf_grammar_and_documentation([MultiToolUse])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "issubclass() arg 1 must be a class",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m env = \u001b[43mEnv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMultiToolUse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m outp = llm.generate(\u001b[33m\"\u001b[39m\u001b[33muse 3 tools!\u001b[39m\u001b[33m\"\u001b[39m, sampling_params=env.sampling_params())\n\u001b[32m      3\u001b[39m outp[\u001b[32m0\u001b[39m].outputs.text\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/src/xverify/env.py:37\u001b[39m, in \u001b[36mEnv.__init__\u001b[39m\u001b[34m(self, model, schema, max_steps)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28mself\u001b[39m.schema = schema\n\u001b[32m     36\u001b[39m \u001b[38;5;28mself\u001b[39m.max_steps = max_steps\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[38;5;28mself\u001b[39m.gbnf, \u001b[38;5;28mself\u001b[39m.doc = \u001b[43mgenerate_gbnf_grammar_and_documentation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/src/xverify/xml/grammar.py:1278\u001b[39m, in \u001b[36mgenerate_gbnf_grammar_and_documentation\u001b[39m\u001b[34m(pydantic_model_list, outer_object_name, outer_object_content, model_prefix, fields_prefix, list_of_outputs, documentation_with_field_description)\u001b[39m\n\u001b[32m   1257\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1258\u001b[39m \u001b[33;03mGenerate GBNF grammar and documentation for a list of Pydantic models.\u001b[39;00m\n\u001b[32m   1259\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1270\u001b[39m \u001b[33;03m    tuple: GBNF grammar string, documentation string.\u001b[39;00m\n\u001b[32m   1271\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1272\u001b[39m documentation = generate_markdown_documentation(\n\u001b[32m   1273\u001b[39m     copy(pydantic_model_list),\n\u001b[32m   1274\u001b[39m     model_prefix,\n\u001b[32m   1275\u001b[39m     fields_prefix,\n\u001b[32m   1276\u001b[39m     documentation_with_field_description=documentation_with_field_description,\n\u001b[32m   1277\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1278\u001b[39m grammar = \u001b[43mgenerate_gbnf_grammar_from_pydantic_models\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1279\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpydantic_model_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mouter_object_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mouter_object_content\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlist_of_outputs\u001b[49m\n\u001b[32m   1280\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1281\u001b[39m grammar = remove_empty_lines(grammar + get_primitive_grammar(grammar))\n\u001b[32m   1282\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m grammar, documentation\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/src/xverify/xml/grammar.py:693\u001b[39m, in \u001b[36mgenerate_gbnf_grammar_from_pydantic_models\u001b[39m\u001b[34m(models, outer_object_name, outer_object_content, list_of_outputs)\u001b[39m\n\u001b[32m    691\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m outer_object_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    692\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m models:\n\u001b[32m--> \u001b[39m\u001b[32m693\u001b[39m         model_rules, _ = \u001b[43mgenerate_gbnf_grammar\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    694\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessed_models\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreated_rules\u001b[49m\n\u001b[32m    695\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    696\u001b[39m         all_rules.extend(model_rules)\n\u001b[32m    698\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m list_of_outputs:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/src/xverify/xml/grammar.py:616\u001b[39m, in \u001b[36mgenerate_gbnf_grammar\u001b[39m\u001b[34m(model, processed_models, created_rules)\u001b[39m\n\u001b[32m    612\u001b[39m     field_info = model.model_fields[field_name]\n\u001b[32m    613\u001b[39m     is_optional = (\n\u001b[32m    614\u001b[39m         field_info.is_required \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_origin(field_type) \u001b[38;5;129;01mis\u001b[39;00m Optional\n\u001b[32m    615\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m616\u001b[39m rule_name, additional_rules = \u001b[43mgenerate_gbnf_rule_for_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    617\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    618\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfield_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    619\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfield_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    620\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_optional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    621\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprocessed_models\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreated_rules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    623\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfield_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    624\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    625\u001b[39m look_for_markdown_code_block = (\n\u001b[32m    626\u001b[39m     \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m rule_name == \u001b[33m\"\u001b[39m\u001b[33mmarkdown_code_block\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    627\u001b[39m )\n\u001b[32m    628\u001b[39m look_for_triple_quoted_string = (\n\u001b[32m    629\u001b[39m     \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m rule_name == \u001b[33m\"\u001b[39m\u001b[33mtriple_quoted_string\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    630\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/src/xverify/xml/grammar.py:431\u001b[39m, in \u001b[36mgenerate_gbnf_rule_for_type\u001b[39m\u001b[34m(model_name, field_name, field_type, is_optional, processed_models, created_rules, field_info)\u001b[39m\n\u001b[32m    428\u001b[39m     union_rules.append(union_gbnf_type)\n\u001b[32m    429\u001b[39m     rules.extend(union_rules_list)\n\u001b[32m--> \u001b[39m\u001b[32m431\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43missubclass\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43munion_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    432\u001b[39m     union_gbnf_type, union_rules_list = generate_gbnf_rule_for_type(\n\u001b[32m    433\u001b[39m         model_name,\n\u001b[32m    434\u001b[39m         field_name,\n\u001b[32m   (...)\u001b[39m\u001b[32m    438\u001b[39m         created_rules,\n\u001b[32m    439\u001b[39m     )\n\u001b[32m    440\u001b[39m     union_rules.append(union_gbnf_type)\n",
      "\u001b[31mTypeError\u001b[39m: issubclass() arg 1 must be a class"
     ]
    }
   ],
   "source": [
    "env = Env(MultiToolUse)\n",
    "outp = llm.generate(\"use 3 tools!\", sampling_params=env.sampling_params())\n",
    "outp[0].outputs.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the cool thing is, this is a Qwen 1.5B model, not even trained on function calling!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
