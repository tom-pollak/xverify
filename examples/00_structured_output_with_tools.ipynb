{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Output with Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from xverify import ToolUse, run_tools, Env\n",
    "from xverify.tools import calculator, search\n",
    "from pydantic import ValidationError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Often when running multi-step reasoning, we want to use tools to help us.\n",
    "\n",
    "However, not many libraries natively support this. Pydantic for instance is optimized for a static declarative schema, which isn't well suited to ad-hoc tool use.\n",
    "\n",
    "Here we can see two examples of tools:\n",
    "- `calculator`: essentially a wrapper around the `eval` function\n",
    "- `search`: uses duckduckgo to search the web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculator(expression='3 + 4 * (6 ** 7)')='1119747'\n",
      "\n",
      "---\n",
      "\n",
      "search(query='What is the capital of France?', num_results=1)='• Paris - Wikipedia\\n  Paris is a global city of culture, finance, diplomacy, and tourism, with an estimated population of 2 million residents in 2025.'\n"
     ]
    }
   ],
   "source": [
    "print(f\"{calculator(expression='3 + 4 * (6 ** 7)')=}\")\n",
    "print(\"\\n---\\n\")\n",
    "print(f\"{search(query='What is the capital of France?', num_results=1)=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is we can't (natively) include a tool call in a Pydantic model (due to the static declarative schema).\n",
    "\n",
    "However, we can use the new `ToolUse` class to handle tool calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reasoning=\"Let's add two numbers\" tool_use=calculator(tool_name='calculator', expression='2 + 2')\n",
      "calc_2_2.tool_use.run_tool()='4'\n"
     ]
    }
   ],
   "source": [
    "class ReasoiningTool(BaseModel):\n",
    "    \"\"\"The result of a reasoning tool\"\"\"\n",
    "\n",
    "    reasoning: str\n",
    "    tool_use: ToolUse[calculator, search]\n",
    "\n",
    "\n",
    "calc_2_2 = ReasoiningTool.model_validate(\n",
    "    {\n",
    "        \"reasoning\": \"Let's add two numbers\",\n",
    "        \"tool_use\": {\"tool_name\": \"calculator\", \"expression\": \"2 + 2\"},\n",
    "    }\n",
    ")\n",
    "print(calc_2_2)\n",
    "print(f\"{calc_2_2.tool_use.run_tool()=}\") # on a ToolUse object, we can call run_tool() to run the tool and get the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is nice because if we can easily validate any arbitary schema and tool use is correct without any ad-hoc parsing (and we'll be able to enforce the LLM output is correct with guided decoding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tool not found!\n",
      "wrong argument!\n",
      "wrong argument type!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    ReasoiningTool.model_validate(\n",
    "        {\n",
    "            \"reasoning\": \"\",\n",
    "            \"tool_use\": {\"tool_name\": \"none_existing_tool\", \"expression\": \"2 + 2\"},\n",
    "        }\n",
    "    )\n",
    "except ValidationError:\n",
    "    print(\"tool not found!\")\n",
    "try:\n",
    "    ReasoiningTool.model_validate(\n",
    "        {\n",
    "            \"reasoning\": \"\",\n",
    "            \"tool_use\": {\"tool_name\": \"calculator\", \"wrong_arg\": \"2 + 2\"},\n",
    "        },\n",
    "    )\n",
    "except ValidationError:\n",
    "    print(\"wrong argument!\")\n",
    "\n",
    "try:\n",
    "    ReasoiningTool.model_validate(\n",
    "        {\n",
    "            \"reasoning\": \"\",\n",
    "            \"tool_use\": {\"tool_name\": \"calculator\", \"expression\": 2 + 2},\n",
    "        },\n",
    "    )\n",
    "except ValidationError:\n",
    "    print(\"wrong argument type!\")\n",
    "\n",
    "try:\n",
    "    # TODO: this should be a validation error\n",
    "    ReasoiningTool.model_validate(\n",
    "        {\n",
    "            \"reasoning\": \"\",\n",
    "            \"tool_use\": {\"tool_name\": \"calculator\", \"expression\": \"2 + 2\", \"extra_arg\": \"extra_arg\"},\n",
    "        },\n",
    "    )\n",
    "except ValidationError:\n",
    "    print(\"extra argument!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can implement a ReACT loop with tools really easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reason_and_Act(scratchpad='the question is 2 + 2', reasoning='we should use the calculator tool!', response=Tools(action='tool_use', tool_use=calculator(tool_name='calculator', expression='2 + 2')))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from typing import Literal, Union\n",
    "\n",
    "class Tools(BaseModel):\n",
    "    \"\"\"\n",
    "    Run a tool.\n",
    "    \"\"\"\n",
    "    action: Literal[\"tool_use\"] = Field(..., description=\"Action discriminator\")\n",
    "    tool_use: ToolUse[calculator, search] = Field(..., description=\"The tool call to use\")\n",
    "\n",
    "class FinalAnswer(BaseModel):\n",
    "    \"\"\"\n",
    "    Return a final answer.\n",
    "    \"\"\"\n",
    "    action: Literal[\"final_answer\"] = Field(..., description=\"Action discriminator\")\n",
    "    answer: str = Field(..., description=\"The final answer to the question\")\n",
    "\n",
    "class Reason_and_Act(BaseModel):\n",
    "    scratchpad: str = Field(..., description=\"Information from the Observation useful to answer the question\")\n",
    "    reasoning: str = Field(..., description=\"It describes your thoughts about the question you have been asked\")\n",
    "    response: Union[Tools, FinalAnswer] = Field(..., description=\"Final output: choose between the tool call or the final answer\", discriminator=\"action\")\n",
    "\n",
    "\n",
    "res = Reason_and_Act.model_validate(\n",
    "    {\n",
    "        \"scratchpad\": \"the question is 2 + 2\",\n",
    "        \"reasoning\": \"we should use the calculator tool!\",\n",
    "        \"response\": {\n",
    "            \"action\": \"tool_use\",\n",
    "            \"tool_use\": {\n",
    "                \"tool_name\": \"calculator\",\n",
    "                \"expression\": \"2 + 2\",\n",
    "            }\n",
    "        }\n",
    "    },\n",
    ")\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And in case we just want to run all the tools in a response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response': {'tool_use': {'calculator': '4'}}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_tools(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will return `None` where no tools were called, which is useful for checking for the end of the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(run_tools(Reason_and_Act(scratchpad=\"\", reasoning=\"\", response=FinalAnswer(action=\"final_answer\", answer=\"42\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you just want the output, run `run_tools` on the instantiated `ToolUse` object itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res.response.tool_use.run_tool()='4'\n"
     ]
    }
   ],
   "source": [
    "if isinstance(res.response, Tools):\n",
    "    print(f\"{res.response.tool_use.run_tool()=}\")\n",
    "else:\n",
    "    print(f\"{res.response.answer=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And of course we can do multiple tool calls in a single response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tool_use': [{'calculator': '4'},\n",
      "              {'search': '• Moon Fact Sheet - NSSDCA\\n'\n",
      "                         '  Equatorial radius (km) 1738.1: 6378.1: 0.2725: '\n",
      "                         'Polar radius (km) 1736.0: 6356.8: 0.2731: Volumetric '\n",
      "                         'mean radius (km) 1737.4: 6371.0: 0.2727: Ellipticity '\n",
      "                         '(Flattening) ...\\n'\n",
      "                         '\\n'\n",
      "                         '• Moon - Wikipedia\\n'\n",
      "                         '  The Moon has a solid iron-rich inner core with a '\n",
      "                         'radius possibly as small as 240 kilometres (150 mi) '\n",
      "                         'and a fluid outer core primarily made of liquid iron '\n",
      "                         'with a radius of roughly 300 kilometres (190 m.'},\n",
      "              {'calculator': '1480647168'}]}\n"
     ]
    }
   ],
   "source": [
    "class MultiToolUse(BaseModel):\n",
    "    tool_use: list[ToolUse[calculator, search]]\n",
    "\n",
    "\n",
    "res = MultiToolUse.model_validate(\n",
    "    {\n",
    "        \"tool_use\": [\n",
    "            {\"tool_name\": \"calculator\", \"expression\": \"2 + 2\"},\n",
    "            {\n",
    "                \"tool_name\": \"search\",\n",
    "                \"query\": \"What is the radius of the moon?\",\n",
    "                \"num_results\": 2,\n",
    "            },\n",
    "            {\"tool_name\": \"calculator\", \"expression\": \"3424 * 432432\"},\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "pprint(run_tools(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is cool, kinda, but now we have structed schema, we can **enforce** the LLM output is correct with guided decoding.\n",
    "\n",
    "Let's go back to our ReACT loop, and use `Env` to enforce the tool calls are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-12 10:46:40 __init__.py:207] Automatically detected platform cuda.\n",
      "INFO 03-12 10:46:46 config.py:549] This model supports multiple tasks: {'score', 'classify', 'generate', 'embed', 'reward'}. Defaulting to 'generate'.\n",
      "INFO 03-12 10:46:46 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='Qwen/Qwen2.5-1.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-1.5B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 03-12 10:46:47 cuda.py:229] Using Flash Attention backend.\n",
      "INFO 03-12 10:46:48 model_runner.py:1110] Starting to load model Qwen/Qwen2.5-1.5B-Instruct...\n",
      "INFO 03-12 10:46:48 weight_utils.py:254] Using model weights format ['*.safetensors']\n",
      "INFO 03-12 10:46:48 weight_utils.py:304] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89843d7916674c6f83b84e6d3c8c47a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-12 10:46:49 model_runner.py:1115] Loading model weights took 2.8875 GB\n",
      "INFO 03-12 10:46:50 worker.py:267] Memory profiling takes 0.53 seconds\n",
      "INFO 03-12 10:46:50 worker.py:267] the current vLLM instance can use total_gpu_memory (23.58GiB) x gpu_memory_utilization (0.90) = 21.22GiB\n",
      "INFO 03-12 10:46:50 worker.py:267] model weights take 2.89GiB; non_torch_memory takes 0.06GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 16.89GiB.\n",
      "INFO 03-12 10:46:50 executor_base.py:111] # cuda blocks: 39529, # CPU blocks: 9362\n",
      "INFO 03-12 10:46:50 executor_base.py:116] Maximum concurrency for 2000 tokens per request: 316.23x\n",
      "INFO 03-12 10:46:54 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:15<00:00,  2.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-12 10:47:09 model_runner.py:1562] Graph capturing finished in 15 secs, took 0.20 GiB\n",
      "INFO 03-12 10:47:09 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 20.05 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-12 10:47:10 chat_utils.py:332] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n",
      "================================================================================\n",
      "Assistant: \n",
      "<Reason_and_Act>\n",
      "<scratchpad>\n",
      "To find the distance from the moon to the sun, I would first need to find both distances separately.\n",
      "</scratchpad>\n",
      "<reasoning>\n",
      "I need to find the distance from Earth to the Sun, and then add the distance from Earth to the Moon to that total.\n",
      "The standard unit of distance is the Astronomical Unit (AU), which is approximately the average distance from Earth to the Sun. An AU is about 93 million miles.\n",
      "The distance from Earth to the Moon is approximately 238,000 miles.\n",
      "To do the calculation, I need to use a calculator tool. I will search for \"distance from Earth to the Sun\" to get the AU value, and then find \"distance from Earth to the Moon\" to add that information.\n",
      "</reasoning>\n",
      "<response>\n",
      "<Tools>\n",
      "<action>\n",
      "tool_use\n",
      "</action>\n",
      "<tool_use>\n",
      "<calculator>\n",
      "<tool_name>\n",
      "calculator\n",
      "</tool_name>\n",
      "<expression>\n",
      "93000000 + 238000\n",
      "</expression>\n",
      "</calculator>\n",
      "</tool_use>\n",
      "</Tools>\n",
      "</response>\n",
      "</Reason_and_Act>\n",
      "================================================================================\n",
      "Tool output: {'response': {'tool_use': {'calculator': '93238000'}}}\n",
      "================================================================================\n",
      "Assistant: \n",
      "<Reason_and_Act>\n",
      "<scratchpad>\n",
      "To find the distance between the Moon and the Sun, we need to perform a mathematical calculation. The distance from the Moon to the Earth varies slightly due to gravitational interactions between the two bodies. However, if we consider the average distance between the Earth and the Moon, we can use that to estimate the distance between the Moon and the Sun.\n",
      "\n",
      "The average radius of the Earth's orbit around the Sun is approximately 93,000,000 miles, and the average radius of the Moon's orbit around the Earth is about 238,855 miles. When the Moon is positioned directly between the Earth and the Sun, the distance between the Earth and the Sun is the sum of these two distances. \n",
      "\n",
      "(93,000,000 + 238,855) miles = 93,238,855 miles\n",
      "</scratchpad>\n",
      "<reasoning>\n",
      "To find the actual average distance between the Moon and the Sun, we would need to use more precise data, but for our purposes here, the average distance varies slightly due to the Moon's position in its orbit. For an approximation, we can use the sum of Earth's orbit around the Sun (93,000,000 miles) and the average radius of the Moon's orbit around the Earth (238,855 miles). This gives us an approximate distance of 93,238,855 miles.\n",
      "</reasoning>\n",
      "<response>\n",
      "<FinalAnswer>\n",
      "<action>\n",
      "final_answer\n",
      "</action>\n",
      "<answer>\n",
      "238,855 miles\n",
      "</answer>\n",
      "</FinalAnswer>\n",
      "</response>\n",
      "</Reason_and_Act>\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM\n",
    "\n",
    "if \"llm\" not in globals():  # interactive use\n",
    "    llm = LLM(model=\"Qwen/Qwen2.5-1.5B-Instruct\", max_model_len=2000)\n",
    "\n",
    "env = Env(Reason_and_Act)\n",
    "\n",
    "env.sampling_params()\n",
    "\n",
    "sampling_params = env.sampling_params(\n",
    "    max_tokens=500,\n",
    "    # guided_decoding=dict(whitespace_pattern=r\"\"),\n",
    "    n=1,\n",
    "    temperature=1.,\n",
    "\n",
    ")\n",
    "\n",
    "max_steps = 5\n",
    "messages: list[dict] = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": f\"\"\"\\\n",
    "You are a helpful assistant, responding in XML structured output.\n",
    "\n",
    "- Think step by step using the scratchpad and reasoning outputs. You have {max_steps - 1} steps to think before responding.\n",
    "- Use the tools provided. DO NOT rely on your own knowledge when a tool is available to help you.\n",
    "- Respond with a final answer only once your are absolutely sure you have the answer.\n",
    "\n",
    "Respond with a XML object, following the schema below:\n",
    "\n",
    "{env.doc}\n",
    "\n",
    "Use the tools!\n",
    "\"\"\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"What is the distance from the moon to the sun?\"},\n",
    "]\n",
    "\n",
    "for _ in range(max_steps):\n",
    "    outp = llm.chat(  # type: ignore\n",
    "        messages=messages,  # type: ignore\n",
    "        sampling_params=sampling_params,\n",
    "        use_tqdm=False,\n",
    "    )\n",
    "    text = outp[0].outputs[0].text\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Assistant:\", text)\n",
    "\n",
    "    struct_res = env.parse(text)\n",
    "    if not struct_res:\n",
    "        print(\"*** Invalid response, skipping ***\")\n",
    "        continue\n",
    "\n",
    "    messages.append({\"role\": \"assistant\", \"content\": struct_res.model_dump()})\n",
    "    tool_outp = run_tools(struct_res)\n",
    "    if tool_outp:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Tool output:\", tool_outp)\n",
    "        messages.append({\"role\": \"user\", \"content\": tool_outp})\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Model: Reason_and_Act\n",
      "  Output Fields:\n",
      "    scratchpad (str):\n",
      "        Description: Information from the Observation useful to answer the question\n",
      "    reasoning (str):\n",
      "        Description: It describes your thoughts about the question you have been asked\n",
      "    response (Tools or FinalAnswer):\n",
      "        Description: Final output: choose between the tool call or the final answer\n",
      "\n",
      "Model: Tools\n",
      "  Description: Run a tool.\n",
      "  Fields:\n",
      "    action (Literal):\n",
      "        Description: Action discriminator\n",
      "    tool_use (calculator or search):\n",
      "        Description: The tool call to use\n",
      "\n",
      "Model: FinalAnswer\n",
      "  Description: Return a final answer.\n",
      "  Fields:\n",
      "    action (Literal):\n",
      "        Description: Action discriminator\n",
      "    answer (str):\n",
      "        Description: The final answer to the question\n",
      "\n",
      "Model: calculator\n",
      "  Description: Evaluates a single line of Python math expression. No imports or variables allowed.\n",
      "\n",
      "    Examples:\n",
      "        <expression>\n",
      "        2 + 2\n",
      "        </expression>\n",
      "        >>> \"4\"\n",
      "        <expression>\n",
      "        3 * (17 + 4)\n",
      "        </expression>\n",
      "        >>> \"63\"\n",
      "        <expression>\n",
      "        100 / 5\n",
      "        </expression>\n",
      "        >>> \"20.0\"\n",
      "    \n",
      "Returns: str - The result of the calculation or an error message\n",
      "  Fields:\n",
      "    tool_name (Literal):\n",
      "        Description: Function to call\n",
      "    expression (str):\n",
      "        Description: A mathematical expression using only numbers and basic operators (+,-,*,/,**,())\n",
      "\n",
      "Model: search\n",
      "  Description: Searches DuckDuckGo and returns concise summaries of top results.\n",
      "\n",
      "    Examples:\n",
      "        <query>\n",
      "        who invented the lightbulb\n",
      "        </query>\n",
      "        <num_results>\n",
      "        3\n",
      "        </num_results>\n",
      "    \n",
      "Returns: str - Formatted string with bullet points of top results, each with title and brief summary\n",
      "  Fields:\n",
      "    tool_name (Literal):\n",
      "        Description: Function to call\n",
      "    query (str):\n",
      "        Description: The search query string\n",
      "    num_results (int):\n",
      "        Description: Number of results to return (default: 5, max: 10)\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(env.doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the cool thing is, this is a Qwen 1.5B model, not even trained on function calling!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
