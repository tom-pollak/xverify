{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Output with Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from xverify import XMLToolUse, JSONToolUse, run_tools, Environment, ValidationError\n",
    "from xverify.tools import calculator, search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Often when running multi-step reasoning, we want to use tools to help us.\n",
    "\n",
    "However, not many libraries natively support this. Pydantic for instance is optimized for a static declarative schema, which isn't well suited to ad-hoc tool use.\n",
    "\n",
    "Here we can see two examples of tools:\n",
    "- `calculator`: essentially a wrapper around the `eval` function\n",
    "- `search`: uses duckduckgo to search the web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculator(expression='3 + 4 * (6 ** 7)')='1119747'\n",
      "\n",
      "---\n",
      "\n",
      "search(query='What is the capital of France?', num_results=1)='• Paris - Wikipedia\\n  Paris is a global city of culture, finance, diplomacy, and tourism, with an estimated population of 2 million residents in 2025.'\n"
     ]
    }
   ],
   "source": [
    "print(f\"{calculator(expression='3 + 4 * (6 ** 7)')=}\")\n",
    "print(\"\\n---\\n\")\n",
    "print(f\"{search(query='What is the capital of France?', num_results=1)=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is we can't (natively) include a tool call in a Pydantic model (due to the static declarative schema).\n",
    "\n",
    "However, we can use the new `ToolUse` class to handle tool calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reasoning=\"Let's add two numbers\" tool_use=calculator(tool_name='calculator', expression='2 + 2')\n",
      "calc_2_2.tool_use.run_tool()='4'\n"
     ]
    }
   ],
   "source": [
    "class ReasoningTool(BaseModel):\n",
    "    \"\"\"The result of a reasoning tool\"\"\"\n",
    "\n",
    "    reasoning: str\n",
    "    tool_use: JSONToolUse[calculator, search]\n",
    "\n",
    "\n",
    "calc_2_2 = ReasoningTool.model_validate(\n",
    "    {\n",
    "        \"reasoning\": \"Let's add two numbers\",\n",
    "        \"tool_use\": {\"tool_name\": \"calculator\", \"expression\": \"2 + 2\"},\n",
    "    }\n",
    ")\n",
    "print(calc_2_2)\n",
    "print(\n",
    "    f\"{calc_2_2.tool_use.run_tool()=}\"\n",
    ")  # on a ToolUse object, we can call run_tool() to run the tool and get the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is nice because if we can easily validate any arbitary schema and tool use is correct without any ad-hoc parsing (and we'll be able to enforce the LLM output is correct with guided decoding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tool not found!\n",
      "wrong argument!\n",
      "wrong argument type!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    ReasoningTool.model_validate(\n",
    "        {\n",
    "            \"reasoning\": \"\",\n",
    "            \"tool_use\": {\"tool_name\": \"none_existing_tool\", \"expression\": \"2 + 2\"},\n",
    "        }\n",
    "    )\n",
    "except ValidationError:\n",
    "    print(\"tool not found!\")\n",
    "try:\n",
    "    ReasoningTool.model_validate(\n",
    "        {\n",
    "            \"reasoning\": \"\",\n",
    "            \"tool_use\": {\"tool_name\": \"calculator\", \"wrong_arg\": \"2 + 2\"},\n",
    "        },\n",
    "    )\n",
    "except ValidationError:\n",
    "    print(\"wrong argument!\")\n",
    "\n",
    "try:\n",
    "    ReasoningTool.model_validate(\n",
    "        {\n",
    "            \"reasoning\": \"\",\n",
    "            \"tool_use\": {\"tool_name\": \"calculator\", \"expression\": 2 + 2},\n",
    "        },\n",
    "    )\n",
    "except ValidationError:\n",
    "    print(\"wrong argument type!\")\n",
    "\n",
    "try:\n",
    "    # TODO: this should be a validation error\n",
    "    ReasoningTool.model_validate(\n",
    "        {\n",
    "            \"reasoning\": \"\",\n",
    "            \"tool_use\": {\n",
    "                \"tool_name\": \"calculator\",\n",
    "                \"expression\": \"2 + 2\",\n",
    "                \"extra_arg\": \"extra_arg\",\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "except ValidationError:\n",
    "    print(\"extra argument!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can implement a ReACT loop with tools really easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reason_and_Act(scratchpad='the question is 2 + 2', reasoning='we should use the calculator tool!', response=FinalAnswer(answer='42'))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Literal, Union\n",
    "\n",
    "\n",
    "class Tools(BaseModel):\n",
    "    \"\"\"\n",
    "    Run a tool.\n",
    "    \"\"\"\n",
    "    tool_use: XMLToolUse[calculator, search] = Field(..., description=\"The tool call to use\")\n",
    "\n",
    "\n",
    "class FinalAnswer(BaseModel):\n",
    "    \"\"\"\n",
    "    Return a final answer.\n",
    "    \"\"\"\n",
    "    answer: str = Field(..., description=\"The final answer to the question\")\n",
    "\n",
    "\n",
    "class Reason_and_Act(BaseModel):\n",
    "    scratchpad: str = Field(\n",
    "        ...,\n",
    "        description=\"Information from the Observation useful to answer the question\",\n",
    "    )\n",
    "    reasoning: str = Field(\n",
    "        ...,\n",
    "        description=\"It describes your thoughts about the question you have been asked\",\n",
    "    )\n",
    "    response: Union[Tools, FinalAnswer]\n",
    "\n",
    "\n",
    "res = Reason_and_Act.model_validate(\n",
    "    {\n",
    "        \"scratchpad\": \"the question is 2 + 2\",\n",
    "        \"reasoning\": \"we should use the calculator tool!\",\n",
    "        \"response\": {\n",
    "            \"tool_use\": {\n",
    "                # \"tool_name\": \"calculator\",\n",
    "                \"expression\": \"2 + 2\",\n",
    "            },\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "Reason_and_Act.model_validate({\n",
    "    \"scratchpad\": \"the question is 2 + 2\",\n",
    "    \"reasoning\": \"we should use the calculator tool!\",\n",
    "    \"response\": {\"answer\": \"42\"}\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And in case we just want to run all the tools in a response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response': {'tool_use': {'calculator': '4'}}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_tools(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will return `None` where no tools were called, which is useful for checking for the end of the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    run_tools(\n",
    "        Reason_and_Act(\n",
    "            scratchpad=\"\",\n",
    "            reasoning=\"\",\n",
    "            response=FinalAnswer(answer=\"42\"),\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you just want the output, run `run_tools` on the instantiated `ToolUse` object itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res.response.tool_use.run_tool()='4'\n"
     ]
    }
   ],
   "source": [
    "if isinstance(res.response, Tools):\n",
    "    print(f\"{res.response.tool_use.run_tool()=}\")\n",
    "else:\n",
    "    print(f\"{res.response.answer=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And of course we can do multiple tool calls in a single response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tool_use': [{'calculator': '4'},\n",
      "              {'search': '• Moon Fact Sheet - NSSDCA\\n'\n",
      "                         '  Equatorial radius (km) 1738.1: 6378.1: 0.2725: '\n",
      "                         'Polar radius (km) 1736.0: 6356.8: 0.2731: Volumetric '\n",
      "                         'mean radius (km) 1737.4: 6371.0: 0.2727: Ellipticity '\n",
      "                         '(Flattening) ...\\n'\n",
      "                         '\\n'\n",
      "                         '• Moon - Wikipedia\\n'\n",
      "                         '  The Moon has a solid iron-rich inner core with a '\n",
      "                         'radius possibly as small as 240 kilometres (150 mi) '\n",
      "                         'and a fluid outer core primarily made of liquid iron '\n",
      "                         'with a radius of roughly 300 kilometres (190 m.'},\n",
      "              {'calculator': '1480647168'}]}\n"
     ]
    }
   ],
   "source": [
    "class MultiToolUse(BaseModel):\n",
    "    tool_use: list[XMLToolUse[calculator, search]]\n",
    "\n",
    "\n",
    "res = MultiToolUse.model_validate(\n",
    "    {\n",
    "        \"tool_use\": [\n",
    "            {\"tool_name\": \"calculator\", \"expression\": \"2 + 2\"},\n",
    "            {\n",
    "                \"tool_name\": \"search\",\n",
    "                \"query\": \"What is the radius of the moon?\",\n",
    "                \"num_results\": 2,\n",
    "            },\n",
    "            {\"tool_name\": \"calculator\", \"expression\": \"3424 * 432432\"},\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "pprint(run_tools(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is cool, kinda, but now we have structed schema, we can **enforce** the LLM output is correct with guided decoding.\n",
    "\n",
    "Let's go back to our ReACT loop, and use `Env` to enforce the tool calls are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Assistant:\n",
      "<Reason_and_Act>\n",
      "<scratchpad>\n",
      "Perform an online search to find the distance between the Earth and the Sun.\n",
      "</scratchpad>\n",
      "<reasoning>\n",
      "I need to find the distance from the Moon to the Sun. Unfortunately, the Moon orbit's exact distance can be calculated using astronomical calculations that may not be accessible through simple search engines. Instead, I'll look for an approximate distance from the Earth to the Sun, which would also give me an approximate distance to the Moon.\n",
      "</reasoning>\n",
      "<response>\n",
      "<Tools>\n",
      "<tool_use>\n",
      "<search>\n",
      "<query>\n",
      "What is the average distance between the Earth and the Sun?\n",
      "</query>\n",
      "<num_results>\n",
      "3\n",
      "</num_results>\n",
      "</search>\n",
      "</tool_use>\n",
      "</Tools>\n",
      "</response>\n",
      "</Reason_and_Act>\n",
      "================================================================================\n",
      "Tool output:\n",
      "{'response': {'tool_use': {'search': \"• How Far is the Sun From Earth? | Distance, KM, Time Takes & Facts\\n  Since the Earth moves around the Sun, the distance differs, with Earth's closest point from the Sun - perihelion - reaching 147.5 million km / 91.3 million mi.\\n\\n• Sun Distance from Earth (km, miles, and light-years) - Planets Education\\n  Though the average distance between the sun and the earth is around 149.598 million kilometers. In actuality, the Sun distance to Earth varies around 3% as the earth orbits the sun.\\n\\n• Astronomical Unit: How far away is the sun? | Space\\n  This is roughly the average distance between Earth and the sun. ... An astronomical unit is the distance between Earth and the sun and measures distances on the scale of star systems. A parsec is ...\"}}}\n",
      "================================================================================\n",
      "Assistant:\n",
      "<Reason_and_Act>\n",
      "<scratchpad>\n",
      "In astrology, the average distance from the Earth to the Sun is about 93 million miles. However, \"average\" implies a variation, and the actual distance to the Moon is more straightforward.\n",
      "</scratchpad>\n",
      "<reasoning>\n",
      "To find the distance from the Moon to the Sun, we need to subtract the average distance of the Earth to the Sun (150 million kilometers) from the average distance of the Sun to the Moon (385,000 kilometers).\n",
      "</reasoning>\n",
      "<response>\n",
      "<FinalAnswer>\n",
      "<answer>\n",
      "344,000,000 miles\n",
      "</answer>\n",
      "</FinalAnswer>\n",
      "</response>\n",
      "</Reason_and_Act>\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM\n",
    "\n",
    "if \"llm\" not in globals():  # interactive use\n",
    "    llm = LLM(model=\"Qwen/Qwen2.5-1.5B-Instruct\", max_model_len=2000)\n",
    "\n",
    "env = Environment(Reason_and_Act)\n",
    "\n",
    "env.sampling_params()\n",
    "\n",
    "sampling_params = env.sampling_params(\n",
    "    max_tokens=500,\n",
    "    n=1,\n",
    "    temperature=1.0,\n",
    ")\n",
    "\n",
    "max_steps = 5\n",
    "messages: list[dict] = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": f\"\"\"\\\n",
    "You are a helpful assistant, responding in XML structured output.\n",
    "\n",
    "- Think step by step using the scratchpad and reasoning outputs. You have {max_steps - 1} steps to think before responding.\n",
    "- Use the tools provided. DO NOT rely on your own knowledge when a tool is available to help you.\n",
    "- Respond with a final answer only once your are absolutely sure you have the answer.\n",
    "\n",
    "Respond with a XML object, following the schema below:\n",
    "\n",
    "{env.doc}\n",
    "\n",
    "Use the tools!\n",
    "\"\"\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"What is the distance from the moon to the sun?\"},\n",
    "]\n",
    "\n",
    "for _ in range(max_steps):\n",
    "    outp = llm.chat(  # type: ignore\n",
    "        messages=messages,  # type: ignore\n",
    "        sampling_params=sampling_params,\n",
    "        use_tqdm=False,\n",
    "    )\n",
    "    text = outp[0].outputs[0].text\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Assistant:\\n{text}\")\n",
    "\n",
    "    struct_res = env.parse(text)\n",
    "    if not struct_res:\n",
    "        print(\"*** Invalid response, skipping ***\")\n",
    "        continue\n",
    "\n",
    "    messages.append({\"role\": \"assistant\", \"content\": struct_res.model_dump()})\n",
    "    tool_outp = run_tools(struct_res)\n",
    "    if tool_outp:\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Tool output:\\n{tool_outp}\")\n",
    "        messages.append({\"role\": \"user\", \"content\": tool_outp})\n",
    "    else:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
