{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Output with Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from xverify import XMLToolUse, JSONToolUse, run_tools, Env\n",
    "from xverify.tools import calculator, search\n",
    "from pydantic import ValidationError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Often when running multi-step reasoning, we want to use tools to help us.\n",
    "\n",
    "However, not many libraries natively support this. Pydantic for instance is optimized for a static declarative schema, which isn't well suited to ad-hoc tool use.\n",
    "\n",
    "Here we can see two examples of tools:\n",
    "- `calculator`: essentially a wrapper around the `eval` function\n",
    "- `search`: uses duckduckgo to search the web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculator(expression='3 + 4 * (6 ** 7)')='1119747'\n",
      "\n",
      "---\n",
      "\n",
      "search(query='What is the capital of France?', num_results=1)='• Paris - Wikipedia\\n  Paris is a global city of culture, finance, diplomacy, and tourism, with an estimated population of 2 million residents in 2025.'\n"
     ]
    }
   ],
   "source": [
    "print(f\"{calculator(expression='3 + 4 * (6 ** 7)')=}\")\n",
    "print(\"\\n---\\n\")\n",
    "print(f\"{search(query='What is the capital of France?', num_results=1)=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is we can't (natively) include a tool call in a Pydantic model (due to the static declarative schema).\n",
    "\n",
    "However, we can use the new `ToolUse` class to handle tool calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reasoning=\"Let's add two numbers\" tool_use=calculator(tool_name='calculator', expression='2 + 2')\n",
      "calc_2_2.tool_use.run_tool()='4'\n"
     ]
    }
   ],
   "source": [
    "class ReasoningTool(BaseModel):\n",
    "    \"\"\"The result of a reasoning tool\"\"\"\n",
    "\n",
    "    reasoning: str\n",
    "    tool_use: JSONToolUse[calculator, search]\n",
    "\n",
    "\n",
    "calc_2_2 = ReasoningTool.model_validate(\n",
    "    {\n",
    "        \"reasoning\": \"Let's add two numbers\",\n",
    "        \"tool_use\": {\"tool_name\": \"calculator\", \"expression\": \"2 + 2\"},\n",
    "    }\n",
    ")\n",
    "print(calc_2_2)\n",
    "print(f\"{calc_2_2.tool_use.run_tool()=}\") # on a ToolUse object, we can call run_tool() to run the tool and get the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is nice because if we can easily validate any arbitary schema and tool use is correct without any ad-hoc parsing (and we'll be able to enforce the LLM output is correct with guided decoding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tool not found!\n",
      "wrong argument!\n",
      "wrong argument type!\n",
      "extra argument!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    ReasoningTool.model_validate(\n",
    "        {\n",
    "            \"reasoning\": \"\",\n",
    "            \"tool_use\": {\"tool_name\": \"none_existing_tool\", \"expression\": \"2 + 2\"},\n",
    "        }\n",
    "    )\n",
    "except ValidationError:\n",
    "    print(\"tool not found!\")\n",
    "try:\n",
    "    ReasoningTool.model_validate(\n",
    "        {\n",
    "            \"reasoning\": \"\",\n",
    "            \"tool_use\": {\"tool_name\": \"calculator\", \"wrong_arg\": \"2 + 2\"},\n",
    "        },\n",
    "    )\n",
    "except ValidationError:\n",
    "    print(\"wrong argument!\")\n",
    "\n",
    "try:\n",
    "    ReasoningTool.model_validate(\n",
    "        {\n",
    "            \"reasoning\": \"\",\n",
    "            \"tool_use\": {\"tool_name\": \"calculator\", \"expression\": 2 + 2},\n",
    "        },\n",
    "    )\n",
    "except ValidationError:\n",
    "    print(\"wrong argument type!\")\n",
    "\n",
    "try:\n",
    "    # TODO: this should be a validation error\n",
    "    ReasoningTool.model_validate(\n",
    "        {\n",
    "            \"reasoning\": \"\",\n",
    "            \"tool_use\": {\"tool_name\": \"calculator\", \"expression\": \"2 + 2\", \"extra_arg\": \"extra_arg\"},\n",
    "        },\n",
    "    )\n",
    "except ValidationError:\n",
    "    print(\"extra argument!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can implement a ReACT loop with tools really easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reason_and_Act(scratchpad='the question is 2 + 2', reasoning='we should use the calculator tool!', response=Tools(action='tool_use', tool_use=calculator(expression='2 + 2')))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from typing import Literal, Union\n",
    "\n",
    "class Tools(BaseModel):\n",
    "    \"\"\"\n",
    "    Run a tool.\n",
    "    \"\"\"\n",
    "    action: Literal[\"tool_use\"] = Field(..., description=\"Action discriminator\")\n",
    "    tool_use: XMLToolUse[calculator, search] = Field(..., description=\"The tool call to use\")\n",
    "\n",
    "class FinalAnswer(BaseModel):\n",
    "    \"\"\"\n",
    "    Return a final answer.\n",
    "    \"\"\"\n",
    "    action: Literal[\"final_answer\"] = Field(..., description=\"Action discriminator\")\n",
    "    answer: str = Field(..., description=\"The final answer to the question\")\n",
    "\n",
    "class Reason_and_Act(BaseModel):\n",
    "    scratchpad: str = Field(..., description=\"Information from the Observation useful to answer the question\")\n",
    "    reasoning: str = Field(..., description=\"It describes your thoughts about the question you have been asked\")\n",
    "    response: Union[Tools, FinalAnswer] = Field(..., description=\"Final output: choose between the tool call or the final answer\", discriminator=\"action\")\n",
    "\n",
    "\n",
    "res = Reason_and_Act.model_validate(\n",
    "    {\n",
    "        \"scratchpad\": \"the question is 2 + 2\",\n",
    "        \"reasoning\": \"we should use the calculator tool!\",\n",
    "        \"response\": {\n",
    "            \"action\": \"tool_use\",\n",
    "            \"tool_use\": {\n",
    "                # \"tool_name\": \"calculator\",\n",
    "                \"expression\": \"2 + 2\",\n",
    "            }\n",
    "        }\n",
    "    },\n",
    ")\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And in case we just want to run all the tools in a response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response': {'tool_use': {'calculator': '4'}}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_tools(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will return `None` where no tools were called, which is useful for checking for the end of the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(run_tools(Reason_and_Act(scratchpad=\"\", reasoning=\"\", response=FinalAnswer(action=\"final_answer\", answer=\"42\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you just want the output, run `run_tools` on the instantiated `ToolUse` object itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res.response.tool_use.run_tool()='4'\n"
     ]
    }
   ],
   "source": [
    "if isinstance(res.response, Tools):\n",
    "    print(f\"{res.response.tool_use.run_tool()=}\")\n",
    "else:\n",
    "    print(f\"{res.response.answer=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And of course we can do multiple tool calls in a single response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tool_use': [{'calculator': '4'},\n",
      "              {'search': '• Moon Fact Sheet - NSSDCA\\n'\n",
      "                         '  Equatorial radius (km) 1738.1: 6378.1: 0.2725: '\n",
      "                         'Polar radius (km) 1736.0: 6356.8: 0.2731: Volumetric '\n",
      "                         'mean radius (km) 1737.4: 6371.0: 0.2727: Ellipticity '\n",
      "                         '(Flattening) ...\\n'\n",
      "                         '\\n'\n",
      "                         '• Moon - Wikipedia\\n'\n",
      "                         '  The Moon has a solid iron-rich inner core with a '\n",
      "                         'radius possibly as small as 240 kilometres (150 mi) '\n",
      "                         'and a fluid outer core primarily made of liquid iron '\n",
      "                         'with a radius of roughly 300 kilometres (190 m.'},\n",
      "              {'calculator': '1480647168'}]}\n"
     ]
    }
   ],
   "source": [
    "class MultiToolUse(BaseModel):\n",
    "    tool_use: list[XMLToolUse[calculator, search]]\n",
    "\n",
    "\n",
    "res = MultiToolUse.model_validate(\n",
    "    {\n",
    "        \"tool_use\": [\n",
    "            {\"tool_name\": \"calculator\", \"expression\": \"2 + 2\"},\n",
    "            {\n",
    "                \"tool_name\": \"search\",\n",
    "                \"query\": \"What is the radius of the moon?\",\n",
    "                \"num_results\": 2,\n",
    "            },\n",
    "            {\"tool_name\": \"calculator\", \"expression\": \"3424 * 432432\"},\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "pprint(run_tools(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is cool, kinda, but now we have structed schema, we can **enforce** the LLM output is correct with guided decoding.\n",
    "\n",
    "Let's go back to our ReACT loop, and use `Env` to enforce the tool calls are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-12 13:19:02 __init__.py:207] Automatically detected platform cuda.\n",
      "INFO 03-12 13:19:07 config.py:549] This model supports multiple tasks: {'score', 'generate', 'embed', 'reward', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 03-12 13:19:07 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='Qwen/Qwen2.5-1.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-1.5B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 03-12 13:19:09 cuda.py:229] Using Flash Attention backend.\n",
      "INFO 03-12 13:19:09 model_runner.py:1110] Starting to load model Qwen/Qwen2.5-1.5B-Instruct...\n",
      "INFO 03-12 13:19:09 weight_utils.py:254] Using model weights format ['*.safetensors']\n",
      "INFO 03-12 13:19:10 weight_utils.py:304] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02756ca1f94842029d894186c56d5bff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-12 13:19:11 model_runner.py:1115] Loading model weights took 2.8875 GB\n",
      "INFO 03-12 13:19:11 worker.py:267] Memory profiling takes 0.55 seconds\n",
      "INFO 03-12 13:19:11 worker.py:267] the current vLLM instance can use total_gpu_memory (23.58GiB) x gpu_memory_utilization (0.90) = 21.22GiB\n",
      "INFO 03-12 13:19:11 worker.py:267] model weights take 2.89GiB; non_torch_memory takes 0.06GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 16.89GiB.\n",
      "INFO 03-12 13:19:12 executor_base.py:111] # cuda blocks: 39529, # CPU blocks: 9362\n",
      "INFO 03-12 13:19:12 executor_base.py:116] Maximum concurrency for 2000 tokens per request: 316.23x\n",
      "INFO 03-12 13:19:16 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:00<00:16,  2.03it/s]"
     ]
    }
   ],
   "source": [
    "from vllm import LLM\n",
    "\n",
    "if \"llm\" not in globals():  # interactive use\n",
    "    llm = LLM(model=\"Qwen/Qwen2.5-1.5B-Instruct\", max_model_len=2000)\n",
    "\n",
    "env = Env(Reason_and_Act)\n",
    "\n",
    "env.sampling_params()\n",
    "\n",
    "sampling_params = env.sampling_params(\n",
    "    max_tokens=500,\n",
    "    n=1,\n",
    "    temperature=1.,\n",
    "\n",
    ")\n",
    "\n",
    "max_steps = 5\n",
    "messages: list[dict] = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": f\"\"\"\\\n",
    "You are a helpful assistant, responding in XML structured output.\n",
    "\n",
    "- Think step by step using the scratchpad and reasoning outputs. You have {max_steps - 1} steps to think before responding.\n",
    "- Use the tools provided. DO NOT rely on your own knowledge when a tool is available to help you.\n",
    "- Respond with a final answer only once your are absolutely sure you have the answer.\n",
    "\n",
    "Respond with a XML object, following the schema below:\n",
    "\n",
    "{env.doc}\n",
    "\n",
    "Use the tools!\n",
    "\"\"\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"What is the distance from the moon to the sun?\"},\n",
    "]\n",
    "\n",
    "for _ in range(max_steps):\n",
    "    outp = llm.chat(  # type: ignore\n",
    "        messages=messages,  # type: ignore\n",
    "        sampling_params=sampling_params,\n",
    "        use_tqdm=False,\n",
    "    )\n",
    "    text = outp[0].outputs[0].text\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Assistant:\", text)\n",
    "\n",
    "    struct_res = env.parse(text)\n",
    "    if not struct_res:\n",
    "        print(\"*** Invalid response, skipping ***\")\n",
    "        continue\n",
    "\n",
    "    messages.append({\"role\": \"assistant\", \"content\": struct_res.model_dump()})\n",
    "    tool_outp = run_tools(struct_res)\n",
    "    if tool_outp:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Tool output:\", tool_outp)\n",
    "        messages.append({\"role\": \"user\", \"content\": tool_outp})\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for Reason_and_Act\nresponse.tool_use.tool_use\n  Field required [type=missing, input_value={'action': 'tool_use', 'a...r': '495028.5095009445'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 49\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# First process dictionaries to handle alternating key-value pairs\u001b[39;00m\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# processed = _process_dicts(parsed)\u001b[39;00m\n\u001b[32m     46\u001b[39m \n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# Then squeeze model keys\u001b[39;00m\n\u001b[32m     48\u001b[39m squeezed = _squeeze_model_keys(parsed, model_names)\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m \u001b[43mReason_and_Act\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqueezed\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/pydantic/main.py:627\u001b[39m, in \u001b[36mBaseModel.model_validate\u001b[39m\u001b[34m(cls, obj, strict, from_attributes, context)\u001b[39m\n\u001b[32m    625\u001b[39m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[32m    626\u001b[39m __tracebackhide__ = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m627\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_attributes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_attributes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[32m    629\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mValidationError\u001b[39m: 1 validation error for Reason_and_Act\nresponse.tool_use.tool_use\n  Field required [type=missing, input_value={'action': 'tool_use', 'a...r': '495028.5095009445'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing"
     ]
    }
   ],
   "source": [
    "txt = \"\"\"\n",
    "<Reason_and_Act>\n",
    "\n",
    "<scratchpad>\n",
    "Using the tool \"search\" to find information about the distance from the moon to the sun.\n",
    "</scratchpad>\n",
    "<reasoning>\n",
    "The distance from the moon to the sun is a physical fact that is not obtained through direct calculation. However, we can use the tool \"search\" to find out the typical distance per year for recreational purposes.\n",
    "</reasoning>\n",
    "<response>\n",
    "<FinalAnswer>\n",
    "\n",
    "<action>\n",
    "tool_use\n",
    "</action>\n",
    "<answer>\n",
    "495028.5095009445\n",
    "</answer>\n",
    "</FinalAnswer>\n",
    "</response>\n",
    "</Reason_and_Act>\n",
    "\"\"\"\n",
    "\n",
    "import xmltodict\n",
    "from xverify.xml.parser import _get_model_names, _squeeze_model_keys\n",
    "\n",
    "# Force various container tags to be parsed as lists\n",
    "parsed = xmltodict.parse(\n",
    "    text, force_list=(\"list-item\", \"set-item\", \"key-item\", \"value-item\")\n",
    ")\n",
    "\n",
    "# Include all container tags in model names for handling\n",
    "model_names = {\n",
    "    \"list\",\n",
    "    \"set\",\n",
    "    \"dict\",\n",
    "    \"list-item\",\n",
    "    \"set-item\",\n",
    "    \"key-item\",\n",
    "    \"value-item\",\n",
    "    *(_get_model_names(Reason_and_Act)),\n",
    "}\n",
    "\n",
    "# First process dictionaries to handle alternating key-value pairs\n",
    "# processed = _process_dicts(parsed)\n",
    "\n",
    "# Then squeeze model keys\n",
    "squeezed = _squeeze_model_keys(parsed, model_names)\n",
    "Reason_and_Act.model_validate(squeezed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'scratchpad': 'Using the tool \"search\" to find information about the distance from the moon to the sun.',\n",
       " 'reasoning': 'The distance from the moon to the sun is a physical fact that is not obtained through direct calculation. However, we can use the tool \"search\" to find out the typical distance per year for recreational purposes.',\n",
       " 'response': {'action': 'tool_use', 'answer': '495028.5095009445'}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squeezed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root ::= grammar-models\n",
      "string ::= nl [^\\n<] ([^<])*\n",
      "boolean ::= nl \"true\" | nl \"false\"\n",
      "integer ::= nl [0-9]+\n",
      "float ::= nl \"-\"? [0-9]+ (\".\" [0-9]+)?\n",
      "null ::= nl \"null\"\n",
      "nl ::= \"\\n\"\n",
      "action-literal ::= nl \"tool_use\"\n",
      "tool_name-literal ::= nl \"calculator\"\n",
      "calculator ::= nl \"<calculator>\" nl nl \"<tool_name>\" tool_name-literal nl \"</tool_name>\" nl \"<expression>\" string nl \"</expression>\" nl \"</calculator>\"\n",
      "search ::= nl \"<search>\" nl nl \"<tool_name>\" tool_name-literal nl \"</tool_name>\" nl \"<query>\" string nl \"</query>\" nl \"<num_results>\" integer nl \"</num_results>\" nl \"</search>\"\n",
      "tool_use-union ::= calculator | search\n",
      "Tools ::= nl \"<Tools>\" nl nl \"<action>\" action-literal nl \"</action>\" nl \"<tool_use>\" tool_use-union nl \"</tool_use>\" nl \"</Tools>\"\n",
      "FinalAnswer ::= nl \"<FinalAnswer>\" nl nl \"<action>\" action-literal nl \"</action>\" nl \"<answer>\" string nl \"</answer>\" nl \"</FinalAnswer>\"\n",
      "response-union ::= Tools | FinalAnswer\n",
      "Reason_and_Act ::= nl \"<Reason_and_Act>\" nl nl \"<scratchpad>\" string nl \"</scratchpad>\" nl \"<reasoning>\" string nl \"</reasoning>\" nl \"<response>\" response-union nl \"</response>\" nl \"</Reason_and_Act>\"\n",
      "grammar-models ::= Reason_and_Act\n"
     ]
    }
   ],
   "source": [
    "print(env.gbnf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Model: Reason_and_Act\n",
      "  Output Fields:\n",
      "    scratchpad (str):\n",
      "        Description: Information from the Observation useful to answer the question\n",
      "    reasoning (str):\n",
      "        Description: It describes your thoughts about the question you have been asked\n",
      "    response (Tools or FinalAnswer):\n",
      "        Description: Final output: choose between the tool call or the final answer\n",
      "\n",
      "Model: Tools\n",
      "  Description: Run a tool.\n",
      "  Fields:\n",
      "    action (Literal['tool_use']):\n",
      "        Description: Action discriminator\n",
      "    tool_use (calculator or search):\n",
      "        Description: The tool call to use\n",
      "\n",
      "Model: FinalAnswer\n",
      "  Description: Return a final answer.\n",
      "  Fields:\n",
      "    action (Literal['final_answer']):\n",
      "        Description: Action discriminator\n",
      "    answer (str):\n",
      "        Description: The final answer to the question\n",
      "\n",
      "Model: calculator\n",
      "  Description: Evaluates a single line of Python math expression. No imports or variables allowed.\n",
      "    \n",
      "        Examples:\n",
      "            <expression>\n",
      "            2 + 2\n",
      "            </expression>\n",
      "            >>> \"4\"\n",
      "            <expression>\n",
      "            3 * (17 + 4)\n",
      "            </expression>\n",
      "            >>> \"63\"\n",
      "            <expression>\n",
      "            100 / 5\n",
      "            </expression>\n",
      "            >>> \"20.0\"\n",
      "        \n",
      "    Returns: str - The result of the calculation or an error message\n",
      "  Fields:\n",
      "    tool_name (Literal['calculator']):\n",
      "        Description: Function to call\n",
      "    expression (str):\n",
      "        Description: A mathematical expression using only numbers and basic operators (+,-,*,/,**,())\n",
      "\n",
      "Model: search\n",
      "  Description: Searches DuckDuckGo and returns concise summaries of top results.\n",
      "    \n",
      "        Examples:\n",
      "            <query>\n",
      "            who invented the lightbulb\n",
      "            </query>\n",
      "            <num_results>\n",
      "            3\n",
      "            </num_results>\n",
      "        \n",
      "    Returns: str - Formatted string with bullet points of top results, each with title and brief summary\n",
      "  Fields:\n",
      "    tool_name (Literal['search']):\n",
      "        Description: Function to call\n",
      "    query (str):\n",
      "        Description: The search query string\n",
      "    num_results (int):\n",
      "        Description: Number of results to return (default: 5, max: 10)\n"
     ]
    }
   ],
   "source": [
    "print(env.doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the cool thing is, this is a Qwen 1.5B model, not even trained on function calling!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
