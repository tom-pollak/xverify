{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Output with Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from xverify import ToolUse, run_tools, Env\n",
    "from xverify.tools import calculator, search\n",
    "from pydantic import ValidationError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Often when running multi-step reasoning, we want to use tools to help us.\n",
    "\n",
    "However, not many libraries natively support this. Pydantic for instance is optimized for a static declarative schema, which isn't well suited to ad-hoc tool use.\n",
    "\n",
    "Here we can see two examples of tools:\n",
    "- `calculator`: essentially a wrapper around the `eval` function\n",
    "- `search`: uses duckduckgo to search the web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculator(expression='3 + 4 * (6 ** 7)')='1119747'\n",
      "\n",
      "---\n",
      "\n",
      "search(query='What is the capital of France?', num_results=1)='• Paris - Wikipedia\\n  Paris is a global city of culture, finance, diplomacy, and tourism, with an estimated population of 2 million residents in 2025.'\n"
     ]
    }
   ],
   "source": [
    "print(f\"{calculator(expression='3 + 4 * (6 ** 7)')=}\")\n",
    "print(\"\\n---\\n\")\n",
    "print(f\"{search(query='What is the capital of France?', num_results=1)=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is we can't (natively) include a tool call in a Pydantic model (due to the static declarative schema).\n",
    "\n",
    "However, we can use the new `ToolUse` class to handle tool calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reasoning=\"Let's add two numbers\" tool_use=calculator(expression='2 + 2', tool_name='calculator')\n",
      "calc_2_2.tool_use.run_tool()='4'\n"
     ]
    }
   ],
   "source": [
    "class ReasoiningTool(BaseModel):\n",
    "    \"\"\"The result of a reasoning tool\"\"\"\n",
    "\n",
    "    reasoning: str\n",
    "    tool_use: ToolUse[calculator, search]\n",
    "\n",
    "\n",
    "calc_2_2 = ReasoiningTool.model_validate(\n",
    "    {\n",
    "        \"reasoning\": \"Let's add two numbers\",\n",
    "        \"tool_use\": {\"tool_name\": \"calculator\", \"expression\": \"2 + 2\"},\n",
    "    }\n",
    ")\n",
    "print(calc_2_2)\n",
    "print(f\"{calc_2_2.tool_use.run_tool()=}\") # on a ToolUse object, we can call run_tool() to run the tool and get the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is nice because if we can easily validate any arbitary schema and tool use is correct without any ad-hoc parsing (and we'll be able to enforce the LLM output is correct with guided decoding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tool not found!\n",
      "wrong argument!\n",
      "wrong argument type!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    ReasoiningTool.model_validate(\n",
    "        {\n",
    "            \"reasoning\": \"\",\n",
    "            \"tool_use\": {\"tool_name\": \"none_existing_tool\", \"expression\": \"2 + 2\"},\n",
    "        }\n",
    "    )\n",
    "except ValidationError:\n",
    "    print(\"tool not found!\")\n",
    "try:\n",
    "    ReasoiningTool.model_validate(\n",
    "        {\n",
    "            \"reasoning\": \"\",\n",
    "            \"tool_use\": {\"tool_name\": \"calculator\", \"wrong_arg\": \"2 + 2\"},\n",
    "        },\n",
    "    )\n",
    "except ValidationError:\n",
    "    print(\"wrong argument!\")\n",
    "\n",
    "try:\n",
    "    ReasoiningTool.model_validate(\n",
    "        {\n",
    "            \"reasoning\": \"\",\n",
    "            \"tool_use\": {\"tool_name\": \"calculator\", \"expression\": 2 + 2},\n",
    "        },\n",
    "    )\n",
    "except ValidationError:\n",
    "    print(\"wrong argument type!\")\n",
    "\n",
    "try:\n",
    "    # TODO: this should be a validation error\n",
    "    ReasoiningTool.model_validate(\n",
    "        {\n",
    "            \"reasoning\": \"\",\n",
    "            \"tool_use\": {\"tool_name\": \"calculator\", \"expression\": \"2 + 2\", \"extra_arg\": \"extra_arg\"},\n",
    "        },\n",
    "    )\n",
    "except ValidationError:\n",
    "    print(\"extra argument!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can implement a ReACT loop with tools really easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reason_and_Act(scratchpad='the question is 2 + 2', reasoning='we should use the calculator tool!', output=Tools(action='tool_use', tool_use=calculator(expression='2 + 2', tool_name='calculator')))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from typing import Literal, Union\n",
    "\n",
    "class Tools(BaseModel):\n",
    "    \"\"\"\n",
    "    Run a tool.\n",
    "    \"\"\"\n",
    "    action: Literal[\"tool_use\"] = Field(..., description=\"Action discriminator\")\n",
    "    tool_use: ToolUse[calculator, search] = Field(..., description=\"The tool call to use\")\n",
    "\n",
    "class FinalAnswer(BaseModel):\n",
    "    \"\"\"\n",
    "    Return a final answer.\n",
    "    \"\"\"\n",
    "    action: Literal[\"final_answer\"] = Field(..., description=\"Action discriminator\")\n",
    "    answer: str = Field(..., description=\"The final answer to the question\")\n",
    "\n",
    "class Reason_and_Act(BaseModel):\n",
    "    scratchpad: str = Field(..., description=\"Information from the Observation useful to answer the question\")\n",
    "    reasoning: str = Field(..., description=\"It describes your thoughts about the question you have been asked\")\n",
    "    output: Union[Tools, FinalAnswer] = Field(..., description=\"Final output: choose between the tool call or the final answer\", discriminator=\"action\")\n",
    "\n",
    "\n",
    "res = Reason_and_Act.model_validate(\n",
    "    {\n",
    "        \"scratchpad\": \"the question is 2 + 2\",\n",
    "        \"reasoning\": \"we should use the calculator tool!\",\n",
    "        \"output\": {\n",
    "            \"action\": \"tool_use\",\n",
    "            \"tool_use\": {\n",
    "                \"tool_name\": \"calculator\",\n",
    "                \"expression\": \"2 + 2\",\n",
    "            }\n",
    "        }\n",
    "    },\n",
    ")\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And in case we just want to run all the tools in a response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': {'tool_use': {'calculator': '4'}}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_tools(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will return `None` where no tools were called, which is useful for checking for the end of the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(run_tools(Reason_and_Act(scratchpad=\"\", reasoning=\"\", output=FinalAnswer(action=\"final_answer\", answer=\"42\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you just want the output, run `run_tools` on the instantiated `ToolUse` object itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res.output.tool_use.run_tool()='4'\n"
     ]
    }
   ],
   "source": [
    "if isinstance(res.output, Tools):\n",
    "    print(f\"{res.output.tool_use.run_tool()=}\")\n",
    "else:\n",
    "    print(f\"{res.output.answer=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And of course we can do multiple tool calls in a single response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tool_use': [{'calculator': '4'},\n",
      "              {'search': '• Moon Fact Sheet - NSSDCA\\n'\n",
      "                         '  Equatorial radius (km) 1738.1: 6378.1: 0.2725: '\n",
      "                         'Polar radius (km) 1736.0: 6356.8: 0.2731: Volumetric '\n",
      "                         'mean radius (km) 1737.4: 6371.0: 0.2727: Ellipticity '\n",
      "                         '(Flattening) ...\\n'\n",
      "                         '\\n'\n",
      "                         '• What is the Radius of Moon? - GeeksforGeeks\\n'\n",
      "                         \"  The Moon's radius is about 27% of Earth's radius, \"\n",
      "                         'and its gravitational force is about 1/6th of '\n",
      "                         \"Earth's gravity.\"},\n",
      "              {'calculator': '1480647168'}]}\n"
     ]
    }
   ],
   "source": [
    "class MultiToolUse(BaseModel):\n",
    "    tool_use: list[ToolUse[calculator, search]]\n",
    "\n",
    "\n",
    "res = MultiToolUse.model_validate(\n",
    "    {\n",
    "        \"tool_use\": [\n",
    "            {\"tool_name\": \"calculator\", \"expression\": \"2 + 2\"},\n",
    "            {\n",
    "                \"tool_name\": \"search\",\n",
    "                \"query\": \"What is the radius of the moon?\",\n",
    "                \"num_results\": 2,\n",
    "            },\n",
    "            {\"tool_name\": \"calculator\", \"expression\": \"3424 * 432432\"},\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "pprint(run_tools(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is cool, kinda, but now we have structed schema, we can **enforce** the LLM output is correct with guided decoding.\n",
    "\n",
    "Let's go back to our ReACT loop, and use `Env` to enforce the tool calls are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root ::= grammar-models\n",
      "grammar-models ::= Reason_and_Act\n",
      "Reason_and_Act ::= \"<Reason_and_Act>\" nl \"<scratchpad>\" nl string nl \"</scratchpad>\" nl \"<reasoning>\" nl string nl \"</reasoning>\" nl \"<output>\" nl Reason_and_Act-output-union nl \"</output>\" nl nl \"</Reason_and_Act>\" nl\n",
      "Tools ::= \"<Tools>\" nl \"<action>\" nl Toolsaction nl \"</action>\" nl \"<tool_use>\" nl Tools-tool_use-union nl \"</tool_use>\" nl nl \"</Tools>\" nl\n",
      "Toolsaction ::= \"tool_use\" \n",
      "calculator ::= \"<calculator>\" nl \"<expression>\" nl string nl \"</expression>\" nl \"<tool_name>\" nl calculatortool_name nl \"</tool_name>\" nl nl \"</calculator>\" nl\n",
      "calculatortool_name ::= \"calculator\" \n",
      "search ::= \"<search>\" nl \"<query>\" nl string nl \"</query>\" nl \"<num_results>\" nl integer nl \"</num_results>\" nl \"<tool_name>\" nl searchtool_name nl \"</tool_name>\" nl nl \"</search>\" nl\n",
      "searchtool_name ::= \"search\" \n",
      "Tools-tool_use-union ::= calculator | search\n",
      "FinalAnswer ::= \"<FinalAnswer>\" nl \"<action>\" nl FinalAnsweraction nl \"</action>\" nl \"<answer>\" nl string nl \"</answer>\" nl nl \"</FinalAnswer>\" nl\n",
      "FinalAnsweraction ::= \"final_answer\" \n",
      "Reason_and_Act-output-union ::= Tools | FinalAnswer\n",
      "boolean ::= \"true\" | \"false\"\n",
      "null ::= \"null\"\n",
      "string ::= ([^<])*\n",
      "nl ::= \"\\n\"\n",
      "float ::= \"-\"? [0-9]+ (\".\" [0-9]+)?\n",
      "integer ::= [0-9]+\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env = Env(Reason_and_Act)\n",
    "print(env.gbnf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Model: Reason_and_Act\n",
      "  Output Fields:\n",
      "    scratchpad (str):\n",
      "        Description: Information from the Observation useful to answer the question\n",
      "    reasoning (str):\n",
      "        Description: It describes your thoughts about the question you have been asked\n",
      "    output (Tools or FinalAnswer):\n",
      "        Description: Final output: choose between the tool call or the final answer\n",
      "\n",
      "Model: Tools\n",
      "  Description: Run a tool.\n",
      "  Fields:\n",
      "    action (Literal):\n",
      "        Description: Action discriminator\n",
      "    tool_use (calculator or search):\n",
      "        Description: The tool call to use\n",
      "\n",
      "Model: FinalAnswer\n",
      "  Description: Return a final answer.\n",
      "  Fields:\n",
      "    action (Literal):\n",
      "        Description: Action discriminator\n",
      "    answer (str):\n",
      "        Description: The final answer to the question\n",
      "\n",
      "Model: calculator\n",
      "  Description: Evaluates a single line of Python math expression. No imports or variables allowed.\n",
      "\n",
      "    Examples:\n",
      "        {\"expression\": \"2 + 2\"} -> \"4\"\n",
      "        {\"expression\": \"3 * (17 + 4)\"} -> \"63\"\n",
      "        {\"expression\": \"100 / 5\"} -> \"20.0\"\n",
      "    \n",
      "Returns: str - The result of the calculation or an error message\n",
      "  Fields:\n",
      "    expression (str):\n",
      "        Description: A mathematical expression using only numbers and basic operators (+,-,*,/,**,())\n",
      "    tool_name (Literal):\n",
      "        Description: Function to call\n",
      "\n",
      "Model: search\n",
      "  Description: Searches DuckDuckGo and returns concise summaries of top results.\n",
      "\n",
      "    Examples:\n",
      "        {\"query\": \"who invented the lightbulb\", \"num_results\": 3}\n",
      "    \n",
      "Returns: str - Formatted string with bullet points of top results, each with title and brief summary\n",
      "  Fields:\n",
      "    query (str):\n",
      "        Description: The search query string\n",
      "    num_results (int):\n",
      "        Description: Number of results to return (default: 5, max: 10)\n",
      "    tool_name (Literal):\n",
      "        Description: Function to call\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(env.doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***\n",
      "root ::= grammar-models\n",
      "grammar-models ::= Reason_and_Act\n",
      "Reason_and_Act ::= nl \"<Reason_and_Act>\" nl \"<scratchpad>\" string nl \"</scratchpad>\" nl \"<reasoning>\" string nl \"</reasoning>\" nl \"<output>\" Reason_and_Act-output-union nl \"</output>\" nl \"</Reason_and_Act>\"\n",
      "Tools ::= nl \"<Tools>\" nl \"<action>\" Toolsaction nl \"</action>\" nl \"<tool_use>\" Tools-tool_use-union nl \"</tool_use>\" nl \"</Tools>\"\n",
      "Toolsaction ::= nl \"tool_use\" \n",
      "calculator ::= nl \"<calculator>\" nl \"<expression>\" string nl \"</expression>\" nl \"<tool_name>\" calculatortool_name nl \"</tool_name>\" nl \"</calculator>\"\n",
      "calculatortool_name ::= nl \"calculator\" \n",
      "search ::= nl \"<search>\" nl \"<query>\" string nl \"</query>\" nl \"<num_results>\" integer nl \"</num_results>\" nl \"<tool_name>\" searchtool_name nl \"</tool_name>\" nl \"</search>\"\n",
      "searchtool_name ::= nl \"search\" \n",
      "Tools-tool_use-union ::= calculator | search\n",
      "FinalAnswer ::= nl \"<FinalAnswer>\" nl \"<action>\" FinalAnsweraction nl \"</action>\" nl \"<answer>\" string nl \"</answer>\" nl \"</FinalAnswer>\"\n",
      "FinalAnsweraction ::= nl \"final_answer\" \n",
      "Reason_and_Act-output-union ::= Tools | FinalAnswer\n",
      "boolean ::= nl \"true\" | nl \"false\"\n",
      "null ::= nl \"null\"\n",
      "string ::= nl [^\\n<] ([^<])*\n",
      "nl ::= \"\\n\"\n",
      "float ::= nl \"-\"? [0-9]+ (\".\" [0-9]+)?\n",
      "integer ::= nl [0-9]+\n",
      "***\n",
      "\n",
      "<Reason_and_Act>\n",
      "<scratchpad>\n",
      "The radius of the moon is approximated at 1,737 kilometers.\n",
      "</scratchpad>\n",
      "<reasoning>\n",
      "To answer the question, I used the information about the average radius of the moon as mentioned in the assumption, without drawing any conclusion from the direct observation.\n",
      "</reasoning>\n",
      "<output>\n",
      "<Tools>\n",
      "<action>\n",
      "tool_use\n",
      "</action>\n",
      "<tool_use>\n",
      "<search>\n",
      "<query>\n",
      "\"radius of the moon\"\n",
      "</query>\n",
      "<num_results>\n",
      "5\n",
      "</num_results>\n",
      "<tool_name>\n",
      "search\n",
      "</tool_name>\n",
      "</search>\n",
      "</tool_use>\n",
      "</Tools>\n",
      "</output>\n",
      "</Reason_and_Act>\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM\n",
    "from vllm.sampling_params import RequestOutputKind\n",
    "\n",
    "if \"llm\" not in globals():  # interactive use\n",
    "    llm = LLM(model=\"Qwen/Qwen2.5-0.5B-Instruct\", max_model_len=2000)\n",
    "\n",
    "env = Env(Reason_and_Act)\n",
    "print(\"***\")\n",
    "print(env.gbnf)\n",
    "print(\"***\")\n",
    "\n",
    "\n",
    "sampling_params = env.sampling_params(\n",
    "    max_tokens=500,\n",
    "    guided_decoding=dict(whitespace_pattern=r\"\"),\n",
    "    n=1,\n",
    "    temperature=1.,\n",
    "    output_kind=RequestOutputKind.FINAL_ONLY,\n",
    ")\n",
    "\n",
    "max_steps = 5\n",
    "messages: list[dict] = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": f\"\"\"\\\n",
    "You are a helpful assistant, responding via JSON structured output. Tool responses from the user are also in JSON.\n",
    "- Think step by step using the scratchpad, reasoning outputs. You have {max_steps - 1} steps to think before responding.\n",
    "- Use the tools provided. DO NOT rely on your own knowledge when a tool is available to help you.\n",
    "- Respond with a final answer once your are sure you have the answer.\n",
    "\n",
    "Respond with a JSON object, following the schema below:\n",
    "\n",
    "{env.doc}\n",
    "\n",
    "\"\"\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"What is the radius of the moon?\"},\n",
    "]\n",
    "\n",
    "outp = llm.chat(  # type: ignore\n",
    "    messages=messages,  # type: ignore\n",
    "    sampling_params=sampling_params,\n",
    "    use_tqdm=False,\n",
    ")\n",
    "print(outp[0].outputs[0].text)\n",
    "\n",
    "\n",
    "# for _ in range(max_steps):\n",
    "#     outp = llm.chat(  # type: ignore\n",
    "#         messages=messages,  # type: ignore\n",
    "#         sampling_params=sampling_params,\n",
    "#         use_tqdm=False,\n",
    "#     )\n",
    "#     struct_res = env.parse(outp[0].outputs[0].text)\n",
    "#     messages.append({\"role\": \"assistant\", \"content\": struct_res.model_dump()})\n",
    "#     tool_outp = run_tools(struct_res)\n",
    "#     if tool_outp:\n",
    "#         messages.append({\"role\": \"user\", \"content\": tool_outp})\n",
    "#     else:\n",
    "#         break\n",
    "\n",
    "# print(json.dumps(messages, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Model: Reason_and_Act\n",
      "  Output Fields:\n",
      "    scratchpad (str):\n",
      "        Description: Information from the Observation useful to answer the question\n",
      "    reasoning (str):\n",
      "        Description: It describes your thoughts about the question you have been asked\n",
      "    output (Tools or FinalAnswer):\n",
      "        Description: Final output: choose between the tool call or the final answer\n",
      "\n",
      "Model: Tools\n",
      "  Description: Run a tool.\n",
      "  Fields:\n",
      "    action (Literal):\n",
      "        Description: Action discriminator\n",
      "    tool_use (calculator or search):\n",
      "        Description: The tool call to use\n",
      "\n",
      "Model: FinalAnswer\n",
      "  Description: Return a final answer.\n",
      "  Fields:\n",
      "    action (Literal):\n",
      "        Description: Action discriminator\n",
      "    answer (str):\n",
      "        Description: The final answer to the question\n",
      "\n",
      "Model: calculator\n",
      "  Description: Evaluates a single line of Python math expression. No imports or variables allowed.\n",
      "\n",
      "    Examples:\n",
      "        {\"expression\": \"2 + 2\"} -> \"4\"\n",
      "        {\"expression\": \"3 * (17 + 4)\"} -> \"63\"\n",
      "        {\"expression\": \"100 / 5\"} -> \"20.0\"\n",
      "    \n",
      "Returns: str - The result of the calculation or an error message\n",
      "  Fields:\n",
      "    expression (str):\n",
      "        Description: A mathematical expression using only numbers and basic operators (+,-,*,/,**,())\n",
      "    tool_name (Literal):\n",
      "        Description: Function to call\n",
      "\n",
      "Model: search\n",
      "  Description: Searches DuckDuckGo and returns concise summaries of top results.\n",
      "\n",
      "    Examples:\n",
      "        {\"query\": \"who invented the lightbulb\", \"num_results\": 3}\n",
      "    \n",
      "Returns: str - Formatted string with bullet points of top results, each with title and brief summary\n",
      "  Fields:\n",
      "    query (str):\n",
      "        Description: The search query string\n",
      "    num_results (int):\n",
      "        Description: Number of results to return (default: 5, max: 10)\n",
      "    tool_name (Literal):\n",
      "        Description: Function to call\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(env.doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n<Reason_and_Act>\\n<scratchpad>\\nTo answer this question, I'll need information about the Earth-Moon system and its known configuration. The Moon's distance from the Earth (perigee) and apogee is about 384, 404 kilometers apart.\\nI also need estimates of the Moon's radius at perigee and apogee:\\n- Perigee radius: about 3,474 kilometers\\n- Apogee radius: about 3,844 kilometers\\nExpectedly, these would give me an estimate for the radius of the Moon once these individual extents were assumed. The median and error estimates are round numbers, so this doesn't help address the specific, easily-supported criteria for the radius.\\nWould you like to scale this into kilometers? Since the usual range for lunar rags is in the tens of kilometers, I'll give it up for now.\\n</scratchpad>\\n<reasoning>\\nThe Moon's radius at apogee is approximately 3,844 kilometers, as mentioned in my initial explanation above. However, the current scope of the question doesn't provide direct access to these numbers or the calculations involved in deriving them.\\n</reasoning>\\n<output>\\n<FinalAnswer>\\n<action>\\nfinal_answer\\n</action>\\n<answer>\\n3,844 kilometers\\n</answer>\\n</FinalAnswer>\\n</output>\\n</Reason_and_Act>\""
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outp[0].outputs[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[RequestOutput(request_id=0, prompt='<|im_start|>system\\nYou are a helpful assistant, responding via JSON structured output. Tool responses from the user are also in JSON.\\n- Think step by step using the scratchpad, reasoning outputs. You have 4 steps to think before responding.\\n- Use the tools provided. DO NOT rely on your own knowledge when a tool is available to help you.\\n- Respond with a final answer once your are sure you have the answer.\\n\\nRespond with a JSON object, following the schema below:\\n\\nOutput Model: Reason_and_Act\\n  Output Fields:\\n    scratchpad (str):\\n        Description: Information from the Observation useful to answer the question\\n    reasoning (str):\\n        Description: It describes your thoughts about the question you have been asked\\n    output (Tools or FinalAnswer):\\n        Description: Final output: choose between the tool call or the final answer\\n\\nModel: Tools\\n  Description: Run a tool.\\n  Fields:\\n    action (Literal):\\n        Description: Action discriminator\\n    tool_use (calculator or search):\\n        Description: The tool call to use\\n\\nModel: FinalAnswer\\n  Description: Return a final answer.\\n  Fields:\\n    action (Literal):\\n        Description: Action discriminator\\n    answer (str):\\n        Description: The final answer to the question\\n\\nModel: calculator\\n  Description: Evaluates a single line of Python math expression. No imports or variables allowed.\\n\\n    Examples:\\n        {\"expression\": \"2 + 2\"} -> \"4\"\\n        {\"expression\": \"3 * (17 + 4)\"} -> \"63\"\\n        {\"expression\": \"100 / 5\"} -> \"20.0\"\\n    \\nReturns: str - The result of the calculation or an error message\\n  Fields:\\n    expression (str):\\n        Description: A mathematical expression using only numbers and basic operators (+,-,*,/,**,())\\n    tool_name (Literal):\\n        Description: Function to call\\n\\nModel: search\\n  Description: Searches DuckDuckGo and returns concise summaries of top results.\\n\\n    Examples:\\n        {\"query\": \"who invented the lightbulb\", \"num_results\": 3}\\n    \\nReturns: str - Formatted string with bullet points of top results, each with title and brief summary\\n  Fields:\\n    query (str):\\n        Description: The search query string\\n    num_results (int):\\n        Description: Number of results to return (default: 5, max: 10)\\n    tool_name (Literal):\\n        Description: Function to call\\n\\n\\n\\n<|im_end|>\\n<|im_start|>user\\nWhat is the radius of the moon?<|im_end|>\\n<|im_start|>assistant\\n', prompt_token_ids=[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 11, 29338, 4566, 4718, 32930, 2550, 13, 13474, 14507, 504, 279, 1196, 525, 1083, 304, 4718, 624, 12, 21149, 3019, 553, 3019, 1667, 279, 18778, 13242, 11, 32711, 16275, 13, 1446, 614, 220, 19, 7354, 311, 1744, 1573, 29338, 624, 12, 5443, 279, 7375, 3897, 13, 9319, 4183, 17188, 389, 697, 1828, 6540, 979, 264, 5392, 374, 2500, 311, 1492, 498, 624, 12, 39533, 448, 264, 1590, 4226, 3055, 697, 525, 2704, 498, 614, 279, 4226, 382, 65354, 448, 264, 4718, 1633, 11, 2701, 279, 10802, 3685, 1447, 5097, 4903, 25, 26759, 8378, 46672, 198, 220, 9258, 24580, 510, 262, 18778, 13242, 320, 495, 982, 286, 7662, 25, 8085, 504, 279, 86429, 5390, 311, 4226, 279, 3405, 198, 262, 32711, 320, 495, 982, 286, 7662, 25, 1084, 16555, 697, 11303, 911, 279, 3405, 498, 614, 1012, 4588, 198, 262, 2550, 320, 16583, 476, 13023, 16141, 982, 286, 7662, 25, 13023, 2550, 25, 5157, 1948, 279, 5392, 1618, 476, 279, 1590, 4226, 271, 1712, 25, 13852, 198, 220, 7662, 25, 6452, 264, 5392, 624, 220, 24580, 510, 262, 1917, 320, 17350, 982, 286, 7662, 25, 5586, 81738, 198, 262, 5392, 15951, 320, 88821, 476, 2711, 982, 286, 7662, 25, 576, 5392, 1618, 311, 990, 271, 1712, 25, 13023, 16141, 198, 220, 7662, 25, 3411, 264, 1590, 4226, 624, 220, 24580, 510, 262, 1917, 320, 17350, 982, 286, 7662, 25, 5586, 81738, 198, 262, 4226, 320, 495, 982, 286, 7662, 25, 576, 1590, 4226, 311, 279, 3405, 271, 1712, 25, 29952, 198, 220, 7662, 25, 75098, 973, 264, 3175, 1555, 315, 13027, 6888, 7493, 13, 2308, 15202, 476, 7332, 5420, 382, 262, 25311, 510, 286, 5212, 28099, 788, 330, 17, 488, 220, 17, 9207, 1464, 330, 19, 698, 286, 5212, 28099, 788, 330, 18, 353, 320, 16, 22, 488, 220, 19, 8, 9207, 1464, 330, 21, 18, 698, 286, 5212, 28099, 788, 330, 16, 15, 15, 608, 220, 20, 9207, 1464, 330, 17, 15, 13, 15, 698, 1066, 16446, 25, 607, 481, 576, 1102, 315, 279, 21937, 476, 458, 1465, 1943, 198, 220, 24580, 510, 262, 7493, 320, 495, 982, 286, 7662, 25, 362, 35972, 7493, 1667, 1172, 5109, 323, 6770, 19624, 17973, 4999, 11, 12314, 34545, 97219, 2398, 262, 5392, 1269, 320, 17350, 982, 286, 7662, 25, 5712, 311, 1618, 271, 1712, 25, 2711, 198, 220, 7662, 25, 81169, 45770, 35, 1942, 10850, 323, 4675, 63594, 68922, 315, 1909, 3059, 382, 262, 25311, 510, 286, 5212, 1631, 788, 330, 14623, 35492, 279, 3100, 16110, 65, 497, 330, 2413, 13576, 788, 220, 18, 532, 1066, 16446, 25, 607, 481, 3377, 12127, 914, 448, 17432, 3501, 315, 1909, 3059, 11, 1817, 448, 2265, 323, 9814, 12126, 198, 220, 24580, 510, 262, 3239, 320, 495, 982, 286, 7662, 25, 576, 2711, 3239, 914, 198, 262, 1629, 13576, 320, 396, 982, 286, 7662, 25, 5624, 315, 3059, 311, 470, 320, 2258, 25, 220, 20, 11, 1932, 25, 220, 16, 15, 340, 262, 5392, 1269, 320, 17350, 982, 286, 7662, 25, 5712, 311, 1618, 1022, 151645, 198, 151644, 872, 198, 3838, 374, 279, 10578, 315, 279, 17788, 30, 151645, 198, 151644, 77091, 198], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=\" <Reason_and_Act>\\\\n<scratchpad>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n</scratchpad>\\n<reasoning>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nThe radius of the moon is approximately 2, 380 kilometers (1, 599 miles). This value is based on observations made by various astronomical bodies and is considered a reliable estimate for the moon's surface area. However, it's important to note that this estimate is an average value and may vary slightly depending on the source of the information. Additionally, the moon's surface is made up of many small craters and mountains, which can affect the way we estimate its size. For the most accurate information, it's recommended to consult a reliable astronomical source or consult a professional astronomer. \\\\n\\n</reasoning>\\n<output>\\n<FinalAnswer>\\\\n<action>\\nfinal_answer\\n</action>\\n<answer>\\nThe radius of the moon is approximately 2, 380 kilometers (1, 599 miles).\\n</answer>\\n</FinalAnswer>\\\\n\\n</output>\\n</Reason_and_Act>\\\\n\", token_ids=(220, 27, 693, 300, 263, 8378, 46672, 29, 59, 77, 27, 54055, 13242, 29, 79083, 79083, 79083, 79083, 24616, 198, 27, 14, 54055, 13242, 29, 198, 27, 19895, 287, 29, 24616, 34583, 785, 10578, 315, 279, 17788, 374, 13187, 220, 17, 11, 220, 18, 23, 15, 40568, 320, 16, 11, 220, 20, 24, 24, 8756, 568, 1096, 897, 374, 3118, 389, 23722, 1865, 553, 5257, 86183, 12866, 323, 374, 6509, 264, 14720, 16045, 369, 279, 17788, 594, 7329, 3082, 13, 4354, 11, 432, 594, 2989, 311, 5185, 429, 419, 16045, 374, 458, 5461, 897, 323, 1231, 13289, 10078, 11649, 389, 279, 2530, 315, 279, 1995, 13, 22406, 11, 279, 17788, 594, 7329, 374, 1865, 705, 315, 1657, 2613, 1560, 11535, 323, 23501, 11, 892, 646, 7802, 279, 1616, 582, 16045, 1181, 1379, 13, 1752, 279, 1429, 13382, 1995, 11, 432, 594, 11102, 311, 8498, 264, 14720, 86183, 2530, 476, 8498, 264, 6584, 45282, 261, 13, 1124, 77, 198, 27, 14, 19895, 287, 29, 198, 27, 3006, 29, 198, 27, 19357, 16141, 29, 59, 77, 27, 1311, 29, 198, 11822, 28534, 198, 522, 1311, 397, 27, 9217, 397, 785, 10578, 315, 279, 17788, 374, 13187, 220, 17, 11, 220, 18, 23, 15, 40568, 320, 16, 11, 220, 20, 24, 24, 8756, 4292, 522, 9217, 29, 198, 522, 19357, 16141, 29, 59, 77, 198, 522, 3006, 29, 198, 522, 25139, 8378, 46672, 29, 59, 77, 151645), cumulative_logprob=None, logprobs=None, finish_reason=stop, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1741703742.5845506, last_token_time=1741703743.915495, first_scheduled_time=1741703742.7351365, first_token_time=1741703743.319905, time_in_queue=0.15058588981628418, finished_time=1741703743.9156013, scheduler_time=0.009481351036811247, model_forward_time=None, model_execute_time=None), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nl <Reason_and_Act>\n",
      "nl <scratchpad>\n",
      "nl {\"expression\": \"pi * r^2 = 1\"} -> \"The radius of the moon is approximately 1.7425698479799956\"\n",
      "nl {\"expression\": \"r = 1.7425698479799956\"} -> \"The radius of the moon is approximately 1.7425698479799956\" \n",
      "nl {\"expression\": \"pi * 1.7425698479799956^2 = 1\"} -> \"True\" \n",
      "nl {\"expression\": \"pi * r^2 = 1\"} -> \"True\" \n",
      "nl {\"expression\": \"r = 1.7425698479799956\"} -> \"True\" \n",
      "nl {\"expression\": \"pi * r^2 = 1\"} -> \"True\" \n",
      "nl {\"expression\": \"r = 1.7425698479799956\"} -> \"True\" \n",
      "nl {\"expression\": \"pi * r^2 = 1\"} -> \"True\" \n",
      "nl {\"expression\": \"r = 1.7425698479799956\"} -> \"True\" \n",
      "nl {\"expression\": \"pi * r^2 = 1\"} -> \"True\" \n",
      "nl {\"expression\": \"r = 1.7425698479799956\"} -> \"True\" \n",
      "nl {\"expression\": \"pi * r^2 = 1\"} -> \"True\" \n",
      "nl {\"expression\": \"r = 1.7425698479799956\"} -> \"True\" \n",
      "nl {\"expression\": \"pi * r^2 = 1\"} -> \"True\" \n",
      "nl {\"expression\": \"r = 1.7425698479799956\"} -> \"True\" \n",
      "nl {\"expression\": \"pi * r^2 = 1\"} -> \"True\" \n",
      "nl {\"expression\": \"\n"
     ]
    }
   ],
   "source": [
    "print(outp[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the cool thing is, this is a Qwen 1.5B model, not even trained on function calling!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
