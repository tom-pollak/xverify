{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from datasets import load_dataset, Dataset\n",
    "from peft import LoraConfig\n",
    "\n",
    "import xverify as xv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Tools(BaseModel):\n",
    "    \"\"\"\n",
    "    Run a tool.\n",
    "    \"\"\"\n",
    "    tool_use: xv.XMLToolUse[xv.calculator, xv.search] = Field(..., description=\"The tool call to use\")\n",
    "\n",
    "\n",
    "class FinalAnswer(BaseModel):\n",
    "    \"\"\"\n",
    "    Return a final answer.\n",
    "    \"\"\"\n",
    "    answer: str = Field(..., description=\"The final answer to the question\")\n",
    "\n",
    "\n",
    "class Reason_and_Act(BaseModel):\n",
    "    scratchpad: str = Field(\n",
    "        ...,\n",
    "        description=\"Information from the Observation useful to answer the question\",\n",
    "    )\n",
    "    reasoning: str = Field(\n",
    "        ...,\n",
    "        description=\"It describes your thoughts about the question you have been asked\",\n",
    "    )\n",
    "    response: Tools | FinalAnswer\n",
    "\n",
    "def tool_response_func(model: Reason_and_Act) -> dict | None:\n",
    "    return xv.run_tools(model.response)\n",
    "\n",
    "guided_schema = xv.GuidedSchema(Reason_and_Act, schema=\"xml\", tool_response_func=tool_response_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def format_prompt(prompt: str, system_prompt: str | None = None) -> list[dict[str, str]]:\n",
    "    messages = []\n",
    "    if system_prompt:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    return messages\n",
    "\n",
    "def extract_hash_answer(text: str) -> str | None:\n",
    "    if \"####\" not in text:\n",
    "        return None\n",
    "    return text.split(\"####\")[1].strip()\n",
    "\n",
    "SYSTEM_PROMPT = f\"\"\"\\\n",
    "You are a helpful assistant, responding in XML structured output.\n",
    "\n",
    "- Think step by step using the scratchpad and reasoning outputs. You have {guided_schema.max_steps - 1} steps to think before responding.\n",
    "- Use the tools provided. DO NOT rely on your own knowledge when a tool is available to help you.\n",
    "- Respond with a final answer only once your are absolutely sure you have the answer.\n",
    "\n",
    "Respond with a XML object, following the schema below:\n",
    "\n",
    "{guided_schema.doc}\n",
    "\"\"\"\n",
    "\n",
    "CODE_PROMPT = \"\"\"\\\n",
    "Given a math problem, use step-by-step reasoning and code execution to solve the problem.\n",
    "\n",
    "For each step:\n",
    "1. Think through your reasoning inside <reasoning> tags\n",
    "2. Write Python scripts inside <code> tags to work out calculations\n",
    "   - Functions and variables do not persist across <code> calls and should be redefined each time\n",
    "   - Scripts should be written in Python 3.10+ syntax, and should run in under 10 seconds\n",
    "   - Any desired outputs should be printed using print() statements\n",
    "   - You may import numpy, scipy, and sympy libraries for your calculations\n",
    "3. You will see the output from print() statements in your code in <output> tags\n",
    "4. Continue until you can give the final answer inside <answer> tags\n",
    "\"\"\"\n",
    "\n",
    "dataset: Dataset = load_dataset(\"openai/gsm8k\", \"main\", split=\"train\") # type: ignore\n",
    "dataset = dataset.map(lambda x: {\n",
    "    \"prompt\": format_prompt(x[\"question\"], CODE_PROMPT),\n",
    "    \"answer\": extract_hash_answer(x[\"answer\"])\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Liger kernel\n",
      "Applied Liger kernels to Qwen2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "No label_names provided for model class `PeftModel`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "/home/tom/fun/xverify/.venv/lib/python3.12/site-packages/trl/trainer/grpo_trainer.py:471: UserWarning: The requested device cuda:0 is also being used for training. For higher throughput and to avoid out-of-memory errors, it is recommended to use a dedicated device for vLLM. If this is intentional, you may ignore this warning but should adjust `vllm_gpu_memory_utilization` accordingly.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-13 10:30:01 __init__.py:207] Automatically detected platform cuda.\n",
      "INFO 03-13 10:30:06 config.py:549] This model supports multiple tasks: {'generate', 'embed', 'classify', 'score', 'reward'}. Defaulting to 'generate'.\n",
      "INFO 03-13 10:30:06 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='Qwen/Qwen2.5-1.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-1.5B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 03-13 10:30:07 cuda.py:229] Using Flash Attention backend.\n",
      "INFO 03-13 10:30:07 model_runner.py:1110] Starting to load model Qwen/Qwen2.5-1.5B-Instruct...\n",
      "INFO 03-13 10:30:08 weight_utils.py:254] Using model weights format ['*.safetensors']\n",
      "INFO 03-13 10:30:08 weight_utils.py:304] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4cbff6b54e943e19233e66e2f1be410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-13 10:30:08 model_runner.py:1115] Loading model weights took 2.8875 GB\n",
      "INFO 03-13 10:30:10 worker.py:267] Memory profiling takes 1.70 seconds\n",
      "INFO 03-13 10:30:10 worker.py:267] the current vLLM instance can use total_gpu_memory (23.58GiB) x gpu_memory_utilization (0.30) = 7.07GiB\n",
      "INFO 03-13 10:30:10 worker.py:267] model weights take 2.89GiB; non_torch_memory takes 0.06GiB; PyTorch activation peak memory takes 2.02GiB; the rest of the memory reserved for KV Cache is 2.11GiB.\n",
      "INFO 03-13 10:30:11 executor_base.py:111] # cuda blocks: 4929, # CPU blocks: 9362\n",
      "INFO 03-13 10:30:11 executor_base.py:116] Maximum concurrency for 32768 tokens per request: 2.41x\n",
      "INFO 03-13 10:30:14 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:15<00:00,  2.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-13 10:30:30 model_runner.py:1562] Graph capturing finished in 15 secs, took 0.20 GiB\n",
      "INFO 03-13 10:30:30 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 21.44 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtompollak\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tom/fun/xverify/examples/wandb/run-20250313_103031-ypbakfop</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tompollak/huggingface/runs/ypbakfop' target=\"_blank\">gsm8k-calculator-peft_qwen2.5-1.5b-instruct</a></strong> to <a href='https://wandb.ai/tompollak/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tompollak/huggingface' target=\"_blank\">https://wandb.ai/tompollak/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tompollak/huggingface/runs/ypbakfop' target=\"_blank\">https://wandb.ai/tompollak/huggingface/runs/ypbakfop</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tom/fun/xverify/.venv/lib/python3.12/site-packages/outlines/fsm/guide.py:110: UserWarning: Outlines' public *community-contributed* CFG structured generation is experimental. Please review https://dottxt-ai.github.io/outlines/latest/reference/generation/cfg#disclaimer\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "GrammarError",
     "evalue": "Unexpected colon, at line 1 column 7\n\nroot ::= grammar-models\n      ^\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/lark/parsers/lalr_parser_state.py:77\u001b[39m, in \u001b[36mParserState.feed_token\u001b[39m\u001b[34m(self, token, is_end)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     action, arg = \u001b[43mstates\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtype\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[31mKeyError\u001b[39m: '_COLON'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mUnexpectedToken\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/lark/load_grammar.py:966\u001b[39m, in \u001b[36m_parse_grammar\u001b[39m\u001b[34m(text, name, start)\u001b[39m\n\u001b[32m    965\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m966\u001b[39m     tree = \u001b[43m_get_parser\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m UnexpectedCharacters \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/lark/parser_frontends.py:104\u001b[39m, in \u001b[36mParsingFrontend.parse\u001b[39m\u001b[34m(self, text, start, on_error)\u001b[39m\n\u001b[32m    103\u001b[39m stream = \u001b[38;5;28mself\u001b[39m._make_lexer_thread(text)\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchosen_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/lark/parsers/lalr_parser.py:42\u001b[39m, in \u001b[36mLALR_Parser.parse\u001b[39m\u001b[34m(self, lexer, start, on_error)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m UnexpectedInput \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/lark/parsers/lalr_parser.py:88\u001b[39m, in \u001b[36m_Parser.parse\u001b[39m\u001b[34m(self, lexer, start, value_stack, state_stack, start_interactive)\u001b[39m\n\u001b[32m     87\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m InteractiveParser(\u001b[38;5;28mself\u001b[39m, parser_state, parser_state.lexer)\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparse_from_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparser_state\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/lark/parsers/lalr_parser.py:111\u001b[39m, in \u001b[36m_Parser.parse_from_state\u001b[39m\u001b[34m(self, state, last_token)\u001b[39m\n\u001b[32m    110\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/lark/parsers/lalr_parser.py:102\u001b[39m, in \u001b[36m_Parser.parse_from_state\u001b[39m\u001b[34m(self, state, last_token)\u001b[39m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m     \u001b[43mstate\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfeed_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    104\u001b[39m end_token = Token.new_borrow_pos(\u001b[33m'\u001b[39m\u001b[33m$END\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m, token) \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;28;01melse\u001b[39;00m Token(\u001b[33m'\u001b[39m\u001b[33m$END\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/lark/parsers/lalr_parser_state.py:80\u001b[39m, in \u001b[36mParserState.feed_token\u001b[39m\u001b[34m(self, token, is_end)\u001b[39m\n\u001b[32m     79\u001b[39m     expected = {s \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m states[state].keys() \u001b[38;5;28;01mif\u001b[39;00m s.isupper()}\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m UnexpectedToken(token, expected, state=\u001b[38;5;28mself\u001b[39m, interactive_parser=\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m arg != end_state\n",
      "\u001b[31mUnexpectedToken\u001b[39m: Unexpected token Token('_COLON', ':') at line 1, column 7.\nExpected one of: \n\t* STRING\n\t* RULE\n\t* REGEXP\n\t* _NL\n\t* _LBRA\n\t* _OR\n\t* _NL_OR\n\t* TERMINAL\n\t* _LPAR\n\t* _TO\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mGrammarError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [_check_answer(c, a) \u001b[38;5;28;01mfor\u001b[39;00m c, a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(completions, answers)]\n\u001b[32m     26\u001b[39m trainer = xv.GRPOGuidedTrainer(\n\u001b[32m     27\u001b[39m     guided_schema=guided_schema,\n\u001b[32m     28\u001b[39m     model=model,\n\u001b[32m   (...)\u001b[39m\u001b[32m     32\u001b[39m     reward_funcs=[exact_answer_reward_func],\n\u001b[32m     33\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/transformers/trainer.py:2241\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2239\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2240\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2242\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2243\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2244\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2245\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/transformers/trainer.py:2548\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2541\u001b[39m context = (\n\u001b[32m   2542\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2543\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2544\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2545\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2546\u001b[39m )\n\u001b[32m   2547\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2548\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2550\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2551\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2552\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2553\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2554\u001b[39m ):\n\u001b[32m   2555\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2556\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/transformers/trainer.py:3692\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3689\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.optimizer, \u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m.optimizer.train):\n\u001b[32m   3690\u001b[39m     \u001b[38;5;28mself\u001b[39m.optimizer.train()\n\u001b[32m-> \u001b[39m\u001b[32m3692\u001b[39m inputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3693\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[32m   3694\u001b[39m     loss_mb = smp_forward_backward(model, inputs, \u001b[38;5;28mself\u001b[39m.args.gradient_accumulation_steps)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/trl/extras/profiling.py:87\u001b[39m, in \u001b[36mprofiling_decorator.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m     86\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m profiling_context(\u001b[38;5;28mself\u001b[39m, func.\u001b[34m__name__\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/trl/trainer/grpo_trainer.py:706\u001b[39m, in \u001b[36mGRPOTrainer._prepare_inputs\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    704\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mode == \u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    705\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.num_iterations == \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m706\u001b[39m         inputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_and_score_completions\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    707\u001b[39m         \u001b[38;5;28mself\u001b[39m._buffered_inputs[\u001b[38;5;28mself\u001b[39m._step % \u001b[38;5;28mself\u001b[39m.args.gradient_accumulation_steps] = inputs\n\u001b[32m    708\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/trl/trainer/grpo_trainer.py:747\u001b[39m, in \u001b[36mGRPOTrainer._generate_and_score_completions\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    745\u001b[39m ordered_set_of_prompts = all_prompts_text[:: \u001b[38;5;28mself\u001b[39m.num_generations]\n\u001b[32m    746\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m profiling_context(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mvLLM.generate\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m747\u001b[39m     all_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    748\u001b[39m \u001b[43m        \u001b[49m\u001b[43mordered_set_of_prompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_params\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msampling_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m    749\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    750\u001b[39m completion_ids = []\n\u001b[32m    751\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m outputs \u001b[38;5;129;01min\u001b[39;00m all_outputs:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/src/xverify/trainers/grpo_guided_trainer.py:68\u001b[39m, in \u001b[36mGRPOGuidedTrainer.generate\u001b[39m\u001b[34m(self, prompts, sampling_params, **kwargs)\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate\u001b[39m(\u001b[38;5;28mself\u001b[39m, prompts, sampling_params, **kwargs) -> \u001b[38;5;28mlist\u001b[39m[RequestOutput]:\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43msampling_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/vllm/utils.py:1057\u001b[39m, in \u001b[36mdeprecate_kwargs.<locals>.wrapper.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1050\u001b[39m             msg += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1052\u001b[39m         warnings.warn(\n\u001b[32m   1053\u001b[39m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(msg),\n\u001b[32m   1054\u001b[39m             stacklevel=\u001b[32m3\u001b[39m,  \u001b[38;5;66;03m# The inner function takes up one level\u001b[39;00m\n\u001b[32m   1055\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1057\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/vllm/entrypoints/llm.py:461\u001b[39m, in \u001b[36mLLM.generate\u001b[39m\u001b[34m(self, prompts, sampling_params, prompt_token_ids, use_tqdm, lora_request, prompt_adapter_request, guided_options_request, priority)\u001b[39m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sampling_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    458\u001b[39m     \u001b[38;5;66;03m# Use default sampling params.\u001b[39;00m\n\u001b[32m    459\u001b[39m     sampling_params = \u001b[38;5;28mself\u001b[39m.get_default_sampling_params()\n\u001b[32m--> \u001b[39m\u001b[32m461\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_and_add_requests\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparsed_prompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43msampling_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt_adapter_request\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt_adapter_request\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m    \u001b[49m\u001b[43mguided_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mguided_options_request\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpriority\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpriority\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    469\u001b[39m outputs = \u001b[38;5;28mself\u001b[39m._run_engine(use_tqdm=use_tqdm)\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.engine_class.validate_outputs(outputs, RequestOutput)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/vllm/entrypoints/llm.py:1330\u001b[39m, in \u001b[36mLLM._validate_and_add_requests\u001b[39m\u001b[34m(self, prompts, params, lora_request, prompt_adapter_request, guided_options, priority)\u001b[39m\n\u001b[32m   1328\u001b[39m \u001b[38;5;66;03m# Add requests to the engine.\u001b[39;00m\n\u001b[32m   1329\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, prompt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(prompts):\n\u001b[32m-> \u001b[39m\u001b[32m1330\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_add_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSequence\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1333\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m   1334\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSequence\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1335\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt_adapter_request\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt_adapter_request\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1336\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpriority\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpriority\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpriority\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1337\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/vllm/entrypoints/llm.py:1348\u001b[39m, in \u001b[36mLLM._add_request\u001b[39m\u001b[34m(self, prompt, params, lora_request, prompt_adapter_request, priority)\u001b[39m\n\u001b[32m   1339\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_add_request\u001b[39m(\n\u001b[32m   1340\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1341\u001b[39m     prompt: PromptType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1345\u001b[39m     priority: \u001b[38;5;28mint\u001b[39m = \u001b[32m0\u001b[39m,\n\u001b[32m   1346\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1347\u001b[39m     request_id = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m.request_counter))\n\u001b[32m-> \u001b[39m\u001b[32m1348\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1349\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1350\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1351\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1352\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1353\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt_adapter_request\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt_adapter_request\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1354\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpriority\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpriority\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1355\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/vllm/utils.py:1057\u001b[39m, in \u001b[36mdeprecate_kwargs.<locals>.wrapper.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1050\u001b[39m             msg += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1052\u001b[39m         warnings.warn(\n\u001b[32m   1053\u001b[39m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(msg),\n\u001b[32m   1054\u001b[39m             stacklevel=\u001b[32m3\u001b[39m,  \u001b[38;5;66;03m# The inner function takes up one level\u001b[39;00m\n\u001b[32m   1055\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1057\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/vllm/engine/llm_engine.py:764\u001b[39m, in \u001b[36mLLMEngine.add_request\u001b[39m\u001b[34m(self, request_id, prompt, params, arrival_time, lora_request, trace_headers, prompt_adapter_request, priority, inputs)\u001b[39m\n\u001b[32m    756\u001b[39m preprocessed_inputs = \u001b[38;5;28mself\u001b[39m.input_preprocessor.preprocess(\n\u001b[32m    757\u001b[39m     prompt,\n\u001b[32m    758\u001b[39m     request_id=request_id,\n\u001b[32m    759\u001b[39m     lora_request=lora_request,\n\u001b[32m    760\u001b[39m     prompt_adapter_request=prompt_adapter_request,\n\u001b[32m    761\u001b[39m )\n\u001b[32m    762\u001b[39m processed_inputs = \u001b[38;5;28mself\u001b[39m.input_processor(preprocessed_inputs)\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_add_processed_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    765\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    766\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprocessed_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprocessed_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    767\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    768\u001b[39m \u001b[43m    \u001b[49m\u001b[43marrival_time\u001b[49m\u001b[43m=\u001b[49m\u001b[43marrival_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    769\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    770\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt_adapter_request\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt_adapter_request\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    771\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrace_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrace_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    772\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpriority\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpriority\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    773\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/vllm/engine/llm_engine.py:565\u001b[39m, in \u001b[36mLLMEngine._add_processed_request\u001b[39m\u001b[34m(self, request_id, processed_inputs, params, arrival_time, lora_request, prompt_adapter_request, trace_headers, priority)\u001b[39m\n\u001b[32m    561\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Add a processed request to the engine's request pool.\u001b[39;00m\n\u001b[32m    562\u001b[39m \u001b[33;03mreturn the created sequence group.\u001b[39;00m\n\u001b[32m    563\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    564\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(params, SamplingParams) \u001b[38;5;129;01mand\u001b[39;00m params.n > \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m565\u001b[39m     \u001b[43mParallelSampleSequenceGroup\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    566\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    568\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    569\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprocessed_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprocessed_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    570\u001b[39m \u001b[43m        \u001b[49m\u001b[43marrival_time\u001b[49m\u001b[43m=\u001b[49m\u001b[43marrival_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    571\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrace_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrace_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt_adapter_request\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt_adapter_request\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpriority\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpriority\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    575\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    576\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    578\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_model_inputs(processed_inputs, lora_request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/vllm/sequence.py:1421\u001b[39m, in \u001b[36mParallelSampleSequenceGroup.add_request\u001b[39m\u001b[34m(request_id, engine, params, **kwargs)\u001b[39m\n\u001b[32m   1419\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m params.seed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1420\u001b[39m     params.seed += i\n\u001b[32m-> \u001b[39m\u001b[32m1421\u001b[39m seq_group = \u001b[43mengine\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_add_processed_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1422\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest_id_i\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1423\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1424\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1425\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m   1426\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m seq_group \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1427\u001b[39m engine.seq_id_to_seq_group[request_id_i] = group\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/vllm/engine/llm_engine.py:600\u001b[39m, in \u001b[36mLLMEngine._add_processed_request\u001b[39m\u001b[34m(self, request_id, processed_inputs, params, arrival_time, lora_request, prompt_adapter_request, trace_headers, priority)\u001b[39m\n\u001b[32m    598\u001b[39m \u001b[38;5;66;03m# Create a SequenceGroup based on SamplingParams or PoolingParams\u001b[39;00m\n\u001b[32m    599\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(params, SamplingParams):\n\u001b[32m--> \u001b[39m\u001b[32m600\u001b[39m     seq_group = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_sequence_group_with_sampling\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    602\u001b[39m \u001b[43m        \u001b[49m\u001b[43mseq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    603\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    604\u001b[39m \u001b[43m        \u001b[49m\u001b[43marrival_time\u001b[49m\u001b[43m=\u001b[49m\u001b[43marrival_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    606\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrace_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrace_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt_adapter_request\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt_adapter_request\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    608\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_seq\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_seq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    609\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpriority\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpriority\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    610\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(params, PoolingParams):\n\u001b[32m    611\u001b[39m     seq_group = \u001b[38;5;28mself\u001b[39m._create_sequence_group_with_pooling(\n\u001b[32m    612\u001b[39m         request_id,\n\u001b[32m    613\u001b[39m         seq,\n\u001b[32m   (...)\u001b[39m\u001b[32m    618\u001b[39m         encoder_seq=encoder_seq,\n\u001b[32m    619\u001b[39m         priority=priority)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/vllm/engine/llm_engine.py:817\u001b[39m, in \u001b[36mLLMEngine._create_sequence_group_with_sampling\u001b[39m\u001b[34m(self, request_id, seq, sampling_params, arrival_time, lora_request, trace_headers, prompt_adapter_request, encoder_seq, priority)\u001b[39m\n\u001b[32m    810\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (sampling_params.logprobs\n\u001b[32m    811\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m sampling_params.logprobs > max_logprobs) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m    812\u001b[39m             sampling_params.prompt_logprobs\n\u001b[32m    813\u001b[39m             \u001b[38;5;129;01mand\u001b[39;00m sampling_params.prompt_logprobs > max_logprobs):\n\u001b[32m    814\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot request more than \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    815\u001b[39m                      \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_logprobs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m logprobs.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m817\u001b[39m sampling_params = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_build_logits_processors\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    818\u001b[39m \u001b[43m    \u001b[49m\u001b[43msampling_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    820\u001b[39m \u001b[38;5;66;03m# Defensive copy of SamplingParams, which are used by the sampler,\u001b[39;00m\n\u001b[32m    821\u001b[39m \u001b[38;5;66;03m# this doesn't deep-copy LogitsProcessor objects\u001b[39;00m\n\u001b[32m    822\u001b[39m sampling_params = sampling_params.clone()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/vllm/engine/llm_engine.py:1995\u001b[39m, in \u001b[36mLLMEngine._build_logits_processors\u001b[39m\u001b[34m(self, sampling_params, lora_request)\u001b[39m\n\u001b[32m   1991\u001b[39m tokenizer = \u001b[38;5;28mself\u001b[39m.get_tokenizer(lora_request=lora_request)\n\u001b[32m   1992\u001b[39m guided_decoding.backend = guided_decoding.backend \u001b[38;5;129;01mor\u001b[39;00m \\\n\u001b[32m   1993\u001b[39m     \u001b[38;5;28mself\u001b[39m.decoding_config.guided_decoding_backend\n\u001b[32m-> \u001b[39m\u001b[32m1995\u001b[39m processor = \u001b[43mget_local_guided_decoding_logits_processor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1996\u001b[39m \u001b[43m    \u001b[49m\u001b[43mguided_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mguided_decoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1997\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1998\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1999\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m processor:\n\u001b[32m   2000\u001b[39m     logits_processors.append(processor)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/vllm/model_executor/guided_decoding/__init__.py:134\u001b[39m, in \u001b[36mget_local_guided_decoding_logits_processor\u001b[39m\u001b[34m(guided_params, tokenizer, model_config)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m guided_params.backend == \u001b[33m'\u001b[39m\u001b[33moutlines\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    131\u001b[39m     \u001b[38;5;66;03m# NOTE: lazy import outlines to avoid https://github.com/vllm-project/vllm/issues/4193\u001b[39;00m\n\u001b[32m    132\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_executor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mguided_decoding\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moutlines_decoding\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[32m    133\u001b[39m         get_local_outlines_guided_decoding_logits_processor)\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_local_outlines_guided_decoding_logits_processor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m        \u001b[49m\u001b[43mguided_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m guided_params.backend == \u001b[33m'\u001b[39m\u001b[33mlm-format-enforcer\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    137\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_executor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mguided_decoding\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlm_format_enforcer_decoding\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[32m    138\u001b[39m         get_local_lm_format_enforcer_guided_decoding_logits_processor)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/vllm/model_executor/guided_decoding/outlines_decoding.py:102\u001b[39m, in \u001b[36mget_local_outlines_guided_decoding_logits_processor\u001b[39m\u001b[34m(guided_params, tokenizer)\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m guide \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode:\n\u001b[32m    100\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_logits_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mguide\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mguided_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwhitespace_pattern\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/vllm/model_executor/guided_decoding/outlines_decoding.py:142\u001b[39m, in \u001b[36m_get_logits_processor\u001b[39m\u001b[34m(guide, tokenizer, mode, whitespace_pattern)\u001b[39m\n\u001b[32m    140\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m RegexLogitsProcessor(guide, tokenizer)\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m mode == GuidedDecodingMode.GRAMMAR:\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mCFGLogitsProcessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mguide\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    144\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnknown guided decoding mode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/vllm/model_executor/guided_decoding/outlines_logits_processors.py:185\u001b[39m, in \u001b[36mCFGLogitsProcessor.__init__\u001b[39m\u001b[34m(self, cfg, tokenizer)\u001b[39m\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, cfg: \u001b[38;5;28mstr\u001b[39m, tokenizer: PreTrainedTokenizerBase):\n\u001b[32m    175\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Compile the FSM that drives the context free grammar generation.\u001b[39;00m\n\u001b[32m    176\u001b[39m \n\u001b[32m    177\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    183\u001b[39m \n\u001b[32m    184\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[43mCFGLogitsProcessor\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_guide\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    186\u001b[39m     \u001b[38;5;28mself\u001b[39m._guide = \u001b[38;5;28mself\u001b[39m._guide.copy()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/outlines/caching.py:122\u001b[39m, in \u001b[36mcache.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m result = wrapper.__memory__.get(cache_key, default=ENOVAL, retry=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m ENOVAL:\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     result = \u001b[43mcached_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m     wrapper.__memory__.set(cache_key, result, expire, retry=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/vllm/model_executor/guided_decoding/outlines_logits_processors.py:172\u001b[39m, in \u001b[36mCFGLogitsProcessor._get_guide\u001b[39m\u001b[34m(cls, cfg, tokenizer)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    169\u001b[39m \u001b[38;5;129m@cache\u001b[39m()\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_guide\u001b[39m(\u001b[38;5;28mcls\u001b[39m, cfg: \u001b[38;5;28mstr\u001b[39m, tokenizer: PreTrainedTokenizerBase) -> Guide:\n\u001b[32m    171\u001b[39m     tokenizer = _adapt_tokenizer(tokenizer)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mCFGGuide\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/outlines/fsm/guide.py:118\u001b[39m, in \u001b[36mCFGGuide.__init__\u001b[39m\u001b[34m(self, cfg_string, tokenizer)\u001b[39m\n\u001b[32m    116\u001b[39m \u001b[38;5;28mself\u001b[39m.tokenizer = tokenizer\n\u001b[32m    117\u001b[39m \u001b[38;5;28mself\u001b[39m.eos_token_id = \u001b[38;5;28mself\u001b[39m.tokenizer.eos_token_id\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m \u001b[38;5;28mself\u001b[39m.parser = \u001b[43mPartialLark\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcfg_string\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparser\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlalr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimport_paths\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgrammars\u001b[49m\u001b[43m.\u001b[49m\u001b[43mGRAMMAR_PATH\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[38;5;28mself\u001b[39m.initial_state = CFGState(\n\u001b[32m    124\u001b[39m     parser_state=\u001b[38;5;28mself\u001b[39m.parser.parse(\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m), prev_token=\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    125\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/outlines/fsm/parsing.py:106\u001b[39m, in \u001b[36mPartialLark.__init__\u001b[39m\u001b[34m(self, grammar, **options)\u001b[39m\n\u001b[32m    104\u001b[39m \u001b[38;5;28mself\u001b[39m.use_value_stack = options.pop(\u001b[33m\"\u001b[39m\u001b[33muse_value_stack\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    105\u001b[39m options[\u001b[33m\"\u001b[39m\u001b[33mregex\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgrammar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.options.parser == \u001b[33m\"\u001b[39m\u001b[33mlalr\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/lark/lark.py:357\u001b[39m, in \u001b[36mLark.__init__\u001b[39m\u001b[34m(self, grammar, **options)\u001b[39m\n\u001b[32m    353\u001b[39m             \u001b[38;5;28mself\u001b[39m.options = old_options\n\u001b[32m    356\u001b[39m     \u001b[38;5;66;03m# Parse the grammar file and compose the grammars\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m357\u001b[39m     \u001b[38;5;28mself\u001b[39m.grammar, used_files = \u001b[43mload_grammar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrammar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msource_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkeep_all_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    359\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(grammar, Grammar)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/lark/load_grammar.py:1415\u001b[39m, in \u001b[36mload_grammar\u001b[39m\u001b[34m(grammar, source, import_paths, global_keep_all_tokens)\u001b[39m\n\u001b[32m   1413\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_grammar\u001b[39m(grammar, source, import_paths, global_keep_all_tokens):\n\u001b[32m   1414\u001b[39m     builder = GrammarBuilder(global_keep_all_tokens, import_paths)\n\u001b[32m-> \u001b[39m\u001b[32m1415\u001b[39m     \u001b[43mbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_grammar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrammar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1416\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m builder.build(), builder.used_files\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/lark/load_grammar.py:1240\u001b[39m, in \u001b[36mGrammarBuilder.load_grammar\u001b[39m\u001b[34m(self, grammar_text, grammar_name, mangle)\u001b[39m\n\u001b[32m   1239\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_grammar\u001b[39m(\u001b[38;5;28mself\u001b[39m, grammar_text: \u001b[38;5;28mstr\u001b[39m, grammar_name: \u001b[38;5;28mstr\u001b[39m=\u001b[33m\"\u001b[39m\u001b[33m<?>\u001b[39m\u001b[33m\"\u001b[39m, mangle: Optional[Callable[[\u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28mstr\u001b[39m]]=\u001b[38;5;28;01mNone\u001b[39;00m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1240\u001b[39m     tree = \u001b[43m_parse_grammar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrammar_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrammar_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1242\u001b[39m     imports: Dict[Tuple[\u001b[38;5;28mstr\u001b[39m, ...], Tuple[Optional[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]]] = {}\n\u001b[32m   1244\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m stmt \u001b[38;5;129;01min\u001b[39;00m tree.children:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/lark/load_grammar.py:975\u001b[39m, in \u001b[36m_parse_grammar\u001b[39m\u001b[34m(text, name, start)\u001b[39m\n\u001b[32m    973\u001b[39m     error = _translate_parser_exception(_get_parser().parse, e)\n\u001b[32m    974\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m error:\n\u001b[32m--> \u001b[39m\u001b[32m975\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m GrammarError(\u001b[33m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m, at line \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m column \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m % (error, e.line, e.column, context))\n\u001b[32m    976\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    978\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m PrepareGrammar().transform(tree)\n",
      "\u001b[31mGrammarError\u001b[39m: Unexpected colon, at line 1 column 7\n\nroot ::= grammar-models\n      ^\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "model, tokenizer = xv.get_model_and_tokenizer(model_name)\n",
    "\n",
    "run_name = \"gsm8k-calculator-peft_\" + model_name.split(\"/\")[-1].lower()\n",
    "training_args = xv.get_default_grpo_config(run_name, num_gpus=1)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"]\n",
    ")\n",
    "\n",
    "def exact_answer_reward_func(completions, answers, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward function that checks if the final answer matches the expected answer.\"\"\"\n",
    "    def _check_answer(trajectory: list[dict[str, str]], answer: int) -> float:\n",
    "        \"\"\"Extract the last answer from a trajectory.\"\"\"\n",
    "        last_message = trajectory[-1]\n",
    "        assert last_message['role'] == 'assistant', \"should be assistant\"\n",
    "        parsed: Reason_and_Act = guided_schema.parse(last_message['content']) # type: ignore\n",
    "        if parsed is None or not isinstance(parsed.response, FinalAnswer):\n",
    "            return 0.\n",
    "        return 1. if parsed.response.answer == answer else 0.\n",
    "    return [_check_answer(c, a) for c, a in zip(completions, answers)]\n",
    "\n",
    "trainer = xv.GRPOGuidedTrainer(\n",
    "    guided_schema=guided_schema,\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    reward_funcs=[exact_answer_reward_func],\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root ::= grammar-models\n",
      "string ::= nl [^\\n<] ([^<])*\n",
      "boolean ::= nl \"true\" | nl \"false\"\n",
      "integer ::= nl [0-9]+\n",
      "float ::= nl \"-\"? [0-9]+ (\".\" [0-9]+)?\n",
      "null ::= nl \"null\"\n",
      "nl ::= \"\\n\"\n",
      "Reason_and_Act ::=  \"<Reason_and_Act>\" nl \"<scratchpad>\" string nl \"</scratchpad>\" nl \"<reasoning>\" string nl \"</reasoning>\" nl \"<response>\" string nl \"</response>\" nl \"</Reason_and_Act>\"\n",
      "grammar-models ::= Reason_and_Act\n"
     ]
    }
   ],
   "source": [
    "print(guided_schema.gbnf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
