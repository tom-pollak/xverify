{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "from datasets import Dataset, load_dataset\n",
    "from peft import LoraConfig  # type: ignore\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "import xverify as xv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tools(BaseModel):\n",
    "    \"\"\"\n",
    "    Run a tool.\n",
    "    \"\"\"\n",
    "\n",
    "    tool_use: xv.XMLToolUse[xv.calculator, xv.search] = Field(\n",
    "        ..., description=\"The tool call to use\"\n",
    "    )\n",
    "\n",
    "\n",
    "class FinalAnswer(BaseModel):\n",
    "    \"\"\"\n",
    "    Return a final answer.\n",
    "    \"\"\"\n",
    "\n",
    "    answer: int = Field(..., description=\"Final answer to the question\")\n",
    "\n",
    "\n",
    "class Reason_and_Act(BaseModel):\n",
    "    scratchpad: str = Field(\n",
    "        ...,\n",
    "        description=\"Information from the Observation useful to answer the question\",\n",
    "    )\n",
    "    reasoning: str = Field(\n",
    "        ...,\n",
    "        description=\"It describes your thoughts about the question you have been asked\",\n",
    "    )\n",
    "    # response: Tools | FinalAnswer # this still doesn't work\n",
    "    response: Union[Tools, FinalAnswer] = Field(..., description=\"Current step response\")\n",
    "\n",
    "\n",
    "def tool_response_func(model: Reason_and_Act) -> dict | None:\n",
    "    return xv.run_tools(model.response)\n",
    "\n",
    "\n",
    "guided_schema = xv.GuidedSchema(\n",
    "    Reason_and_Act,\n",
    "    schema=\"xml\",\n",
    "    tool_response_func=tool_response_func,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(\n",
    "    prompt: str, system_prompt: str | None = None\n",
    ") -> list[dict[str, str]]:\n",
    "    messages = []\n",
    "    if system_prompt:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    return messages\n",
    "\n",
    "\n",
    "def extract_hash_answer(text: str) -> str | None:\n",
    "    if \"####\" not in text:\n",
    "        return None\n",
    "    return text.split(\"####\")[1].strip()\n",
    "\n",
    "max_steps = 10\n",
    "\n",
    "SYSTEM_PROMPT = f\"\"\"\\\n",
    "You are a helpful assistant, responding in XML structured output.\n",
    "\n",
    "- Think step by step using the scratchpad and reasoning outputs. You have {max_steps - 1} steps to think before responding.\n",
    "- Use the tools provided. DO NOT rely on your own knowledge when a tool is available to help you.\n",
    "- Respond with a final answer only once your are absolutely sure you have the answer.\n",
    "\n",
    "Respond with a XML object, following the schema below:\n",
    "\n",
    "{guided_schema.doc}\n",
    "\"\"\"\n",
    "\n",
    "dataset: Dataset = load_dataset(\"openai/gsm8k\", \"main\", split=\"train\")  # type: ignore\n",
    "dataset = dataset.map(lambda x: {\n",
    "    'prompt': [\n",
    "        {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "        {'role': 'user', 'content': x['question']}\n",
    "    ],\n",
    "    'answer': extract_hash_answer(x['answer'])\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Liger kernel\n",
      "Applied Liger kernels to Qwen2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "No label_names provided for model class `PeftModel`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-17 14:33:56 __init__.py:207] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tom/fun/xverify/.venv/lib/python3.12/site-packages/trl/trainer/grpo_trainer.py:471: UserWarning: The requested device cuda:0 is also being used for training. For higher throughput and to avoid out-of-memory errors, it is recommended to use a dedicated device for vLLM. If this is intentional, you may ignore this warning but should adjust `vllm_gpu_memory_utilization` accordingly.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-17 14:34:02 config.py:549] This model supports multiple tasks: {'score', 'embed', 'generate', 'reward', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 03-17 14:34:02 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='Qwen/Qwen2.5-1.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-1.5B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 03-17 14:34:03 cuda.py:229] Using Flash Attention backend.\n",
      "INFO 03-17 14:34:03 model_runner.py:1110] Starting to load model Qwen/Qwen2.5-1.5B-Instruct...\n",
      "INFO 03-17 14:34:04 weight_utils.py:254] Using model weights format ['*.safetensors']\n",
      "INFO 03-17 14:34:04 weight_utils.py:304] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6182b64f15c04d749b571bc688daa09b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-17 14:34:05 model_runner.py:1115] Loading model weights took 2.8875 GB\n",
      "INFO 03-17 14:34:07 worker.py:267] Memory profiling takes 1.83 seconds\n",
      "INFO 03-17 14:34:07 worker.py:267] the current vLLM instance can use total_gpu_memory (23.58GiB) x gpu_memory_utilization (0.30) = 7.07GiB\n",
      "INFO 03-17 14:34:07 worker.py:267] model weights take 2.89GiB; non_torch_memory takes 0.06GiB; PyTorch activation peak memory takes 2.02GiB; the rest of the memory reserved for KV Cache is 2.11GiB.\n",
      "INFO 03-17 14:34:07 executor_base.py:111] # cuda blocks: 4929, # CPU blocks: 9362\n",
      "INFO 03-17 14:34:07 executor_base.py:116] Maximum concurrency for 32768 tokens per request: 2.41x\n",
      "INFO 03-17 14:34:11 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:16<00:00,  2.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-17 14:34:27 model_runner.py:1562] Graph capturing finished in 16 secs, took 0.20 GiB\n",
      "INFO 03-17 14:34:27 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 22.34 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def exact_answer_reward_func(completions, answer, **_) -> list[float]:\n",
    "    \"\"\"Reward function that checks if the final answer matches the expected answer.\"\"\"\n",
    "\n",
    "    def _check_answer(trajectory: list[dict[str, str]], answer: str) -> float:\n",
    "        \"\"\"Extract the last answer from a trajectory.\"\"\"\n",
    "        last_message = trajectory[-1]\n",
    "        assert last_message[\"role\"] == \"assistant\", \"should be assistant\"\n",
    "        parsed: Reason_and_Act | None = guided_schema.parse(last_message[\"content\"])  # type: ignore\n",
    "        if parsed is None or not isinstance(parsed.response, FinalAnswer):\n",
    "            return 0.0\n",
    "        return 1.0 if parsed.response.answer == int(answer) else 0.0\n",
    "\n",
    "    return [_check_answer(c, a) for c, a in zip(completions, answer)]\n",
    "\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "model, tokenizer = xv.get_model_and_tokenizer(model_name)\n",
    "\n",
    "\n",
    "run_name = \"gsm8k-calculator-peft_\" + model_name.split(\"/\")[-1].lower()\n",
    "training_args = xv.get_default_grpo_config(run_name, num_gpus=1)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n",
    ")\n",
    "\n",
    "\n",
    "trainer = xv.GRPOGuidedTrainer(\n",
    "    guided_schema=guided_schema,\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    reward_funcs=[exact_answer_reward_func],\n",
    ")\n",
    "# trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
