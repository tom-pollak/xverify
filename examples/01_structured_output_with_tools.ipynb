{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Output with Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "\n",
    "from xverify import GuidedSchema, JSONToolUse, XMLToolUse, run_tools\n",
    "from xverify.tools import calculator, search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Often when running multi-step reasoning, we want to use tools to help us.\n",
    "\n",
    "However, not many libraries natively support this. Pydantic for instance is optimized for a static declarative schema, which isn't well suited to ad-hoc tool use.\n",
    "\n",
    "Here we can see two examples of tools:\n",
    "- `calculator`: essentially a wrapper around the `eval` function\n",
    "- `search`: uses duckduckgo to search the web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculator(expression='3 + 4 * (6 ** 7)')='1119747'\n",
      "\n",
      "---\n",
      "\n",
      "search(query='What is the capital of France?', num_results=1)='• Paris - Wikipedia\\n  Paris is a global city of culture, finance, diplomacy, and tourism, with an estimated population of 2 million residents in 2025.'\n"
     ]
    }
   ],
   "source": [
    "print(f\"{calculator(expression='3 + 4 * (6 ** 7)')=}\")\n",
    "print(\"\\n---\\n\")\n",
    "print(f\"{search(query='What is the capital of France?', num_results=1)=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is we can't (natively) include a tool call in a Pydantic model (due to the static declarative schema).\n",
    "\n",
    "However, we can use the new `ToolUse` class to handle tool calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reasoning=\"Let's add two numbers\" tool_use=calculator(tool_name='calculator', expression='2 + 2')\n",
      "calc_2_2.tool_use.run_tool()='4'\n"
     ]
    }
   ],
   "source": [
    "class ReasoningTool(BaseModel):\n",
    "    \"\"\"The result of a reasoning tool\"\"\"\n",
    "\n",
    "    reasoning: str\n",
    "    tool_use: JSONToolUse[calculator, search]\n",
    "\n",
    "\n",
    "calc_2_2 = ReasoningTool.model_validate(\n",
    "    {\n",
    "        \"reasoning\": \"Let's add two numbers\",\n",
    "        \"tool_use\": {\"tool_name\": \"calculator\", \"expression\": \"2 + 2\"},\n",
    "    }\n",
    ")\n",
    "print(calc_2_2)\n",
    "print(\n",
    "    f\"{calc_2_2.tool_use.run_tool()=}\"\n",
    ")  # on a ToolUse object, we can call run_tool() to run the tool and get the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is nice because if we can easily validate any arbitary schema and tool use is correct without any ad-hoc parsing (and we'll be able to enforce the LLM output is correct with guided decoding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tool not found!\n",
      "wrong argument!\n",
      "wrong argument type!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    ReasoningTool.model_validate(\n",
    "        {\n",
    "            \"reasoning\": \"\",\n",
    "            \"tool_use\": {\"tool_name\": \"none_existing_tool\", \"expression\": \"2 + 2\"},\n",
    "        }\n",
    "    )\n",
    "except ValidationError:\n",
    "    print(\"tool not found!\")\n",
    "try:\n",
    "    ReasoningTool.model_validate(\n",
    "        {\n",
    "            \"reasoning\": \"\",\n",
    "            \"tool_use\": {\"tool_name\": \"calculator\", \"wrong_arg\": \"2 + 2\"},\n",
    "        },\n",
    "    )\n",
    "except ValidationError:\n",
    "    print(\"wrong argument!\")\n",
    "\n",
    "try:\n",
    "    ReasoningTool.model_validate(\n",
    "        {\n",
    "            \"reasoning\": \"\",\n",
    "            \"tool_use\": {\"tool_name\": \"calculator\", \"expression\": 2 + 2},\n",
    "        },\n",
    "    )\n",
    "except ValidationError:\n",
    "    print(\"wrong argument type!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can implement a ReACT loop with tools really easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reason_and_Act(scratchpad='the question is 2 + 2', reasoning='we should use the calculator tool!', response=FinalAnswer(answer='42'))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Literal, Union\n",
    "\n",
    "\n",
    "class Tools(BaseModel):\n",
    "    \"\"\"\n",
    "    Run a tool.\n",
    "    \"\"\"\n",
    "\n",
    "    tool_use: XMLToolUse[calculator, search] = Field(\n",
    "        ..., description=\"The tool call to use\"\n",
    "    )\n",
    "\n",
    "\n",
    "class FinalAnswer(BaseModel):\n",
    "    \"\"\"\n",
    "    Return a final answer.\n",
    "    \"\"\"\n",
    "\n",
    "    answer: str = Field(..., description=\"The final answer to the question\")\n",
    "\n",
    "\n",
    "class Reason_and_Act(BaseModel):\n",
    "    scratchpad: str = Field(\n",
    "        ...,\n",
    "        description=\"Information from the Observation useful to answer the question\",\n",
    "    )\n",
    "    reasoning: str = Field(\n",
    "        ...,\n",
    "        description=\"It describes your thoughts about the question you have been asked\",\n",
    "    )\n",
    "    response: Union[Tools, FinalAnswer]\n",
    "\n",
    "\n",
    "res = Reason_and_Act.model_validate(\n",
    "    {\n",
    "        \"scratchpad\": \"the question is 2 + 2\",\n",
    "        \"reasoning\": \"we should use the calculator tool!\",\n",
    "        \"response\": {\n",
    "            \"tool_use\": {\n",
    "                # \"tool_name\": \"calculator\",\n",
    "                \"expression\": \"2 + 2\",\n",
    "            },\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "Reason_and_Act.model_validate(\n",
    "    {\n",
    "        \"scratchpad\": \"the question is 2 + 2\",\n",
    "        \"reasoning\": \"we should use the calculator tool!\",\n",
    "        \"response\": {\"answer\": \"42\"},\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And in case we just want to run all the tools in a response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response': {'tool_use': {'calculator': '4'}}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_tools(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will return `None` where no tools were called, which is useful for checking for the end of the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    run_tools(\n",
    "        Reason_and_Act(\n",
    "            scratchpad=\"\",\n",
    "            reasoning=\"\",\n",
    "            response=FinalAnswer(answer=\"42\"),\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you just want the output, run `run_tools` on the instantiated `ToolUse` object itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res.response.tool_use.run_tool()='4'\n"
     ]
    }
   ],
   "source": [
    "if isinstance(res.response, Tools):\n",
    "    print(f\"{res.response.tool_use.run_tool()=}\")\n",
    "else:\n",
    "    print(f\"{res.response.answer=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And of course we can do multiple tool calls in a single response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tool_use': [{'calculator': '4'},\n",
      "              {'search': '• Moon Fact Sheet - NSSDCA\\n'\n",
      "                         '  Equatorial radius (km) 1738.1: 6378.1: 0.2725: '\n",
      "                         'Polar radius (km) 1736.0: 6356.8: 0.2731: Volumetric '\n",
      "                         'mean radius (km) 1737.4: 6371.0: 0.2727: Ellipticity '\n",
      "                         '(Flattening) ...\\n'\n",
      "                         '\\n'\n",
      "                         '• Moon - Wikipedia\\n'\n",
      "                         '  The Moon has a solid iron-rich inner core with a '\n",
      "                         'radius possibly as small as 240 kilometres (150 mi) '\n",
      "                         'and a fluid outer core primarily made of liquid iron '\n",
      "                         'with a radius of roughly 300 kilometres (190 m.'},\n",
      "              {'calculator': '1480647168'}]}\n"
     ]
    }
   ],
   "source": [
    "class MultiToolUse(BaseModel):\n",
    "    tool_use: list[XMLToolUse[calculator, search]]\n",
    "\n",
    "\n",
    "res = MultiToolUse.model_validate(\n",
    "    {\n",
    "        \"tool_use\": [\n",
    "            {\"tool_name\": \"calculator\", \"expression\": \"2 + 2\"},\n",
    "            {\n",
    "                \"tool_name\": \"search\",\n",
    "                \"query\": \"What is the radius of the moon?\",\n",
    "                \"num_results\": 2,\n",
    "            },\n",
    "            {\"tool_name\": \"calculator\", \"expression\": \"3424 * 432432\"},\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "pprint(run_tools(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is cool, kinda, but now we have structed schema, we can **enforce** the LLM output is correct with guided decoding.\n",
    "\n",
    "Let's go back to our ReACT loop, and use `Env` to enforce the tool calls are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root ::= grammar-models\n",
      "string ::= nl [^\\n<] ([^<])*\n",
      "boolean ::= nl \"true\" | nl \"false\"\n",
      "integer ::= nl [0-9]+\n",
      "float ::= nl \"-\"? [0-9]+ (\".\" [0-9]+)?\n",
      "null ::= nl \"null\"\n",
      "nl ::= \"\\n\"\n",
      "calculator ::= nl \"<calculator>\" nl \"<expression>\" string nl \"</expression>\" nl \"</calculator>\"\n",
      "search ::= nl \"<search>\" nl \"<query>\" string nl \"</query>\" nl \"<num_results>\" integer nl \"</num_results>\" nl \"</search>\"\n",
      "tool_use-union ::= calculator | search\n",
      "Tools ::= nl \"<Tools>\" nl \"<tool_use>\" tool_use-union nl \"</tool_use>\" nl \"</Tools>\"\n",
      "FinalAnswer ::= nl \"<FinalAnswer>\" nl \"<answer>\" string nl \"</answer>\" nl \"</FinalAnswer>\"\n",
      "response-union ::= Tools | FinalAnswer\n",
      "Reason_and_Act ::=  \"<Reason_and_Act>\" nl \"<scratchpad>\" string nl \"</scratchpad>\" nl \"<reasoning>\" string nl \"</reasoning>\" nl \"<response>\" response-union nl \"</response>\" nl \"</Reason_and_Act>\"\n",
      "grammar-models ::= Reason_and_Act\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "env = GuidedSchema(Reason_and_Act)\n",
    "print(env.gbnf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-20 10:44:58 __init__.py:207] Automatically detected platform cuda.\n",
      "INFO 03-20 10:45:04 config.py:549] This model supports multiple tasks: {'reward', 'embed', 'generate', 'score', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 03-20 10:45:04 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='Qwen/Qwen2.5-1.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-1.5B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 03-20 10:45:05 cuda.py:229] Using Flash Attention backend.\n",
      "INFO 03-20 10:45:05 model_runner.py:1110] Starting to load model Qwen/Qwen2.5-1.5B-Instruct...\n",
      "INFO 03-20 10:45:06 weight_utils.py:254] Using model weights format ['*.safetensors']\n",
      "INFO 03-20 10:45:06 weight_utils.py:304] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fef7aa809e54e2c85a0063b0a8f7b00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-20 10:45:12 model_runner.py:1115] Loading model weights took 2.8875 GB\n",
      "INFO 03-20 10:45:12 worker.py:267] Memory profiling takes 0.56 seconds\n",
      "INFO 03-20 10:45:12 worker.py:267] the current vLLM instance can use total_gpu_memory (23.58GiB) x gpu_memory_utilization (0.90) = 21.22GiB\n",
      "INFO 03-20 10:45:12 worker.py:267] model weights take 2.89GiB; non_torch_memory takes 3.65GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 13.30GiB.\n",
      "INFO 03-20 10:45:13 executor_base.py:111] # cuda blocks: 31127, # CPU blocks: 9362\n",
      "INFO 03-20 10:45:13 executor_base.py:116] Maximum concurrency for 2000 tokens per request: 249.02x\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 488.00 MiB. GPU 0 has a total capacity of 23.58 GiB of which 245.56 MiB is free. Process 26922 has 16.59 GiB memory in use. Including non-PyTorch memory, this process has 6.69 GiB memory in use. Of the allocated memory 6.22 GiB is allocated by PyTorch, and 166.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LLM\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mllm\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m():  \u001b[38;5;66;03m# interactive use\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     llm = \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mQwen/Qwen2.5-1.5B-Instruct\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_model_len\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m env = GuidedSchema(Reason_and_Act)\n\u001b[32m      8\u001b[39m env.sampling_params()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/vllm/utils.py:1022\u001b[39m, in \u001b[36mdeprecate_args.<locals>.wrapper.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1015\u001b[39m             msg += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1017\u001b[39m         warnings.warn(\n\u001b[32m   1018\u001b[39m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(msg),\n\u001b[32m   1019\u001b[39m             stacklevel=\u001b[32m3\u001b[39m,  \u001b[38;5;66;03m# The inner function takes up one level\u001b[39;00m\n\u001b[32m   1020\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1022\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/vllm/entrypoints/llm.py:242\u001b[39m, in \u001b[36mLLM.__init__\u001b[39m\u001b[34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_overrides, mm_processor_kwargs, task, override_pooler_config, compilation_config, **kwargs)\u001b[39m\n\u001b[32m    239\u001b[39m \u001b[38;5;66;03m# Logic to switch between engines is done at runtime instead of import\u001b[39;00m\n\u001b[32m    240\u001b[39m \u001b[38;5;66;03m# to avoid import order issues\u001b[39;00m\n\u001b[32m    241\u001b[39m \u001b[38;5;28mself\u001b[39m.engine_class = \u001b[38;5;28mself\u001b[39m.get_engine_class()\n\u001b[32m--> \u001b[39m\u001b[32m242\u001b[39m \u001b[38;5;28mself\u001b[39m.llm_engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mUsageContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[38;5;28mself\u001b[39m.request_counter = Counter()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/vllm/engine/llm_engine.py:489\u001b[39m, in \u001b[36mLLMEngine.from_engine_args\u001b[39m\u001b[34m(cls, engine_args, usage_context, stat_loggers)\u001b[39m\n\u001b[32m    487\u001b[39m executor_class = \u001b[38;5;28mcls\u001b[39m._get_executor_cls(engine_config)\n\u001b[32m    488\u001b[39m \u001b[38;5;66;03m# Create the LLM engine.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m489\u001b[39m engine = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m    \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m engine\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/vllm/engine/llm_engine.py:276\u001b[39m, in \u001b[36mLLMEngine.__init__\u001b[39m\u001b[34m(self, vllm_config, executor_class, log_stats, usage_context, stat_loggers, input_registry, mm_registry, use_cached_outputs)\u001b[39m\n\u001b[32m    273\u001b[39m \u001b[38;5;28mself\u001b[39m.model_executor = executor_class(vllm_config=vllm_config, )\n\u001b[32m    275\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model_config.runner_type != \u001b[33m\"\u001b[39m\u001b[33mpooling\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m276\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_initialize_kv_caches\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[38;5;66;03m# If usage stat is enabled, collect relevant info.\u001b[39;00m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_usage_stats_enabled():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/vllm/engine/llm_engine.py:434\u001b[39m, in \u001b[36mLLMEngine._initialize_kv_caches\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    431\u001b[39m \u001b[38;5;28mself\u001b[39m.cache_config.num_gpu_blocks = num_gpu_blocks\n\u001b[32m    432\u001b[39m \u001b[38;5;28mself\u001b[39m.cache_config.num_cpu_blocks = num_cpu_blocks\n\u001b[32m--> \u001b[39m\u001b[32m434\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_executor\u001b[49m\u001b[43m.\u001b[49m\u001b[43minitialize_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_gpu_blocks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_cpu_blocks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    435\u001b[39m elapsed = time.time() - start\n\u001b[32m    436\u001b[39m logger.info((\u001b[33m\"\u001b[39m\u001b[33minit engine (profile, create kv cache, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    437\u001b[39m              \u001b[33m\"\u001b[39m\u001b[33mwarmup model) took \u001b[39m\u001b[38;5;132;01m%.2f\u001b[39;00m\u001b[33m seconds\u001b[39m\u001b[33m\"\u001b[39m), elapsed)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/vllm/executor/executor_base.py:122\u001b[39m, in \u001b[36mExecutorBase.initialize_cache\u001b[39m\u001b[34m(self, num_gpu_blocks, num_cpu_blocks)\u001b[39m\n\u001b[32m    119\u001b[39m \u001b[38;5;28mself\u001b[39m.cache_config.num_gpu_blocks = num_gpu_blocks\n\u001b[32m    120\u001b[39m \u001b[38;5;28mself\u001b[39m.cache_config.num_cpu_blocks = num_cpu_blocks\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollective_rpc\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minitialize_cache\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m                    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_gpu_blocks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_cpu_blocks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py:56\u001b[39m, in \u001b[36mUniProcExecutor.collective_rpc\u001b[39m\u001b[34m(self, method, timeout, args, kwargs)\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     55\u001b[39m     kwargs = {}\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m answer = \u001b[43mrun_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdriver_worker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [answer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/vllm/utils.py:2196\u001b[39m, in \u001b[36mrun_method\u001b[39m\u001b[34m(obj, method, args, kwargs)\u001b[39m\n\u001b[32m   2194\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2195\u001b[39m     func = partial(method, obj)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2196\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/vllm/worker/worker.py:306\u001b[39m, in \u001b[36mWorker.initialize_cache\u001b[39m\u001b[34m(self, num_gpu_blocks, num_cpu_blocks)\u001b[39m\n\u001b[32m    304\u001b[39m     context = nullcontext()\n\u001b[32m    305\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m306\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_init_cache_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    307\u001b[39m \u001b[38;5;28mself\u001b[39m._warm_up_model()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/vllm/worker/worker.py:312\u001b[39m, in \u001b[36mWorker._init_cache_engine\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    309\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_init_cache_engine\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    310\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cache_config.num_gpu_blocks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    311\u001b[39m     \u001b[38;5;28mself\u001b[39m.cache_engine = [\n\u001b[32m--> \u001b[39m\u001b[32m312\u001b[39m         \u001b[43mCacheEngine\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m                    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparallel_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    314\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.parallel_config.pipeline_parallel_size)\n\u001b[32m    315\u001b[39m     ]\n\u001b[32m    316\u001b[39m     \u001b[38;5;28mself\u001b[39m.gpu_cache = [\n\u001b[32m    317\u001b[39m         \u001b[38;5;28mself\u001b[39m.cache_engine[ve].gpu_cache\n\u001b[32m    318\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m ve \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.parallel_config.pipeline_parallel_size)\n\u001b[32m    319\u001b[39m     ]\n\u001b[32m    320\u001b[39m     bind_kv_cache(\u001b[38;5;28mself\u001b[39m.compilation_config.static_forward_context,\n\u001b[32m    321\u001b[39m                   \u001b[38;5;28mself\u001b[39m.gpu_cache)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/vllm/worker/cache_engine.py:69\u001b[39m, in \u001b[36mCacheEngine.__init__\u001b[39m\u001b[34m(self, cache_config, model_config, parallel_config, device_config)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28mself\u001b[39m.attn_backend = get_attn_backend(\u001b[38;5;28mself\u001b[39m.head_size,\n\u001b[32m     62\u001b[39m                                      model_config.dtype,\n\u001b[32m     63\u001b[39m                                      cache_config.cache_dtype,\n\u001b[32m     64\u001b[39m                                      \u001b[38;5;28mself\u001b[39m.block_size,\n\u001b[32m     65\u001b[39m                                      model_config.is_attention_free,\n\u001b[32m     66\u001b[39m                                      use_mla=model_config.use_mla)\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# Initialize the cache.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m \u001b[38;5;28mself\u001b[39m.gpu_cache = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_allocate_kv_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_gpu_blocks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[38;5;28mself\u001b[39m.cpu_cache = \u001b[38;5;28mself\u001b[39m._allocate_kv_cache(\u001b[38;5;28mself\u001b[39m.num_cpu_blocks, \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/vllm/worker/cache_engine.py:103\u001b[39m, in \u001b[36mCacheEngine._allocate_kv_cache\u001b[39m\u001b[34m(self, num_blocks, device)\u001b[39m\n\u001b[32m     97\u001b[39m     alloc_shape = kv_cache_shape\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.num_attention_layers):\n\u001b[32m    100\u001b[39m     \u001b[38;5;66;03m# null block in CpuGpuBlockAllocator requires at least that\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;66;03m# block to be zeroed-out.\u001b[39;00m\n\u001b[32m    102\u001b[39m     \u001b[38;5;66;03m# We zero-out everything for simplicity.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m     layer_kv_cache = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43malloc_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43mpin_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpin_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m     \u001b[38;5;66;03m# If we allocated with padding for alignment reasons truncate the\u001b[39;00m\n\u001b[32m    109\u001b[39m     \u001b[38;5;66;03m# shape while preserving the aligned stride\u001b[39;00m\n\u001b[32m    110\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.align_cache:\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 488.00 MiB. GPU 0 has a total capacity of 23.58 GiB of which 245.56 MiB is free. Process 26922 has 16.59 GiB memory in use. Including non-PyTorch memory, this process has 6.69 GiB memory in use. Of the allocated memory 6.22 GiB is allocated by PyTorch, and 166.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from vllm import LLM\n",
    "\n",
    "if \"llm\" not in globals():  # interactive use\n",
    "    llm = LLM(model=\"Qwen/Qwen2.5-1.5B-Instruct\", max_model_len=2000)\n",
    "\n",
    "env = GuidedSchema(Reason_and_Act)\n",
    "\n",
    "env.sampling_params()\n",
    "\n",
    "sampling_params = env.sampling_params(\n",
    "    max_tokens=500,\n",
    "    n=1,\n",
    "    temperature=1.0,\n",
    ")\n",
    "\n",
    "max_steps = 5\n",
    "messages: list[dict] = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": f\"\"\"\\\n",
    "You are a helpful assistant, responding in XML structured output.\n",
    "\n",
    "- Think step by step using the scratchpad and reasoning outputs. You have {max_steps - 1} steps to think before responding.\n",
    "- Use the tools provided. DO NOT rely on your own knowledge when a tool is available to help you.\n",
    "- Respond with a final answer only once your are absolutely sure you have the answer.\n",
    "\n",
    "Respond with a XML object, following the schema below:\n",
    "\n",
    "{env.doc}\n",
    "\n",
    "Use the tools!\n",
    "\"\"\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"What is the distance from the moon to the sun?\"},\n",
    "]\n",
    "\n",
    "for _ in range(max_steps):\n",
    "    outp = llm.chat(  # type: ignore\n",
    "        messages=messages,  # type: ignore\n",
    "        sampling_params=sampling_params,\n",
    "        use_tqdm=False,\n",
    "    )\n",
    "    text = outp[0].outputs[0].text\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Assistant:\\n{text}\")\n",
    "\n",
    "    struct_res = env.parse(text)\n",
    "    if not struct_res:\n",
    "        print(\"*** Invalid response, skipping ***\")\n",
    "        continue\n",
    "\n",
    "    messages.append({\"role\": \"assistant\", \"content\": struct_res.model_dump()})\n",
    "    tool_outp = run_tools(struct_res)\n",
    "    if tool_outp:\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Tool output:\\n{tool_outp}\")\n",
    "        messages.append({\"role\": \"user\", \"content\": tool_outp})\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43menv\u001b[49m.gbnf)\n",
      "\u001b[31mNameError\u001b[39m: name 'env' is not defined"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
