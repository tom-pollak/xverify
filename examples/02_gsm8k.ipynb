{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-25 13:35:36 [__init__.py:256] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "from typing import Union\n",
    "\n",
    "from datasets import Dataset, load_dataset\n",
    "from peft import LoraConfig  # type: ignore\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "import xverify as xv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tools(BaseModel):\n",
    "    \"\"\"\n",
    "    Run a tool.\n",
    "    \"\"\"\n",
    "\n",
    "    tool_use: xv.XMLToolUse[xv.calculator, xv.search] = Field(\n",
    "        ..., description=\"The tool call to use\"\n",
    "    )\n",
    "\n",
    "\n",
    "class FinalAnswer(BaseModel):\n",
    "    \"\"\"\n",
    "    Return a final answer.\n",
    "    \"\"\"\n",
    "\n",
    "    answer: int = Field(..., description=\"Final answer to the question\")\n",
    "\n",
    "\n",
    "class Reason_and_Act(BaseModel):\n",
    "    scratchpad: str = Field(\n",
    "        ...,\n",
    "        description=\"Information from the Observation useful to answer the question\",\n",
    "    )\n",
    "    reasoning: str = Field(\n",
    "        ...,\n",
    "        description=\"It describes your thoughts about the question you have been asked\",\n",
    "    )\n",
    "    response: Tools | FinalAnswer\n",
    "    # response: Union[Tools, FinalAnswer] = Field(..., description=\"Current step response\")\n",
    "\n",
    "\n",
    "def tool_response_func(model: Reason_and_Act) -> dict | None:\n",
    "    return xv.run_tools(model.response)\n",
    "\n",
    "\n",
    "guided_schema = xv.GuidedSchema(\n",
    "    Reason_and_Act,\n",
    "    schema=\"xml\",\n",
    "    tool_response_func=tool_response_func,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(\n",
    "    prompt: str, system_prompt: str | None = None\n",
    ") -> list[dict[str, str]]:\n",
    "    messages = []\n",
    "    if system_prompt:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    return messages\n",
    "\n",
    "\n",
    "def extract_hash_answer(text: str) -> str | None:\n",
    "    if \"####\" not in text:\n",
    "        return None\n",
    "    return text.split(\"####\")[1].strip()\n",
    "\n",
    "max_steps = 10\n",
    "\n",
    "SYSTEM_PROMPT = f\"\"\"\\\n",
    "You are a helpful assistant, responding in XML structured output.\n",
    "\n",
    "- Think step by step using the scratchpad and reasoning outputs. You have {max_steps - 1} steps to think before responding.\n",
    "- Use the tools provided. DO NOT rely on your own knowledge when a tool is available to help you.\n",
    "- Respond with a final answer only once your are absolutely sure you have the answer.\n",
    "\n",
    "Respond with a XML object, following the schema below:\n",
    "\n",
    "{guided_schema.doc}\n",
    "\"\"\"\n",
    "\n",
    "dataset: Dataset = load_dataset(\"openai/gsm8k\", \"main\", split=\"train\")  # type: ignore\n",
    "dataset = dataset.map(lambda x: {\n",
    "    'prompt': [\n",
    "        {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "        {'role': 'user', 'content': x['question']}\n",
    "    ],\n",
    "    'answer': extract_hash_answer(x['answer'])\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Liger kernel\n",
      "Applied Liger kernels to Qwen2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ef4d20ffb744b7594a945205144e051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModel`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-25 13:36:26 [utils.py:925] Found nccl from library libnccl.so.2\n",
      "INFO 03-25 13:36:26 [pynccl.py:69] vLLM is using nccl==2.21.5\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "NCCL error: invalid usage (run with NCCL_DEBUG=WARN for details)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 53\u001b[39m\n\u001b[32m     21\u001b[39m training_args = xv.get_default_grpo_config(\n\u001b[32m     22\u001b[39m     run_name,\n\u001b[32m     23\u001b[39m     num_gpus=\u001b[32m1\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     39\u001b[39m     save_total_limit=\u001b[32m1\u001b[39m,\n\u001b[32m     40\u001b[39m )\n\u001b[32m     42\u001b[39m peft_config = LoraConfig(\n\u001b[32m     43\u001b[39m     r=\u001b[32m16\u001b[39m,\n\u001b[32m     44\u001b[39m     lora_alpha=\u001b[32m64\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     49\u001b[39m     ],\n\u001b[32m     50\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m trainer = \u001b[43mxv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mGRPOGuidedTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m    \u001b[49m\u001b[43mguided_schema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mguided_schema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreward_funcs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mexact_answer_reward_func\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m trainer.train()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/src/xverify/grpo/guided_trainer.py:66\u001b[39m, in \u001b[36mGRPOGuidedTrainer.__init__\u001b[39m\u001b[34m(self, guided_schema, model, reward_funcs, args, train_dataset, eval_dataset, processing_class, reward_processing_classes, callbacks, optimizers, peft_config)\u001b[39m\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m args.vllm_guided_decoding_regex \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     64\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mGuided decoding is set by GRPOEnvTrainer\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreward_funcs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreward_funcs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreward_processing_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreward_processing_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[38;5;28mself\u001b[39m.guided_schema = guided_schema\n\u001b[32m     80\u001b[39m \u001b[38;5;66;03m# update sampling params to use our guided decoding params\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/trl/trainer/grpo_trainer.py:477\u001b[39m, in \u001b[36mGRPOTrainer.__init__\u001b[39m\u001b[34m(self, model, reward_funcs, args, train_dataset, eval_dataset, processing_class, reward_processing_classes, callbacks, optimizers, peft_config)\u001b[39m\n\u001b[32m    471\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m    472\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mvLLM is not available and `use_vllm` is set to True. Please install vLLM with \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    473\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`pip install vllm` to use it.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    474\u001b[39m     )\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.is_main_process:\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     \u001b[38;5;28mself\u001b[39m.vllm_client = \u001b[43mVLLMClient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvllm_server_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvllm_server_port\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconnection_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvllm_server_timeout\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[38;5;66;03m# vLLM specific sampling arguments\u001b[39;00m\n\u001b[32m    482\u001b[39m \u001b[38;5;28mself\u001b[39m.guided_decoding_regex = args.vllm_guided_decoding_regex\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/trl/extras/vllm_client.py:95\u001b[39m, in \u001b[36mVLLMClient.__init__\u001b[39m\u001b[34m(self, host, server_port, group_port, connection_timeout)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28mself\u001b[39m.group_port = group_port\n\u001b[32m     94\u001b[39m \u001b[38;5;28mself\u001b[39m.check_server(connection_timeout)  \u001b[38;5;66;03m# check server and fail after timeout\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minit_communicator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m atexit.register(\u001b[38;5;28mself\u001b[39m.close_communicator)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/trl/extras/vllm_client.py:215\u001b[39m, in \u001b[36mVLLMClient.init_communicator\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    213\u001b[39m \u001b[38;5;66;03m# Set up the communication group for weight broadcasting\u001b[39;00m\n\u001b[32m    214\u001b[39m pg = StatelessProcessGroup.create(host=\u001b[38;5;28mself\u001b[39m.host, port=\u001b[38;5;28mself\u001b[39m.group_port, rank=\u001b[38;5;28mself\u001b[39m.rank, world_size=world_size)\n\u001b[32m--> \u001b[39m\u001b[32m215\u001b[39m \u001b[38;5;28mself\u001b[39m.pynccl_comm = \u001b[43mPyNcclCommunicator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcuda:0\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/vllm/distributed/device_communicators/pynccl.py:99\u001b[39m, in \u001b[36mPyNcclCommunicator.__init__\u001b[39m\u001b[34m(self, group, device, library_path)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;66;03m# nccl communicator and stream will use this device\u001b[39;00m\n\u001b[32m     96\u001b[39m \u001b[38;5;66;03m# `torch.cuda.device` is a context manager that changes the\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# current cuda device to the specified one\u001b[39;00m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.cuda.device(device):\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m     \u001b[38;5;28mself\u001b[39m.comm: ncclComm_t = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnccl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mncclCommInitRank\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43munique_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrank\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    102\u001b[39m     stream = current_stream()\n\u001b[32m    103\u001b[39m     \u001b[38;5;66;03m# A small all_reduce for warmup.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/vllm/distributed/device_communicators/pynccl_wrapper.py:277\u001b[39m, in \u001b[36mNCCLLibrary.ncclCommInitRank\u001b[39m\u001b[34m(self, world_size, unique_id, rank)\u001b[39m\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mncclCommInitRank\u001b[39m(\u001b[38;5;28mself\u001b[39m, world_size: \u001b[38;5;28mint\u001b[39m, unique_id: ncclUniqueId,\n\u001b[32m    275\u001b[39m                      rank: \u001b[38;5;28mint\u001b[39m) -> ncclComm_t:\n\u001b[32m    276\u001b[39m     comm = ncclComm_t()\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mNCCL_CHECK\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_funcs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mncclCommInitRank\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomm\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m                                                    \u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munique_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m                                                    \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    280\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m comm\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/vllm/distributed/device_communicators/pynccl_wrapper.py:256\u001b[39m, in \u001b[36mNCCLLibrary.NCCL_CHECK\u001b[39m\u001b[34m(self, result)\u001b[39m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result != \u001b[32m0\u001b[39m:\n\u001b[32m    255\u001b[39m     error_str = \u001b[38;5;28mself\u001b[39m.ncclGetErrorString(result)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNCCL error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: NCCL error: invalid usage (run with NCCL_DEBUG=WARN for details)"
     ]
    }
   ],
   "source": [
    "def exact_answer_reward_func(completions, answer, **_) -> list[float]:\n",
    "    \"\"\"Reward function that checks if the final answer matches the expected answer.\"\"\"\n",
    "\n",
    "    def _check_answer(trajectory: list[dict[str, str]], answer: str) -> float:\n",
    "        \"\"\"Extract the last answer from a trajectory.\"\"\"\n",
    "        last_message = trajectory[-1]\n",
    "        assert last_message[\"role\"] == \"assistant\", \"should be assistant\"\n",
    "        parsed: Reason_and_Act | None = guided_schema.parse(last_message[\"content\"])  # type: ignore\n",
    "        if parsed is None or not isinstance(parsed.response, FinalAnswer):\n",
    "            return 0.0\n",
    "        return 1.0 if str(parsed.response.answer) == answer else 0.0\n",
    "\n",
    "    return [_check_answer(c, a) for c, a in zip(completions, answer)]\n",
    "\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "model, tokenizer = xv.get_model_and_tokenizer(model_name)\n",
    "\n",
    "\n",
    "run_name = \"gsm8k-calculator-peft_\" + model_name.split(\"/\")[-1].lower()\n",
    "training_args = xv.get_default_grpo_config(\n",
    "    run_name,\n",
    "    num_gpus=1,\n",
    "    ### GRPO params ###\n",
    "    learning_rate=5e-6,\n",
    "    num_generations=6,\n",
    "    per_device_train_batch_size=6,  # 1 prompts per batch\n",
    "    gradient_accumulation_steps=1,  # 1 prompt per forward\n",
    "    logging_steps=1,\n",
    "    num_iterations=2,  # 1 on-policy + 1 off-policy\n",
    "    max_steps=250,\n",
    "    max_prompt_length=200,\n",
    "    max_completion_length=512,\n",
    "    vllm_gpu_memory_utilization=0.3,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    # checkpointing\n",
    "    save_steps=50,\n",
    "    save_only_model=False,\n",
    "    save_total_limit=1,\n",
    ")\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "trainer = xv.GRPOGuidedTrainer(\n",
    "    guided_schema=guided_schema,\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    reward_funcs=[exact_answer_reward_func],\n",
    ")\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
