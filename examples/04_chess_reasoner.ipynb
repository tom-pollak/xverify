{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-25 13:32:09 [__init__.py:256] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "import chess\n",
    "import chess.engine\n",
    "import chess.pgn\n",
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "from dotenv import load_dotenv\n",
    "from peft import LoraConfig  # type: ignore\n",
    "from pydantic import BaseModel, Field\n",
    "from tqdm import tqdm\n",
    "\n",
    "import wandb\n",
    "import xverify as xv\n",
    "from xverify import GuidedSchema\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Can I push to HF\n",
    "if os.environ.get(\"HF_TOKEN\") is None:\n",
    "    raise ValueError(\"HF_TOKEN not found! Please set\")\n",
    "\n",
    "ENGINE = chess.engine.SimpleEngine.popen_uci(\"stockfish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== CONFIGURATION PARAMETERS ========\n",
    "# fmt: off\n",
    "NUM_SAMPLES = 1000\n",
    "\n",
    "# Reward function weights\n",
    "UCI_FORMAT_WEIGHT = 0.25         # Valid UCI notation\n",
    "LEGAL_MOVE_WEIGHT = 0.5          # Legal move\n",
    "MOVE_QUALITY_WEIGHT = 2.         # Good move quality\n",
    "\n",
    "# deduct points for repeating moves\n",
    "REPEAT_MOVE_PENALTY = 0.05\n",
    "MOVE_HISTORY = deque(maxlen=50)\n",
    "\n",
    "# Engine settings\n",
    "ENGINE_ANALYSIS_TIME = 5.  # Time limit for engine analysis in seconds\n",
    "\n",
    "# fmt: on\n",
    "# =========================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Given a chess position in FEN notation, analyze it and suggest the best move in UCI notation.\n",
      "\n",
      "Respond in the following format:\n",
      "\n",
      "Output Model: Chess_Reason_and_Act\n",
      "  Output Fields:\n",
      "    scratchpad (str):\n",
      "        Description: Information from the Observation useful to answer the question\n",
      "    reasoning (str):\n",
      "        Description: It describes your thoughts about the question you have been asked\n",
      "    best_move (str):\n",
      "        Description: The best move to make in the current position, in UCI notation (e.g. b7b3)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Chess_Reason_and_Act(BaseModel):\n",
    "    scratchpad: str = Field(\n",
    "        ...,\n",
    "        description=\"Information from the Observation useful to answer the question\",\n",
    "    )\n",
    "    reasoning: str = Field(\n",
    "        ...,\n",
    "        description=\"It describes your thoughts about the question you have been asked\",\n",
    "    )\n",
    "    best_move: str = Field(\n",
    "        ...,\n",
    "        description=\"The best move to make in the current position, in UCI notation (e.g. b7b3)\",\n",
    "    )\n",
    "\n",
    "\n",
    "guided_schema = GuidedSchema(Chess_Reason_and_Act)\n",
    "\n",
    "\n",
    "SYSTEM_PROMPT = f\"\"\"\n",
    "Given a chess position in FEN notation, analyze it and suggest the best move in UCI notation.\n",
    "\n",
    "Respond in the following format:\n",
    "\n",
    "{guided_schema.doc}\n",
    "\"\"\"\n",
    "\n",
    "print(SYSTEM_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer(trajectory: list[dict[str, str]]) -> str:\n",
    "    \"\"\"Extract the last answer from a trajectory.\"\"\"\n",
    "    last_message = trajectory[-1]\n",
    "    assert last_message[\"role\"] == \"assistant\", \"should be assistant\"\n",
    "    parsed: Chess_Reason_and_Act | None = guided_schema.parse(last_message[\"content\"])  # type: ignore\n",
    "    return parsed.best_move if parsed else \"\"\n",
    "\n",
    "def extract_completions(completions) -> list[str]:\n",
    "    \"\"\"Extract the last answer from a trajectory.\"\"\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    return [extract_answer(r) for r in responses]\n",
    "\n",
    "def is_valid_uci_format(move_str: str) -> bool:\n",
    "    \"\"\"Check if a string is in valid UCI move format (e.g., e2e4)\"\"\"\n",
    "    try:\n",
    "        chess.Move.from_uci(move_str)\n",
    "        return True\n",
    "    except:  # noqa: E722\n",
    "        return False\n",
    "\n",
    "\n",
    "def is_legal_move(move_str: str, board: chess.Board) -> bool:\n",
    "    \"\"\"Check if a move string is valid for the given board position\"\"\"\n",
    "    try:\n",
    "        move = chess.Move.from_uci(move_str)\n",
    "        return move in board.legal_moves\n",
    "    except:  # noqa: E722\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "def valid_uci_reward(completions, fen, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward function that checks if the move is a valid UCI format\"\"\"\n",
    "    extracted_moves = extract_completions(completions)\n",
    "\n",
    "    rewards = []\n",
    "    for i, move in enumerate(extracted_moves):\n",
    "        move = move.strip()\n",
    "        valid_uci = is_valid_uci_format(move)\n",
    "        rewards.append(UCI_FORMAT_WEIGHT if valid_uci else 0.0)\n",
    "\n",
    "    return rewards\n",
    "\n",
    "\n",
    "def legal_move_reward(completions, fen, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward function that checks if the move is legal\"\"\"\n",
    "    extracted_moves = extract_completions(completions)\n",
    "\n",
    "    rewards = []\n",
    "    for i, move in enumerate(extracted_moves):\n",
    "        move = move.strip()\n",
    "        board = chess.Board(fen[i])\n",
    "\n",
    "        legal = is_legal_move(move, board)\n",
    "        if legal:\n",
    "            reward = LEGAL_MOVE_WEIGHT\n",
    "            frequency = MOVE_HISTORY.count(move)\n",
    "            frequency_penalty = frequency * REPEAT_MOVE_PENALTY\n",
    "            reward = max(0.0, reward * (1.0 - frequency_penalty))\n",
    "        else:\n",
    "            reward = 0.0\n",
    "\n",
    "        rewards.append(reward)\n",
    "\n",
    "    return rewards\n",
    "\n",
    "\n",
    "def engine_analysis_reward(completions, fen, **kwargs) -> list[float]:\n",
    "    \"\"\"\n",
    "    Reward based on how good the suggested move is according to the engine.\n",
    "    Uses centipawn loss to evaluate move quality.\n",
    "    This is the final reward function, so it's responsible for calling the logging function.\n",
    "    \"\"\"\n",
    "    engine_time = 0.0\n",
    "    centipawn_losses = []\n",
    "\n",
    "    extracted_moves = extract_completions(completions)\n",
    "\n",
    "    move_rewards = []\n",
    "\n",
    "    for i, move in enumerate(extracted_moves):\n",
    "        move = move.strip()\n",
    "        board = chess.Board(fen[i])\n",
    "\n",
    "        # Skip evaluation for invalid moves\n",
    "        if not move or not is_valid_uci_format(move) or not is_legal_move(move, board):\n",
    "            move_rewards.append(0.0)\n",
    "            centipawn_losses.append(None)\n",
    "            continue\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "\n",
    "        # Engine analysis of current position\n",
    "        initial_eval = ENGINE.analyse(\n",
    "            board, chess.engine.Limit(time=ENGINE_ANALYSIS_TIME)\n",
    "        )\n",
    "        best_move = initial_eval[\"pv\"][0]\n",
    "        initial_score = initial_eval[\"score\"].relative.score(mate_score=10000)\n",
    "\n",
    "        # Make player's move and get new evaluation\n",
    "        player_move = chess.Move.from_uci(move)\n",
    "        board.push(player_move)\n",
    "        player_eval = ENGINE.analyse(\n",
    "            board, chess.engine.Limit(time=ENGINE_ANALYSIS_TIME)\n",
    "        )\n",
    "\n",
    "        # Negate because it's from opponent's perspective\n",
    "        after_move_score = -player_eval[\"score\"].relative.score(mate_score=10000)\n",
    "\n",
    "        # Calculate centipawn loss\n",
    "        centipawn_loss = initial_score - after_move_score\n",
    "\n",
    "        centipawn_losses.append(centipawn_loss)\n",
    "\n",
    "        # Reward scaling\n",
    "        # - Less than 300 (bishop / rook blunder) is 0.0\n",
    "        # - Best move is 1.0\n",
    "        reward = 0.0\n",
    "        if centipawn_loss <= 0:\n",
    "            reward = 1.0\n",
    "        elif centipawn_loss >= 300:\n",
    "            reward = 0.0\n",
    "        else:\n",
    "            reward = 1.0 - (centipawn_loss / 300.0)\n",
    "\n",
    "        move_rewards.append(reward * MOVE_QUALITY_WEIGHT)\n",
    "\n",
    "        engine_time += time.perf_counter() - start_time\n",
    "\n",
    "    wandb.log({\"train/engine_time\": engine_time})\n",
    "    wandb.log({\"train/centipawn_losses\": centipawn_losses})\n",
    "\n",
    "    return move_rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading chess positions: 100%|██████████| 1000/1000 [00:04<00:00, 212.34it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c0df95718d6457e9b40c05ce36992f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def get_random_position(row) -> str:\n",
    "    \"\"\"Extract a random position from a chess game\"\"\"\n",
    "    pgn = io.StringIO(row[\"text\"])\n",
    "    game = chess.pgn.read_game(pgn)\n",
    "    if not game:\n",
    "        return chess.STARTING_FEN\n",
    "\n",
    "    board = game.board()\n",
    "    mainline_moves = list(game.mainline_moves())\n",
    "    if not mainline_moves:\n",
    "        return chess.STARTING_FEN\n",
    "\n",
    "    # Choose a random point in the game (not too early, not too late)\n",
    "    min_move = min(5, len(mainline_moves) // 5)\n",
    "    max_move = max(min_move + 1, len(mainline_moves) - 5)\n",
    "    if max_move <= min_move:\n",
    "        max_move = min(len(mainline_moves), min_move + 10)\n",
    "\n",
    "    # Apply moves up to the random point\n",
    "    move_count = random.randint(min_move, max_move)\n",
    "    for move in mainline_moves[:move_count]:\n",
    "        board.push(move)\n",
    "\n",
    "    return board.fen()\n",
    "\n",
    "\n",
    "def format_dataset(row):\n",
    "    \"\"\"Format dataset for GRPO training\"\"\"\n",
    "    return {\n",
    "        \"prompt\": [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Analyze this chess position and give the best move: {row['fen']}\",\n",
    "            },\n",
    "        ],\n",
    "        \"fen\": row[\"fen\"],\n",
    "    }\n",
    "\n",
    "dataset = load_dataset( # type: ignore\n",
    "    \"Icannos/lichess_games\",\n",
    "    streaming=True,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "positions = []\n",
    "for _, row in tqdm(\n",
    "    zip(range(NUM_SAMPLES), dataset[\"train\"]),\n",
    "    desc=\"Loading chess positions\",\n",
    "    total=NUM_SAMPLES,\n",
    "):\n",
    "    positions.append(get_random_position(row))\n",
    "\n",
    "dataset: Dataset = Dataset.from_dict({\"fen\": positions})\n",
    "dataset = dataset.map(format_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Liger kernel\n",
      "Applied Liger kernels to Qwen2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "663d8e9a56214dfe948018eaeb894ccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model_name = \"google/gemma-3-4b-it\"\n",
    "model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "lora_rank = 16\n",
    "lora_alpha = 64\n",
    "\n",
    "gpu_memory_utilization = 0.85\n",
    "\n",
    "model, tokenizer = xv.get_model_and_tokenizer(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    use_cache=False,\n",
    ")\n",
    "\n",
    "max_seq_length = 1280\n",
    "max_prompt_length = 256\n",
    "max_completion_length = max_seq_length - max_prompt_length\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=lora_rank,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=\"all-linear\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModel`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-25 13:32:34 [utils.py:925] Found nccl from library libnccl.so.2\n",
      "INFO 03-25 13:32:34 [pynccl.py:69] vLLM is using nccl==2.21.5\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "NCCL error: invalid usage (run with NCCL_DEBUG=WARN for details)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m      1\u001b[39m checkpoint_path = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# No checkpoint: None\u001b[39;00m\n\u001b[32m      3\u001b[39m training_args = xv.get_default_grpo_config(\n\u001b[32m      4\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mchess-reasoner-training\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      5\u001b[39m     learning_rate=\u001b[32m5e-6\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     19\u001b[39m     vllm_gpu_memory_utilization=gpu_memory_utilization,\n\u001b[32m     20\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m trainer = \u001b[43mxv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mGRPOGuidedTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mguided_schema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mguided_schema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreward_funcs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlegal_move_reward\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalid_uci_reward\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine_analysis_reward\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m trainer.train(resume_from_checkpoint=checkpoint_path)\n\u001b[32m     38\u001b[39m model.save_lora(\u001b[33m\"\u001b[39m\u001b[33mchess_reasoner_llama_8b_lora\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/src/xverify/grpo/guided_trainer.py:66\u001b[39m, in \u001b[36mGRPOGuidedTrainer.__init__\u001b[39m\u001b[34m(self, guided_schema, model, reward_funcs, args, train_dataset, eval_dataset, processing_class, reward_processing_classes, callbacks, optimizers, peft_config)\u001b[39m\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m args.vllm_guided_decoding_regex \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     64\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mGuided decoding is set by GRPOEnvTrainer\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreward_funcs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreward_funcs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreward_processing_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreward_processing_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[38;5;28mself\u001b[39m.guided_schema = guided_schema\n\u001b[32m     80\u001b[39m \u001b[38;5;66;03m# update sampling params to use our guided decoding params\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/trl/trainer/grpo_trainer.py:477\u001b[39m, in \u001b[36mGRPOTrainer.__init__\u001b[39m\u001b[34m(self, model, reward_funcs, args, train_dataset, eval_dataset, processing_class, reward_processing_classes, callbacks, optimizers, peft_config)\u001b[39m\n\u001b[32m    471\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m    472\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mvLLM is not available and `use_vllm` is set to True. Please install vLLM with \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    473\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`pip install vllm` to use it.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    474\u001b[39m     )\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.is_main_process:\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     \u001b[38;5;28mself\u001b[39m.vllm_client = \u001b[43mVLLMClient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvllm_server_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvllm_server_port\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconnection_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvllm_server_timeout\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[38;5;66;03m# vLLM specific sampling arguments\u001b[39;00m\n\u001b[32m    482\u001b[39m \u001b[38;5;28mself\u001b[39m.guided_decoding_regex = args.vllm_guided_decoding_regex\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/trl/extras/vllm_client.py:95\u001b[39m, in \u001b[36mVLLMClient.__init__\u001b[39m\u001b[34m(self, host, server_port, group_port, connection_timeout)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28mself\u001b[39m.group_port = group_port\n\u001b[32m     94\u001b[39m \u001b[38;5;28mself\u001b[39m.check_server(connection_timeout)  \u001b[38;5;66;03m# check server and fail after timeout\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minit_communicator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m atexit.register(\u001b[38;5;28mself\u001b[39m.close_communicator)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/trl/extras/vllm_client.py:215\u001b[39m, in \u001b[36mVLLMClient.init_communicator\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    213\u001b[39m \u001b[38;5;66;03m# Set up the communication group for weight broadcasting\u001b[39;00m\n\u001b[32m    214\u001b[39m pg = StatelessProcessGroup.create(host=\u001b[38;5;28mself\u001b[39m.host, port=\u001b[38;5;28mself\u001b[39m.group_port, rank=\u001b[38;5;28mself\u001b[39m.rank, world_size=world_size)\n\u001b[32m--> \u001b[39m\u001b[32m215\u001b[39m \u001b[38;5;28mself\u001b[39m.pynccl_comm = \u001b[43mPyNcclCommunicator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcuda:0\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/vllm/distributed/device_communicators/pynccl.py:99\u001b[39m, in \u001b[36mPyNcclCommunicator.__init__\u001b[39m\u001b[34m(self, group, device, library_path)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;66;03m# nccl communicator and stream will use this device\u001b[39;00m\n\u001b[32m     96\u001b[39m \u001b[38;5;66;03m# `torch.cuda.device` is a context manager that changes the\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# current cuda device to the specified one\u001b[39;00m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.cuda.device(device):\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m     \u001b[38;5;28mself\u001b[39m.comm: ncclComm_t = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnccl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mncclCommInitRank\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43munique_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrank\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    102\u001b[39m     stream = current_stream()\n\u001b[32m    103\u001b[39m     \u001b[38;5;66;03m# A small all_reduce for warmup.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/vllm/distributed/device_communicators/pynccl_wrapper.py:277\u001b[39m, in \u001b[36mNCCLLibrary.ncclCommInitRank\u001b[39m\u001b[34m(self, world_size, unique_id, rank)\u001b[39m\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mncclCommInitRank\u001b[39m(\u001b[38;5;28mself\u001b[39m, world_size: \u001b[38;5;28mint\u001b[39m, unique_id: ncclUniqueId,\n\u001b[32m    275\u001b[39m                      rank: \u001b[38;5;28mint\u001b[39m) -> ncclComm_t:\n\u001b[32m    276\u001b[39m     comm = ncclComm_t()\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mNCCL_CHECK\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_funcs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mncclCommInitRank\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomm\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m                                                    \u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munique_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m                                                    \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    280\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m comm\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/vllm/distributed/device_communicators/pynccl_wrapper.py:256\u001b[39m, in \u001b[36mNCCLLibrary.NCCL_CHECK\u001b[39m\u001b[34m(self, result)\u001b[39m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result != \u001b[32m0\u001b[39m:\n\u001b[32m    255\u001b[39m     error_str = \u001b[38;5;28mself\u001b[39m.ncclGetErrorString(result)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNCCL error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: NCCL error: invalid usage (run with NCCL_DEBUG=WARN for details)"
     ]
    }
   ],
   "source": [
    "checkpoint_path = None  # No checkpoint: None\n",
    "\n",
    "training_args = xv.get_default_grpo_config(\n",
    "    \"chess-reasoner-training\",\n",
    "    learning_rate=5e-6,\n",
    "    weight_decay=0.1,\n",
    "    optim=\"adamw_8bit\",\n",
    "    per_device_train_batch_size=6,\n",
    "    gradient_accumulation_steps=1,\n",
    "    num_generations=6,\n",
    "    max_prompt_length=max_seq_length,\n",
    "    max_completion_length=max_completion_length,\n",
    "    max_steps=NUM_SAMPLES,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=3,\n",
    "    max_grad_norm=0.1,\n",
    "    report_to=\"wandb\",\n",
    "    output_dir=\"outputs\",\n",
    "    vllm_gpu_memory_utilization=gpu_memory_utilization,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = xv.GRPOGuidedTrainer(\n",
    "    guided_schema=guided_schema,\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    reward_funcs=[\n",
    "        legal_move_reward,\n",
    "        valid_uci_reward,\n",
    "        engine_analysis_reward,\n",
    "    ],\n",
    ")\n",
    "\n",
    "trainer.train(resume_from_checkpoint=checkpoint_path)\n",
    "\n",
    "model.save_lora(\"chess_reasoner_llama_8b_lora\")\n",
    "print(\"Model saved to chess_reasoner_llama_8b_lora\")\n",
    "\n",
    "ENGINE.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def push_to_hub(model, tokenizer, repo_id):\n",
    "    \"\"\"Upload the trained model to Hugging Face Hub\"\"\"\n",
    "    tqdm.write(f\"Uploading model to Hugging Face Hub: {repo_id}\")\n",
    "    model.push_to_hub(repo_id)\n",
    "    tokenizer.push_to_hub(repo_id)\n",
    "    tqdm.write(f\"Successfully uploaded model to: https://huggingface.co/{repo_id}\")\n",
    "\n",
    "# push_to_hub(model, tokenizer, \"tommyp111/chess-reasoner\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
