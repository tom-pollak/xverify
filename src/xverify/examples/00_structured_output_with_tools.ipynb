{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Output with Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tom/fun/xverify/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-03-10 20:03:48,727\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "from textwrap import dedent\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from xverify import ToolUse, run_tools, Env\n",
    "from xverify.tools import calculator, search\n",
    "from pydantic import ValidationError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Often when running multi-step reasoning, we want to use tools to help us.\n",
    "\n",
    "However, not many libraries natively support this. Pydantic for instance is optimized for a static declarative schema, which isn't well suited to ad-hoc tool use.\n",
    "\n",
    "Here we can see two examples of tools:\n",
    "- `calculator`: essentially a wrapper around the `eval` function\n",
    "- `search`: uses duckduckgo to search the web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculator(expression='3 + 4 * (6 ** 7)')='1119747'\n",
      "\n",
      "---\n",
      "\n",
      "search(query='What is the capital of France?', num_results=1)='• Paris - Wikipedia\\n  Paris (French pronunciation: ⓘ) is the capital and largest city of France.'\n"
     ]
    }
   ],
   "source": [
    "print(f\"{calculator(expression='3 + 4 * (6 ** 7)')=}\")\n",
    "print(\"\\n---\\n\")\n",
    "print(f\"{search(query='What is the capital of France?', num_results=1)=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is we can't (natively) include a tool call in a Pydantic model (due to the static declarative schema).\n",
    "\n",
    "However, we can use the new `ToolUse` class to handle tool calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reasoning=\"Let's add two numbers\" tool_use=calculator(expression='2 + 2', tool_name='calculator')\n",
      "calc_2_2.tool_use.run_tool()='4'\n"
     ]
    }
   ],
   "source": [
    "class ReasoiningToolResult(BaseModel):\n",
    "    \"\"\"The result of a reasoning tool\"\"\"\n",
    "\n",
    "    reasoning: str\n",
    "    tool_use: ToolUse[calculator, search]\n",
    "\n",
    "\n",
    "calc_2_2 = ReasoiningToolResult.model_validate(\n",
    "    {\n",
    "        \"reasoning\": \"Let's add two numbers\",\n",
    "        \"tool_use\": {\"tool_name\": \"calculator\", \"expression\": \"2 + 2\"},\n",
    "    }\n",
    ")\n",
    "print(calc_2_2)\n",
    "print(f\"{calc_2_2.tool_use.run_tool()=}\") # on a ToolUse object, we can call run_tool() to run the tool and get the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is nice because if we can easily validate any arbitary schema and tool use is correct without any ad-hoc parsing (and we'll be able to enforce the LLM output is correct with guided decoding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tool not found!\n",
      "wrong argument!\n",
      "wrong argument type!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    ReasoiningToolResult.model_validate(\n",
    "        {\n",
    "            \"reasoning\": \"\",\n",
    "            \"tool_use\": {\"tool_name\": \"none_existing_tool\", \"expression\": \"2 + 2\"},\n",
    "        }\n",
    "    )\n",
    "except ValidationError:\n",
    "    print(\"tool not found!\")\n",
    "try:\n",
    "    ReasoiningToolResult.model_validate(\n",
    "        {\n",
    "            \"reasoning\": \"\",\n",
    "            \"tool_use\": {\"tool_name\": \"calculator\", \"wrong_arg\": \"2 + 2\"},\n",
    "        },\n",
    "    )\n",
    "except ValidationError:\n",
    "    print(\"wrong argument!\")\n",
    "\n",
    "try:\n",
    "    ReasoiningToolResult.model_validate(\n",
    "        {\n",
    "            \"reasoning\": \"\",\n",
    "            \"tool_use\": {\"tool_name\": \"calculator\", \"expression\": 2 + 2},\n",
    "        },\n",
    "    )\n",
    "except ValidationError:\n",
    "    print(\"wrong argument type!\")\n",
    "\n",
    "try:\n",
    "    # TODO: this should be a validation error\n",
    "    ReasoiningToolResult.model_validate(\n",
    "        {\n",
    "            \"reasoning\": \"\",\n",
    "            \"tool_use\": {\"tool_name\": \"calculator\", \"expression\": \"2 + 2\", \"extra_arg\": \"extra_arg\"},\n",
    "        },\n",
    "    )\n",
    "except ValidationError:\n",
    "    print(\"extra argument!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can implement a ReACT loop with tools really easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(decision=Reason_and_Act(scratchpad='the question is 2 + 2', reasoning='we should use the calculator tool!', tool_use=calculator(expression='2 + 2', tool_name='calculator')))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Reason_and_Act(BaseModel):\n",
    "    scratchpad: str = Field(..., description=\"Information from the Observation useful to answer the question\")\n",
    "    reasoning: str = Field(..., description=\"It describes your thoughts about the question you have been asked\")\n",
    "    tool_use: ToolUse[calculator, search] = Field(..., description=\"The tool call to use\")\n",
    "\n",
    "class FinalAnswer(BaseModel):\n",
    "    answer: str = Field(..., description=\"The final answer to the question\")\n",
    "\n",
    "class Response(BaseModel):\n",
    "    decision: Reason_and_Act | FinalAnswer\n",
    "\n",
    "\n",
    "res = Response.model_validate(\n",
    "    {\n",
    "        \"decision\": {\n",
    "            \"scratchpad\": \"the question is 2 + 2\",\n",
    "            \"reasoning\": \"we should use the calculator tool!\",\n",
    "            \"tool_use\": {\n",
    "                \"tool_name\": \"calculator\",\n",
    "                \"expression\": \"2 + 2\",\n",
    "            },\n",
    "        },\n",
    "    }\n",
    ")\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And in case we just want to run all the tools in a response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decision': {'tool_use': {'calculator': '4'}}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_tools(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will return `None` where no tools were called, which is useful for checking for the end of the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(run_tools(Response(decision=FinalAnswer(answer=\"42\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you just want the output, run `run_tools` on the instantiated `ToolUse` object itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res.decision.tool_use.run_tool()='4'\n"
     ]
    }
   ],
   "source": [
    "if isinstance(res.decision, Reason_and_Act):\n",
    "    print(f\"{res.decision.tool_use.run_tool()=}\")\n",
    "else:\n",
    "    print(f\"{res.decision.answer=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And of course we can do multiple tool calls in a single response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tool_use': [{'calculator': '4'},\n",
      "              {'search': '• Moon Fact Sheet - NSSDCA\\n'\n",
      "                         '  Equatorial radius (km) 1738.1: 6378.1: 0.2725: '\n",
      "                         'Polar radius (km) 1736.0: 6356.8: 0.2731: Volumetric '\n",
      "                         'mean radius (km) 1737.4: 6371.0: 0.2727: Ellipticity '\n",
      "                         '(Flattening) ...\\n'\n",
      "                         '\\n'\n",
      "                         '• Moon - Wikipedia\\n'\n",
      "                         '  The Moon has a solid iron-rich inner core with a '\n",
      "                         'radius possibly as small as 240 kilometres (150 mi) '\n",
      "                         'and a fluid outer core primarily made of liquid iron '\n",
      "                         'with a radius of roughly 300 kilometres (190 m.'},\n",
      "              {'calculator': '1480647168'}]}\n"
     ]
    }
   ],
   "source": [
    "class MultiToolUse(BaseModel):\n",
    "    tool_use: list[ToolUse[calculator, search]]\n",
    "\n",
    "\n",
    "res = MultiToolUse.model_validate(\n",
    "    {\n",
    "        \"tool_use\": [\n",
    "            {\"tool_name\": \"calculator\", \"expression\": \"2 + 2\"},\n",
    "            {\n",
    "                \"tool_name\": \"search\",\n",
    "                \"query\": \"What is the radius of the moon?\",\n",
    "                \"num_results\": 2,\n",
    "            },\n",
    "            {\"tool_name\": \"calculator\", \"expression\": \"3424 * 432432\"},\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "pprint(run_tools(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is cool, kinda, but now we have structed schema, we can **enforce** the LLM output is correct with guided decoding.\n",
    "\n",
    "Let's go back to our ReACT loop, and use `Env` to enforce the tool calls are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-10 20:03:52 __init__.py:207] Automatically detected platform cuda.\n",
      "INFO 03-10 20:03:57 config.py:549] This model supports multiple tasks: {'score', 'classify', 'reward', 'embed', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 03-10 20:03:57 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='Qwen/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 03-10 20:04:02 cuda.py:229] Using Flash Attention backend.\n",
      "INFO 03-10 20:04:02 model_runner.py:1110] Starting to load model Qwen/Qwen2.5-14B-Instruct...\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 270.00 MiB. GPU 0 has a total capacity of 23.58 GiB of which 86.88 MiB is free. Including non-PyTorch memory, this process has 23.45 GiB memory in use. Of the allocated memory 23.15 GiB is allocated by PyTorch, and 12.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LLM\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mllm\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m():  \u001b[38;5;66;03m# interactive use\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     llm = \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mQwen/Qwen2.5-14B-Instruct\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_model_len\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m env = Env(Response)\n\u001b[32m      7\u001b[39m sampling_params = env.sampling_params(\n\u001b[32m      8\u001b[39m     max_tokens=\u001b[32m500\u001b[39m,\n\u001b[32m      9\u001b[39m     guided_decoding=\u001b[38;5;28mdict\u001b[39m(whitespace_pattern=\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mn ]?\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     10\u001b[39m     n=\u001b[32m1\u001b[39m,\n\u001b[32m     11\u001b[39m     temperature=\u001b[32m0.5\u001b[39m,\n\u001b[32m     12\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/vllm/utils.py:1022\u001b[39m, in \u001b[36mdeprecate_args.<locals>.wrapper.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1015\u001b[39m             msg += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1017\u001b[39m         warnings.warn(\n\u001b[32m   1018\u001b[39m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(msg),\n\u001b[32m   1019\u001b[39m             stacklevel=\u001b[32m3\u001b[39m,  \u001b[38;5;66;03m# The inner function takes up one level\u001b[39;00m\n\u001b[32m   1020\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1022\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/vllm/entrypoints/llm.py:242\u001b[39m, in \u001b[36mLLM.__init__\u001b[39m\u001b[34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_overrides, mm_processor_kwargs, task, override_pooler_config, compilation_config, **kwargs)\u001b[39m\n\u001b[32m    239\u001b[39m \u001b[38;5;66;03m# Logic to switch between engines is done at runtime instead of import\u001b[39;00m\n\u001b[32m    240\u001b[39m \u001b[38;5;66;03m# to avoid import order issues\u001b[39;00m\n\u001b[32m    241\u001b[39m \u001b[38;5;28mself\u001b[39m.engine_class = \u001b[38;5;28mself\u001b[39m.get_engine_class()\n\u001b[32m--> \u001b[39m\u001b[32m242\u001b[39m \u001b[38;5;28mself\u001b[39m.llm_engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mUsageContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[38;5;28mself\u001b[39m.request_counter = Counter()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/vllm/engine/llm_engine.py:489\u001b[39m, in \u001b[36mLLMEngine.from_engine_args\u001b[39m\u001b[34m(cls, engine_args, usage_context, stat_loggers)\u001b[39m\n\u001b[32m    487\u001b[39m executor_class = \u001b[38;5;28mcls\u001b[39m._get_executor_cls(engine_config)\n\u001b[32m    488\u001b[39m \u001b[38;5;66;03m# Create the LLM engine.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m489\u001b[39m engine = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m    \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m engine\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/vllm/engine/llm_engine.py:273\u001b[39m, in \u001b[36mLLMEngine.__init__\u001b[39m\u001b[34m(self, vllm_config, executor_class, log_stats, usage_context, stat_loggers, input_registry, mm_registry, use_cached_outputs)\u001b[39m\n\u001b[32m    269\u001b[39m \u001b[38;5;28mself\u001b[39m.input_registry = input_registry\n\u001b[32m    270\u001b[39m \u001b[38;5;28mself\u001b[39m.input_processor = input_registry.create_input_processor(\n\u001b[32m    271\u001b[39m     \u001b[38;5;28mself\u001b[39m.model_config)\n\u001b[32m--> \u001b[39m\u001b[32m273\u001b[39m \u001b[38;5;28mself\u001b[39m.model_executor = \u001b[43mexecutor_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model_config.runner_type != \u001b[33m\"\u001b[39m\u001b[33mpooling\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    276\u001b[39m     \u001b[38;5;28mself\u001b[39m._initialize_kv_caches()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/vllm/executor/executor_base.py:52\u001b[39m, in \u001b[36mExecutorBase.__init__\u001b[39m\u001b[34m(self, vllm_config)\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28mself\u001b[39m.prompt_adapter_config = vllm_config.prompt_adapter_config\n\u001b[32m     51\u001b[39m \u001b[38;5;28mself\u001b[39m.observability_config = vllm_config.observability_config\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_init_executor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;28mself\u001b[39m.is_sleeping = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py:47\u001b[39m, in \u001b[36mUniProcExecutor._init_executor\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28mself\u001b[39m.collective_rpc(\u001b[33m\"\u001b[39m\u001b[33minit_worker\u001b[39m\u001b[33m\"\u001b[39m, args=([kwargs], ))\n\u001b[32m     46\u001b[39m \u001b[38;5;28mself\u001b[39m.collective_rpc(\u001b[33m\"\u001b[39m\u001b[33minit_device\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollective_rpc\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mload_model\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py:56\u001b[39m, in \u001b[36mUniProcExecutor.collective_rpc\u001b[39m\u001b[34m(self, method, timeout, args, kwargs)\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     55\u001b[39m     kwargs = {}\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m answer = \u001b[43mrun_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdriver_worker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [answer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/vllm/utils.py:2196\u001b[39m, in \u001b[36mrun_method\u001b[39m\u001b[34m(obj, method, args, kwargs)\u001b[39m\n\u001b[32m   2194\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2195\u001b[39m     func = partial(method, obj)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2196\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/vllm/worker/worker.py:183\u001b[39m, in \u001b[36mWorker.load_model\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    181\u001b[39m     context = nullcontext()\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_runner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/vllm/worker/model_runner.py:1112\u001b[39m, in \u001b[36mGPUModelRunnerBase.load_model\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1110\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mStarting to load model \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m.model_config.model)\n\u001b[32m   1111\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m DeviceMemoryProfiler(\u001b[38;5;28mself\u001b[39m.device) \u001b[38;5;28;01mas\u001b[39;00m m:\n\u001b[32m-> \u001b[39m\u001b[32m1112\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1114\u001b[39m \u001b[38;5;28mself\u001b[39m.model_memory_usage = m.consumed_memory\n\u001b[32m   1115\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mLoading model weights took \u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[33m GB\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1116\u001b[39m             \u001b[38;5;28mself\u001b[39m.model_memory_usage / \u001b[38;5;28mfloat\u001b[39m(\u001b[32m2\u001b[39m**\u001b[32m30\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/__init__.py:14\u001b[39m, in \u001b[36mget_model\u001b[39m\u001b[34m(vllm_config)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_model\u001b[39m(*, vllm_config: VllmConfig) -> nn.Module:\n\u001b[32m     13\u001b[39m     loader = get_model_loader(vllm_config.load_config)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py:406\u001b[39m, in \u001b[36mDefaultModelLoader.load_model\u001b[39m\u001b[34m(self, vllm_config)\u001b[39m\n\u001b[32m    404\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m set_default_torch_dtype(model_config.dtype):\n\u001b[32m    405\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m target_device:\n\u001b[32m--> \u001b[39m\u001b[32m406\u001b[39m         model = \u001b[43m_initialize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    408\u001b[39m     weights_to_load = {name \u001b[38;5;28;01mfor\u001b[39;00m name, _ \u001b[38;5;129;01min\u001b[39;00m model.named_parameters()}\n\u001b[32m    409\u001b[39m     loaded_weights = model.load_weights(\n\u001b[32m    410\u001b[39m         \u001b[38;5;28mself\u001b[39m._get_all_weights(model_config, model))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py:125\u001b[39m, in \u001b[36m_initialize_model\u001b[39m\u001b[34m(vllm_config, prefix)\u001b[39m\n\u001b[32m    122\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mvllm_config\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m all_params \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mprefix\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m all_params:\n\u001b[32m    123\u001b[39m     \u001b[38;5;66;03m# new-style model class\u001b[39;00m\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_current_vllm_config(vllm_config, check_compile=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    127\u001b[39m msg = (\u001b[33m\"\u001b[39m\u001b[33mvLLM model class should accept `vllm_config` and `prefix` as \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    128\u001b[39m        \u001b[33m\"\u001b[39m\u001b[33minput arguments. Possibly you have an old-style model class\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    129\u001b[39m        \u001b[33m\"\u001b[39m\u001b[33m registered from out of tree and it is used for new vLLM version. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    130\u001b[39m        \u001b[33m\"\u001b[39m\u001b[33mCheck https://docs.vllm.ai/en/latest/design/arch_overview.html \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    131\u001b[39m        \u001b[33m\"\u001b[39m\u001b[33mfor the design and update the model class accordingly.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    132\u001b[39m warnings.warn(msg, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py:453\u001b[39m, in \u001b[36mQwen2ForCausalLM.__init__\u001b[39m\u001b[34m(self, vllm_config, prefix)\u001b[39m\n\u001b[32m    450\u001b[39m \u001b[38;5;28mself\u001b[39m.lora_config = lora_config\n\u001b[32m    452\u001b[39m \u001b[38;5;28mself\u001b[39m.quant_config = quant_config\n\u001b[32m--> \u001b[39m\u001b[32m453\u001b[39m \u001b[38;5;28mself\u001b[39m.model = \u001b[43mQwen2Model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    454\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_prefix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m get_pp_group().is_last_rank:\n\u001b[32m    457\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m config.tie_word_embeddings:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/vllm/compilation/decorators.py:151\u001b[39m, in \u001b[36m_support_torch_compile.<locals>.__init__\u001b[39m\u001b[34m(self, vllm_config, prefix, **kwargs)\u001b[39m\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *, vllm_config: VllmConfig, prefix: \u001b[38;5;28mstr\u001b[39m = \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m     \u001b[43mold_init\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    152\u001b[39m     \u001b[38;5;28mself\u001b[39m.vllm_config = vllm_config\n\u001b[32m    153\u001b[39m     \u001b[38;5;66;03m# for CompilationLevel.DYNAMO_AS_IS , the upper level model runner\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;66;03m# will handle the compilation, so we don't need to do anything here.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py:307\u001b[39m, in \u001b[36mQwen2Model.__init__\u001b[39m\u001b[34m(self, vllm_config, prefix)\u001b[39m\n\u001b[32m    304\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    305\u001b[39m     \u001b[38;5;28mself\u001b[39m.embed_tokens = PPMissingLayer()\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m \u001b[38;5;28mself\u001b[39m.start_layer, \u001b[38;5;28mself\u001b[39m.end_layer, \u001b[38;5;28mself\u001b[39m.layers = \u001b[43mmake_layers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    308\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_hidden_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    309\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mQwen2DecoderLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[43m                                     \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[43m                                     \u001b[49m\u001b[43mquant_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquant_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m                                     \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mprefix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.layers\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[38;5;28mself\u001b[39m.make_empty_intermediate_tensors = (\n\u001b[32m    317\u001b[39m     make_empty_intermediate_tensors_factory(\n\u001b[32m    318\u001b[39m         [\u001b[33m\"\u001b[39m\u001b[33mhidden_states\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mresidual\u001b[39m\u001b[33m\"\u001b[39m], config.hidden_size))\n\u001b[32m    319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m get_pp_group().is_last_rank:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/vllm/model_executor/models/utils.py:558\u001b[39m, in \u001b[36mmake_layers\u001b[39m\u001b[34m(num_hidden_layers, layer_fn, prefix)\u001b[39m\n\u001b[32m    552\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributed\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_pp_indices\n\u001b[32m    553\u001b[39m start_layer, end_layer = get_pp_indices(num_hidden_layers,\n\u001b[32m    554\u001b[39m                                         get_pp_group().rank_in_group,\n\u001b[32m    555\u001b[39m                                         get_pp_group().world_size)\n\u001b[32m    556\u001b[39m modules = torch.nn.ModuleList(\n\u001b[32m    557\u001b[39m     [PPMissingLayer() \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_layer)] + [\n\u001b[32m--> \u001b[39m\u001b[32m558\u001b[39m         maybe_offload_to_cpu(\u001b[43mlayer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mprefix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43midx\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m    559\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_layer, end_layer)\n\u001b[32m    560\u001b[39m     ] + [PPMissingLayer() \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(end_layer, num_hidden_layers)])\n\u001b[32m    561\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m start_layer, end_layer, modules\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py:309\u001b[39m, in \u001b[36mQwen2Model.__init__.<locals>.<lambda>\u001b[39m\u001b[34m(prefix)\u001b[39m\n\u001b[32m    304\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    305\u001b[39m     \u001b[38;5;28mself\u001b[39m.embed_tokens = PPMissingLayer()\n\u001b[32m    307\u001b[39m \u001b[38;5;28mself\u001b[39m.start_layer, \u001b[38;5;28mself\u001b[39m.end_layer, \u001b[38;5;28mself\u001b[39m.layers = make_layers(\n\u001b[32m    308\u001b[39m     config.num_hidden_layers,\n\u001b[32m--> \u001b[39m\u001b[32m309\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m prefix: \u001b[43mQwen2DecoderLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[43m                                     \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[43m                                     \u001b[49m\u001b[43mquant_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquant_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m                                     \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    313\u001b[39m     prefix=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.layers\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    314\u001b[39m )\n\u001b[32m    316\u001b[39m \u001b[38;5;28mself\u001b[39m.make_empty_intermediate_tensors = (\n\u001b[32m    317\u001b[39m     make_empty_intermediate_tensors_factory(\n\u001b[32m    318\u001b[39m         [\u001b[33m\"\u001b[39m\u001b[33mhidden_states\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mresidual\u001b[39m\u001b[33m\"\u001b[39m], config.hidden_size))\n\u001b[32m    319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m get_pp_group().is_last_rank:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py:220\u001b[39m, in \u001b[36mQwen2DecoderLayer.__init__\u001b[39m\u001b[34m(self, config, cache_config, quant_config, prefix)\u001b[39m\n\u001b[32m    206\u001b[39m     attn_type = AttentionType.ENCODER_ONLY\n\u001b[32m    208\u001b[39m \u001b[38;5;28mself\u001b[39m.self_attn = Qwen2Attention(\n\u001b[32m    209\u001b[39m     hidden_size=\u001b[38;5;28mself\u001b[39m.hidden_size,\n\u001b[32m    210\u001b[39m     num_heads=config.num_attention_heads,\n\u001b[32m   (...)\u001b[39m\u001b[32m    218\u001b[39m     attn_type=attn_type,\n\u001b[32m    219\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m \u001b[38;5;28mself\u001b[39m.mlp = \u001b[43mQwen2MLP\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m    \u001b[49m\u001b[43mintermediate_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mintermediate_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_act\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhidden_act\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquant_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquant_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mprefix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.mlp\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[38;5;28mself\u001b[39m.input_layernorm = RMSNorm(config.hidden_size,\n\u001b[32m    228\u001b[39m                                eps=config.rms_norm_eps)\n\u001b[32m    229\u001b[39m \u001b[38;5;28mself\u001b[39m.post_attention_layernorm = RMSNorm(config.hidden_size,\n\u001b[32m    230\u001b[39m                                         eps=config.rms_norm_eps)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py:75\u001b[39m, in \u001b[36mQwen2MLP.__init__\u001b[39m\u001b[34m(self, hidden_size, intermediate_size, hidden_act, quant_config, prefix)\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m     67\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     68\u001b[39m     hidden_size: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     72\u001b[39m     prefix: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     73\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     74\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m     \u001b[38;5;28mself\u001b[39m.gate_up_proj = \u001b[43mMergedColumnParallelLinear\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mintermediate_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquant_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquant_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mprefix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.gate_up_proj\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m     \u001b[38;5;28mself\u001b[39m.down_proj = RowParallelLinear(\n\u001b[32m     83\u001b[39m         intermediate_size,\n\u001b[32m     84\u001b[39m         hidden_size,\n\u001b[32m   (...)\u001b[39m\u001b[32m     87\u001b[39m         prefix=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.down_proj\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     88\u001b[39m     )\n\u001b[32m     89\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m hidden_act != \u001b[33m\"\u001b[39m\u001b[33msilu\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py:441\u001b[39m, in \u001b[36mMergedColumnParallelLinear.__init__\u001b[39m\u001b[34m(self, input_size, output_sizes, bias, gather_output, skip_bias_add, params_dtype, quant_config, prefix)\u001b[39m\n\u001b[32m    439\u001b[39m tp_size = get_tensor_model_parallel_world_size()\n\u001b[32m    440\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(output_size % tp_size == \u001b[32m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m output_size \u001b[38;5;129;01min\u001b[39;00m output_sizes)\n\u001b[32m--> \u001b[39m\u001b[32m441\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m                 \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput_sizes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[43m                 \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[43m                 \u001b[49m\u001b[43mgather_output\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgather_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    445\u001b[39m \u001b[43m                 \u001b[49m\u001b[43mskip_bias_add\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_bias_add\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    446\u001b[39m \u001b[43m                 \u001b[49m\u001b[43mparams_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[43m                 \u001b[49m\u001b[43mquant_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquant_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    448\u001b[39m \u001b[43m                 \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py:314\u001b[39m, in \u001b[36mColumnParallelLinear.__init__\u001b[39m\u001b[34m(self, input_size, output_size, bias, gather_output, skip_bias_add, params_dtype, quant_config, output_sizes, prefix)\u001b[39m\n\u001b[32m    311\u001b[39m     output_sizes = [output_size]\n\u001b[32m    313\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.quant_method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mquant_method\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_weights\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_size_per_partition\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minput_size_per_partition\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_partition_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moutput_partition_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparams_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    322\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight_loader_v2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mquant_method\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__class__\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\n\u001b[32m    323\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mWEIGHT_LOADER_V2_SUPPORTED\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m bias:\n\u001b[32m    325\u001b[39m     \u001b[38;5;28mself\u001b[39m.bias = Parameter(\n\u001b[32m    326\u001b[39m         torch.empty(\u001b[38;5;28mself\u001b[39m.output_size_per_partition,\n\u001b[32m    327\u001b[39m                     dtype=params_dtype))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py:129\u001b[39m, in \u001b[36mUnquantizedLinearMethod.create_weights\u001b[39m\u001b[34m(self, layer, input_size_per_partition, output_partition_sizes, input_size, output_size, params_dtype, **extra_weight_attrs)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate_weights\u001b[39m(\u001b[38;5;28mself\u001b[39m, layer: torch.nn.Module,\n\u001b[32m    125\u001b[39m                    input_size_per_partition: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m    126\u001b[39m                    output_partition_sizes: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m], input_size: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m    127\u001b[39m                    output_size: \u001b[38;5;28mint\u001b[39m, params_dtype: torch.dtype,\n\u001b[32m    128\u001b[39m                    **extra_weight_attrs):\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m     weight = Parameter(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput_partition_sizes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m                                   \u001b[49m\u001b[43minput_size_per_partition\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[43m                                   \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams_dtype\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    132\u001b[39m                        requires_grad=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    133\u001b[39m     set_weight_attrs(weight, {\u001b[33m\"\u001b[39m\u001b[33minput_dim\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m1\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33moutput_dim\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m0\u001b[39m})\n\u001b[32m    134\u001b[39m     layer.register_parameter(\u001b[33m\"\u001b[39m\u001b[33mweight\u001b[39m\u001b[33m\"\u001b[39m, weight)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/torch/utils/_device.py:106\u001b[39m, in \u001b[36mDeviceContext.__torch_function__\u001b[39m\u001b[34m(self, func, types, args, kwargs)\u001b[39m\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs.get(\u001b[33m'\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    105\u001b[39m     kwargs[\u001b[33m'\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28mself\u001b[39m.device\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 0 has a total capacity of 23.58 GiB of which 86.88 MiB is free. Including non-PyTorch memory, this process has 23.45 GiB memory in use. Of the allocated memory 23.15 GiB is allocated by PyTorch, and 12.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from vllm import LLM\n",
    "\n",
    "if \"llm\" not in globals():  # interactive use\n",
    "    llm = LLM(model=\"Qwen/Qwen2.5-14B-Instruct\", max_model_len=2000)\n",
    "\n",
    "env = Env(Response)\n",
    "sampling_params = env.sampling_params(\n",
    "    max_tokens=500,\n",
    "    guided_decoding=dict(whitespace_pattern=r\"[\\n ]?\"),\n",
    "    n=1,\n",
    "    temperature=0.5,\n",
    ")\n",
    "\n",
    "max_steps = 5\n",
    "messages: list[dict] = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": f\"\"\"\\\n",
    "You are a helpful assistant, responding via JSON structured output. Tool responses from the user are also in JSON.\n",
    "- Think step by step using the scratchpad, reasoning outputs. You have {max_steps - 1} steps to think before responding.\n",
    "- Use the tools provided. DO NOT rely on your own knowledge when a tool is available to help you.\n",
    "- Respond with a final answer once your are sure you have the answer.\n",
    "\n",
    "Respond with a JSON object, following the schema below:\n",
    "\n",
    "{json.dumps(env.model.model_json_schema(), indent=2)}\n",
    "\n",
    "\"\"\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"What is the radius of the moon?\"},\n",
    "]\n",
    "\n",
    "for _ in range(max_steps):\n",
    "    outp = llm.chat(  # type: ignore\n",
    "        messages=messages,  # type: ignore\n",
    "        sampling_params=sampling_params,\n",
    "        use_tqdm=False,\n",
    "    )\n",
    "    struct_res = env.parse(outp[0].outputs[0].text)\n",
    "    messages.append({\"role\": \"assistant\", \"content\": struct_res.model_dump()})\n",
    "    tool_outp = run_tools(struct_res)\n",
    "    if tool_outp:\n",
    "        messages.append({\"role\": \"user\", \"content\": tool_outp})\n",
    "    else:\n",
    "        break\n",
    "\n",
    "print(json.dumps(messages, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"$defs\": {\n",
      "    \"FinalAnswer\": {\n",
      "      \"properties\": {\n",
      "        \"answer\": {\n",
      "          \"description\": \"The final answer to the question\",\n",
      "          \"title\": \"Answer\",\n",
      "          \"type\": \"string\"\n",
      "        }\n",
      "      },\n",
      "      \"required\": [\n",
      "        \"answer\"\n",
      "      ],\n",
      "      \"title\": \"FinalAnswer\",\n",
      "      \"type\": \"object\"\n",
      "    },\n",
      "    \"Reason_and_Act\": {\n",
      "      \"properties\": {\n",
      "        \"scratchpad\": {\n",
      "          \"description\": \"Information from the Observation useful to answer the question\",\n",
      "          \"title\": \"Scratchpad\",\n",
      "          \"type\": \"string\"\n",
      "        },\n",
      "        \"reasoning\": {\n",
      "          \"description\": \"It describes your thoughts about the question you have been asked\",\n",
      "          \"title\": \"Reasoning\",\n",
      "          \"type\": \"string\"\n",
      "        },\n",
      "        \"tool_use\": {\n",
      "          \"description\": \"The tool call to use\",\n",
      "          \"discriminator\": {\n",
      "            \"mapping\": {\n",
      "              \"calculator\": \"#/$defs/calculator\",\n",
      "              \"search\": \"#/$defs/search\"\n",
      "            },\n",
      "            \"propertyName\": \"tool_name\"\n",
      "          },\n",
      "          \"oneOf\": [\n",
      "            {\n",
      "              \"$ref\": \"#/$defs/calculator\"\n",
      "            },\n",
      "            {\n",
      "              \"$ref\": \"#/$defs/search\"\n",
      "            }\n",
      "          ],\n",
      "          \"title\": \"Tool Use\"\n",
      "        }\n",
      "      },\n",
      "      \"required\": [\n",
      "        \"scratchpad\",\n",
      "        \"reasoning\",\n",
      "        \"tool_use\"\n",
      "      ],\n",
      "      \"title\": \"Reason_and_Act\",\n",
      "      \"type\": \"object\"\n",
      "    },\n",
      "    \"calculator\": {\n",
      "      \"description\": \"Evaluates a single line of Python math expression. No imports or variables allowed.\\n\\n    Examples:\\n        {\\\"expression\\\": \\\"2 + 2\\\"} -> \\\"4\\\"\\n        {\\\"expression\\\": \\\"3 * (17 + 4)\\\"} -> \\\"63\\\"\\n        {\\\"expression\\\": \\\"100 / 5\\\"} -> \\\"20.0\\\"\\n    \\nReturns: str - The result of the calculation or an error message\",\n",
      "      \"properties\": {\n",
      "        \"expression\": {\n",
      "          \"description\": \"A mathematical expression using only numbers and basic operators (+,-,*,/,**,())\",\n",
      "          \"title\": \"Expression\",\n",
      "          \"type\": \"string\"\n",
      "        },\n",
      "        \"tool_name\": {\n",
      "          \"const\": \"calculator\",\n",
      "          \"description\": \"Function to call\",\n",
      "          \"title\": \"Tool Name\",\n",
      "          \"type\": \"string\"\n",
      "        }\n",
      "      },\n",
      "      \"required\": [\n",
      "        \"expression\",\n",
      "        \"tool_name\"\n",
      "      ],\n",
      "      \"title\": \"calculator\",\n",
      "      \"type\": \"object\"\n",
      "    },\n",
      "    \"search\": {\n",
      "      \"description\": \"Searches DuckDuckGo and returns concise summaries of top results.\\n\\n    Examples:\\n        {\\\"query\\\": \\\"who invented the lightbulb\\\", \\\"num_results\\\": 3}\\n    \\nReturns: str - Formatted string with bullet points of top results, each with title and brief summary\",\n",
      "      \"properties\": {\n",
      "        \"query\": {\n",
      "          \"description\": \"The search query string\",\n",
      "          \"title\": \"Query\",\n",
      "          \"type\": \"string\"\n",
      "        },\n",
      "        \"num_results\": {\n",
      "          \"default\": 5,\n",
      "          \"description\": \"Number of results to return (default: 5, max: 10)\",\n",
      "          \"title\": \"Num Results\",\n",
      "          \"type\": \"integer\"\n",
      "        },\n",
      "        \"tool_name\": {\n",
      "          \"const\": \"search\",\n",
      "          \"description\": \"Function to call\",\n",
      "          \"title\": \"Tool Name\",\n",
      "          \"type\": \"string\"\n",
      "        }\n",
      "      },\n",
      "      \"required\": [\n",
      "        \"query\",\n",
      "        \"tool_name\"\n",
      "      ],\n",
      "      \"title\": \"search\",\n",
      "      \"type\": \"object\"\n",
      "    }\n",
      "  },\n",
      "  \"properties\": {\n",
      "    \"decision\": {\n",
      "      \"anyOf\": [\n",
      "        {\n",
      "          \"$ref\": \"#/$defs/Reason_and_Act\"\n",
      "        },\n",
      "        {\n",
      "          \"$ref\": \"#/$defs/FinalAnswer\"\n",
      "        }\n",
      "      ],\n",
      "      \"title\": \"Decision\"\n",
      "    }\n",
      "  },\n",
      "  \"required\": [\n",
      "    \"decision\"\n",
      "  ],\n",
      "  \"title\": \"Response\",\n",
      "  \"type\": \"object\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "env = Env(Response)\n",
    "print(json.dumps(env.model.model_json_schema(), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'json_schema_for_humans' has no attribute 'generate'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson_schema_for_humans\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mjson_schema_for_humans\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m(env.model.model_json_schema())\n",
      "\u001b[31mAttributeError\u001b[39m: module 'json_schema_for_humans' has no attribute 'generate'"
     ]
    }
   ],
   "source": [
    "import json_schema_for_humans\n",
    "json_schema_for_humans.generate(env.model.model_json_schema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Error loading schema from uri /home/tom/fun/xverify/src/xverify/examples/schema.json: [Errno 2] No such file or directory: '/home/tom/fun/xverify/src/xverify/examples/schema.json'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Generating result.html ==\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Cannot generate documentation since root schema could not be loaded",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     12\u001b[39m result_file.seek(\u001b[32m0\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Generate the schema documentation using the file-like objects\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[43mgenerate\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_from_file_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mschema_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Print the generated documentation\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(result_file.getvalue())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/json_schema_for_humans/generate.py:95\u001b[39m, in \u001b[36mgenerate_from_file_object\u001b[39m\u001b[34m(schema_file, result_file, minify, deprecated_from_description, default_from_description, expand_buttons, copy_css, copy_js, link_to_reused_ref, config)\u001b[39m\n\u001b[32m     93\u001b[39m schemas_to_render = [SchemaToRender(schema_file, result_file, Path(result_file.name).parent)]\n\u001b[32m     94\u001b[39m template_renderer = TemplateRenderer(config)\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m \u001b[43mgenerate_schemas_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mschemas_to_render\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate_renderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m copy_additional_files_to_target(schemas_to_render, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/json_schema_for_humans/generate.py:153\u001b[39m, in \u001b[36mgenerate_schemas_doc\u001b[39m\u001b[34m(schemas_to_render, template_renderer, loaded_schemas)\u001b[39m\n\u001b[32m    150\u001b[39m rendered_schemas: Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m] = {}\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m schema_to_render \u001b[38;5;129;01min\u001b[39;00m schemas_to_render:\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     rendered_schemas[schema_to_render.schema_file_name] = \u001b[43m_generate_schema\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m        \u001b[49m\u001b[43mschema_to_render\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate_renderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloaded_schemas\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m rendered_schemas\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/json_schema_for_humans/generate.py:138\u001b[39m, in \u001b[36m_generate_schema\u001b[39m\u001b[34m(schema_to_render, template_renderer, loaded_schemas)\u001b[39m\n\u001b[32m    136\u001b[39m start = datetime.now()\n\u001b[32m    137\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m== Generating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mschema_to_render.name_for_output\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ==\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m result = \u001b[43mschema_to_render\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemplate_renderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloaded_schemas\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m== Generated \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mschema_to_render.name_for_output\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime.now()\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39mstart\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ==\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/json_schema_for_humans/schema/schema_to_render.py:56\u001b[39m, in \u001b[36mSchemaToRender.render\u001b[39m\u001b[34m(self, template_renderer, loaded_schemas)\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Render the schema. If a result file is defined, write to disk. Return the rendered documentation\"\"\"\u001b[39;00m\n\u001b[32m     51\u001b[39m schema_file = (\n\u001b[32m     52\u001b[39m     \u001b[38;5;28mopen\u001b[39m(file=\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m.schema_file), encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     53\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.schema_file, Path)\n\u001b[32m     54\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.schema_file\n\u001b[32m     55\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m intermediate_schema = \u001b[43mbuild_intermediate_representation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mschema_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate_renderer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloaded_schemas\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m rendered = template_renderer.render(intermediate_schema)\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.should_write_to_disk:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/json_schema_for_humans/schema/intermediate_representation.py:80\u001b[39m, in \u001b[36mbuild_intermediate_representation\u001b[39m\u001b[34m(schema_path, config, loaded_schemas)\u001b[39m\n\u001b[32m     78\u001b[39m loaded_schema = _load_schema(absolute_schema_path, [], loaded_schemas)\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m loaded_schema:\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCannot generate documentation since root schema could not be loaded\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     82\u001b[39m intermediate_representation = _build_node(\n\u001b[32m     83\u001b[39m     config=config,\n\u001b[32m     84\u001b[39m     resolved_references=resolved_references,\n\u001b[32m   (...)\u001b[39m\u001b[32m     93\u001b[39m     schema=loaded_schema,\n\u001b[32m     94\u001b[39m )\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m intermediate_representation\n",
      "\u001b[31mException\u001b[39m: Cannot generate documentation since root schema could not be loaded"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import json\n",
    "\n",
    "# Create a file-like object to mimic file I/O\n",
    "schema_file = io.StringIO(json.dumps(env.model.model_json_schema()))\n",
    "result_file = io.StringIO()\n",
    "\n",
    "# Add name attribute to StringIO objects\n",
    "schema_file.name = \"schema.json\"\n",
    "result_file.name = \"result.html\"\n",
    "schema_file.seek(0)\n",
    "result_file.seek(0)\n",
    "\n",
    "\n",
    "# Generate the schema documentation using the file-like objects\n",
    "generate.generate_from_file_object(schema_file, result_file)\n",
    "\n",
    "# Print the generated documentation\n",
    "print(result_file.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_io.StringIO' object has no attribute 'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m result_file = io.StringIO()\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Generate the schema documentation using the file-like objects\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mgenerate\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_from_file_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mschema_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Print the generated documentation\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(result_file.getvalue())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/json_schema_for_humans/generate.py:93\u001b[39m, in \u001b[36mgenerate_from_file_object\u001b[39m\u001b[34m(schema_file, result_file, minify, deprecated_from_description, default_from_description, expand_buttons, copy_css, copy_js, link_to_reused_ref, config)\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Generate the JSON schema documentation from opened file objects for both input and output files. The\u001b[39;00m\n\u001b[32m     82\u001b[39m \u001b[33;03mresult_file should be opened in write mode.\u001b[39;00m\n\u001b[32m     83\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     84\u001b[39m config = config \u001b[38;5;129;01mor\u001b[39;00m get_final_config(\n\u001b[32m     85\u001b[39m     deprecated_from_description=deprecated_from_description,\n\u001b[32m     86\u001b[39m     default_from_description=default_from_description,\n\u001b[32m   (...)\u001b[39m\u001b[32m     90\u001b[39m     link_to_reused_ref=link_to_reused_ref,\n\u001b[32m     91\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m schemas_to_render = [SchemaToRender(schema_file, result_file, Path(\u001b[43mresult_file\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m).parent)]\n\u001b[32m     94\u001b[39m template_renderer = TemplateRenderer(config)\n\u001b[32m     95\u001b[39m generate_schemas_doc(schemas_to_render, template_renderer)\n",
      "\u001b[31mAttributeError\u001b[39m: '_io.StringIO' object has no attribute 'name'"
     ]
    }
   ],
   "source": [
    "import io\n",
    "from json_schema_for_humans import generate\n",
    "\n",
    "# Create a file-like object to mimic file I/O\n",
    "schema_file = io.StringIO(json.dumps(env.model.model_json_schema()))\n",
    "result_file = io.StringIO()\n",
    "\n",
    "# Generate the schema documentation using the file-like objects\n",
    "generate.generate_from_file_object(schema_file, result_file)\n",
    "\n",
    "# Print the generated documentation\n",
    "print(result_file.getvalue())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to find a schema to render from ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson_schema_for_humans\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m generate\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mgenerate\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_from_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloaded_schemas\u001b[49m\u001b[43m=\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_json_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fun/xverify/.venv/lib/python3.12/site-packages/json_schema_for_humans/generate.py:37\u001b[39m, in \u001b[36mgenerate_from_schema\u001b[39m\u001b[34m(schema_file, loaded_schemas, minify, deprecated_from_description, default_from_description, expand_buttons, link_to_reused_ref, config)\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m rendered_content \u001b[38;5;129;01min\u001b[39;00m generate_schemas_doc(schemas_to_render, template_renderer, loaded_schemas).items():\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m rendered_content[\u001b[32m1\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnable to find a schema to render from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mschema_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: Unable to find a schema to render from "
     ]
    }
   ],
   "source": [
    "\n",
    "generate.generate_from_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the cool thing is, this is a Qwen 1.5B model, not even trained on function calling!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
